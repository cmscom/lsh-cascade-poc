{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実験用DuckDBの作成\n",
    "\n",
    "## 目的\n",
    "\n",
    "4つのWikipediaデータセット（英語/日本語 × タイトル/本文）を統合し、\n",
    "E5-largeでembeddingを生成してDuckDBに保存する。\n",
    "\n",
    "## データ\n",
    "\n",
    "| ファイル | 件数 | 内容 |\n",
    "|----------|------|------|\n",
    "| wikipedia_titles_en_100k | 100,000 | 英語タイトル |\n",
    "| wikipedia_titles_ja_100k | 100,000 | 日本語タイトル |\n",
    "| wikipedia_en_body_100k | 100,000 | 英語本文 |\n",
    "| wikipedia_ja_body_100k | 100,000 | 日本語本文 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.10.0+cu128\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import duckdb\n",
    "\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'CUDA device: {torch.cuda.get_device_name(0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. データの読み込みと統合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データ読み込み中...\n",
      "英語タイトル: 100,000件\n",
      "日本語タイトル: 100,000件\n",
      "英語本文: 100,000件\n",
      "日本語本文: 100,000件\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '../data'\n",
    "\n",
    "# 4つのデータを読み込み\n",
    "print('データ読み込み中...')\n",
    "\n",
    "# タイトルのみ\n",
    "titles_en = pd.read_parquet(f'{DATA_DIR}/wikipedia_titles_en_100k.parquet')\n",
    "titles_ja = pd.read_parquet(f'{DATA_DIR}/wikipedia_titles_ja_100k.parquet')\n",
    "\n",
    "# 本文あり\n",
    "body_en = pd.read_parquet(f'{DATA_DIR}/wikipedia_en_body_100k.parquet')\n",
    "body_ja = pd.read_parquet(f'{DATA_DIR}/wikipedia_ja_body_100k.parquet')\n",
    "\n",
    "print(f'英語タイトル: {len(titles_en):,}件')\n",
    "print(f'日本語タイトル: {len(titles_ja):,}件')\n",
    "print(f'英語本文: {len(body_en):,}件')\n",
    "print(f'日本語本文: {len(body_ja):,}件')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "統合データ: 400,000件\n",
      "\n",
      "データセット別件数:\n",
      "dataset\n",
      "titles_en    100000\n",
      "titles_ja    100000\n",
      "body_en      100000\n",
      "body_ja      100000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 統合用のデータフレームを作成\n",
    "def prepare_dataset(df, dataset_name, has_body=False):\n",
    "    \"\"\"データセットを統一フォーマットに変換\"\"\"\n",
    "    result = pd.DataFrame()\n",
    "    result['original_id'] = df['id']\n",
    "    result['title'] = df['title']\n",
    "    result['lang'] = df['lang']\n",
    "    result['dataset'] = dataset_name\n",
    "    \n",
    "    if has_body and 'text' in df.columns:\n",
    "        # タイトル + 本文を結合\n",
    "        result['text'] = df['title'] + '\\n\\n' + df['text'].fillna('')\n",
    "    else:\n",
    "        # タイトルのみ\n",
    "        result['text'] = df['title']\n",
    "    \n",
    "    return result\n",
    "\n",
    "# 各データセットを変換\n",
    "datasets = [\n",
    "    prepare_dataset(titles_en, 'titles_en', has_body=False),\n",
    "    prepare_dataset(titles_ja, 'titles_ja', has_body=False),\n",
    "    prepare_dataset(body_en, 'body_en', has_body=True),\n",
    "    prepare_dataset(body_ja, 'body_ja', has_body=True),\n",
    "]\n",
    "\n",
    "# 統合\n",
    "all_data = pd.concat(datasets, ignore_index=True)\n",
    "all_data['id'] = range(len(all_data))  # 連番ID\n",
    "\n",
    "print(f'\\n統合データ: {len(all_data):,}件')\n",
    "print(f'\\nデータセット別件数:')\n",
    "print(all_data['dataset'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "テキスト長の統計:\n",
      "              count    mean      std   min    25%     50%     75%        max\n",
      "dataset                                                                     \n",
      "body_en    100000.0  7216.0  26580.0  18.0  615.0  2752.0  6642.0  2084288.0\n",
      "body_ja    100000.0  4736.0  10274.0  17.0  937.0  2233.0  4733.0   693922.0\n",
      "titles_en  100000.0    28.0     16.0   1.0   15.0    24.0    38.0      238.0\n",
      "titles_ja  100000.0    12.0      9.0   1.0    5.0    10.0    16.0      124.0\n"
     ]
    }
   ],
   "source": [
    "# テキスト長の確認\n",
    "all_data['text_len'] = all_data['text'].str.len()\n",
    "\n",
    "print('テキスト長の統計:')\n",
    "print(all_data.groupby('dataset')['text_len'].describe().round(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "切り詰め後の最大長: 2000\n",
      "切り詰められたレコード数: 111,419件\n"
     ]
    }
   ],
   "source": [
    "# テキストを切り詰め（E5-largeの制限対応）\n",
    "# E5-largeは512トークン制限だが、文字数で約2000文字程度を目安に\n",
    "MAX_CHARS = 2000\n",
    "\n",
    "all_data['text_truncated'] = all_data['text'].str[:MAX_CHARS]\n",
    "\n",
    "# 切り詰め後の長さ確認\n",
    "all_data['truncated_len'] = all_data['text_truncated'].str.len()\n",
    "print(f'切り詰め後の最大長: {all_data[\"truncated_len\"].max()}')\n",
    "print(f'切り詰められたレコード数: {(all_data[\"text_len\"] > MAX_CHARS).sum():,}件')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. E5-largeでEmbedding生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E5-largeモデルを読み込み中...\n",
      "モデル読み込み完了\n",
      "Embedding次元: 1024\n"
     ]
    }
   ],
   "source": [
    "# E5-largeモデルの読み込み\n",
    "print('E5-largeモデルを読み込み中...')\n",
    "model = SentenceTransformer('intfloat/multilingual-e5-large', device='cuda')\n",
    "print(f'モデル読み込み完了')\n",
    "print(f'Embedding次元: {model.get_sentence_embedding_dimension()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding生成開始: 400,000件\n",
      "バッチサイズ: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6250/6250 [34:09<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding形状: (400000, 1024)\n"
     ]
    }
   ],
   "source": [
    "# バッチでembedding生成\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "texts = all_data['text_truncated'].tolist()\n",
    "# E5用のプレフィックス追加\n",
    "texts_with_prefix = ['passage: ' + t for t in texts]\n",
    "\n",
    "print(f'Embedding生成開始: {len(texts):,}件')\n",
    "print(f'バッチサイズ: {BATCH_SIZE}')\n",
    "\n",
    "embeddings = []\n",
    "for i in tqdm(range(0, len(texts_with_prefix), BATCH_SIZE)):\n",
    "    batch = texts_with_prefix[i:i+BATCH_SIZE]\n",
    "    batch_embeddings = model.encode(batch, convert_to_numpy=True, show_progress_bar=False)\n",
    "    embeddings.append(batch_embeddings)\n",
    "\n",
    "embeddings = np.vstack(embeddings)\n",
    "print(f'\\nEmbedding形状: {embeddings.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最終カラム: ['original_id', 'title', 'lang', 'dataset', 'text', 'id', 'text_truncated', 'embedding']\n",
      "データ数: 400,000\n"
     ]
    }
   ],
   "source": [
    "# embeddingをデータフレームに追加\n",
    "all_data['embedding'] = list(embeddings)\n",
    "\n",
    "# 不要なカラムを削除\n",
    "all_data = all_data.drop(columns=['text_len', 'truncated_len'])\n",
    "\n",
    "print(f'最終カラム: {list(all_data.columns)}')\n",
    "print(f'データ数: {len(all_data):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DuckDBに保存（HNSWインデックス付き）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DuckDBファイルを作成\n",
    "DB_PATH = '../data/experiment_400k.duckdb'\n",
    "\n",
    "# 既存ファイルがあれば削除\n",
    "import os\n",
    "if os.path.exists(DB_PATH):\n",
    "    os.remove(DB_PATH)\n",
    "    print(f'既存ファイルを削除: {DB_PATH}')\n",
    "\n",
    "# 接続\n",
    "con = duckdb.connect(DB_PATH)\n",
    "\n",
    "# VSS拡張をインストール・ロード\n",
    "con.execute(\"INSTALL vss\")\n",
    "con.execute(\"LOAD vss\")\n",
    "\n",
    "# HNSWインデックスの永続化を有効化\n",
    "con.execute(\"SET hnsw_enable_experimental_persistence = true\")\n",
    "\n",
    "print('DuckDB接続完了')\n",
    "print('VSS拡張ロード完了')\n",
    "print('HNSW永続化有効化完了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "テーブル作成完了\n"
     ]
    }
   ],
   "source": [
    "# テーブル作成\n",
    "con.execute(\"\"\"\n",
    "    CREATE TABLE documents (\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        original_id INTEGER,\n",
    "        title VARCHAR,\n",
    "        lang VARCHAR,\n",
    "        dataset VARCHAR,\n",
    "        text VARCHAR,\n",
    "        text_truncated VARCHAR,\n",
    "        embedding FLOAT[1024]\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "print('テーブル作成完了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データ挿入中: 400,000件\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [30:43<00:00, 46.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データ挿入完了\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# データ挿入（バッチ処理）\n",
    "INSERT_BATCH = 10000\n",
    "\n",
    "print(f'データ挿入中: {len(all_data):,}件')\n",
    "\n",
    "for i in tqdm(range(0, len(all_data), INSERT_BATCH)):\n",
    "    batch = all_data.iloc[i:i+INSERT_BATCH]\n",
    "    \n",
    "    # embedding をリストに変換\n",
    "    records = []\n",
    "    for _, row in batch.iterrows():\n",
    "        records.append((\n",
    "            int(row['id']),\n",
    "            int(row['original_id']),\n",
    "            row['title'],\n",
    "            row['lang'],\n",
    "            row['dataset'],\n",
    "            row['text'],\n",
    "            row['text_truncated'],\n",
    "            row['embedding'].tolist()\n",
    "        ))\n",
    "    \n",
    "    con.executemany(\"\"\"\n",
    "        INSERT INTO documents VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\", records)\n",
    "\n",
    "print('データ挿入完了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HNSW永続化有効化完了\n"
     ]
    }
   ],
   "source": [
    "# HNSW永続化を有効化\n",
    "con.execute(\"SET hnsw_enable_experimental_persistence = true\")\n",
    "print('HNSW永続化有効化完了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HNSWインデックス作成中...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd1b5cd40017487ea8874b268366f0c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HNSWインデックス作成完了\n"
     ]
    }
   ],
   "source": [
    "# HNSWインデックス作成\n",
    "print('HNSWインデックス作成中...')\n",
    "\n",
    "con.execute(\"\"\"\n",
    "    CREATE INDEX hnsw_idx ON documents \n",
    "    USING HNSW (embedding) \n",
    "    WITH (metric = 'cosine')\n",
    "\"\"\")\n",
    "\n",
    "print('HNSWインデックス作成完了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "データベース確認\n",
      "================================================================================\n",
      "総レコード数: 400,000\n",
      "\n",
      "データセット別件数:\n",
      "  dataset lang    cnt\n",
      "  body_en   en 100000\n",
      "  body_ja   ja 100000\n",
      "titles_en   en 100000\n",
      "titles_ja   ja 100000\n",
      "\n",
      "サンプルデータ:\n",
      " id                                                              title lang   dataset  text_len\n",
      "  0                Category:People by populated place in County Galway   en titles_en        51\n",
      "  1                                                            Sulkowo   en titles_en         7\n",
      "  2                                                          Felleries   en titles_en         9\n",
      "  3 Wikipedia:WikiProject Spam/LinkReports/comprarviagragenericoes.net   en titles_en        66\n",
      "  4                        Wikipedia:Articles for deletion/Hamdy Ahmed   en titles_en        43\n"
     ]
    }
   ],
   "source": [
    "# 確認\n",
    "print('=' * 80)\n",
    "print('データベース確認')\n",
    "print('=' * 80)\n",
    "\n",
    "count = con.execute(\"SELECT COUNT(*) FROM documents\").fetchone()[0]\n",
    "print(f'総レコード数: {count:,}')\n",
    "\n",
    "print('\\nデータセット別件数:')\n",
    "result = con.execute(\"\"\"\n",
    "    SELECT dataset, lang, COUNT(*) as cnt \n",
    "    FROM documents \n",
    "    GROUP BY dataset, lang\n",
    "    ORDER BY dataset\n",
    "\"\"\").fetchdf()\n",
    "print(result.to_string(index=False))\n",
    "\n",
    "# サンプルデータ\n",
    "print('\\nサンプルデータ:')\n",
    "sample = con.execute(\"\"\"\n",
    "    SELECT id, title, lang, dataset, LENGTH(text_truncated) as text_len\n",
    "    FROM documents \n",
    "    LIMIT 5\n",
    "\"\"\").fetchdf()\n",
    "print(sample.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "類似検索テスト\n",
      "================================================================================\n",
      "クエリ: 機械学習とは何ですか\n",
      "\n",
      "Top-5 類似結果:\n",
      "    id                  title lang   dataset  similarity\n",
      "303584  Transformer (機械学習モデル)   ja   body_ja    0.821834\n",
      "365449 Wikipedia:削除依頼/ニューラル機械   ja   body_ja    0.817743\n",
      "365320                   機械翻訳   ja   body_ja    0.817332\n",
      "341690               アンドリュー・ン   ja   body_ja    0.816761\n",
      "110973              ディープラーニング   ja titles_ja    0.816399\n"
     ]
    }
   ],
   "source": [
    "# 類似検索テスト\n",
    "print('\\n' + '=' * 80)\n",
    "print('類似検索テスト')\n",
    "print('=' * 80)\n",
    "\n",
    "# テストクエリ\n",
    "test_query = \"機械学習とは何ですか\"\n",
    "query_embedding = model.encode(['query: ' + test_query], convert_to_numpy=True)[0]\n",
    "\n",
    "print(f'クエリ: {test_query}')\n",
    "print('\\nTop-5 類似結果:')\n",
    "\n",
    "result = con.execute(\"\"\"\n",
    "    SELECT id, title, lang, dataset,\n",
    "           array_cosine_similarity(embedding, ?::FLOAT[1024]) as similarity\n",
    "    FROM documents\n",
    "    ORDER BY similarity DESC\n",
    "    LIMIT 5\n",
    "\"\"\", [query_embedding.tolist()]).fetchdf()\n",
    "\n",
    "print(result.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DuckDBファイル: ../data/experiment_400k.duckdb\n",
      "ファイルサイズ: 6844.8 MB\n"
     ]
    }
   ],
   "source": [
    "# クローズ\n",
    "con.close()\n",
    "\n",
    "# ファイルサイズ確認\n",
    "size_mb = os.path.getsize(DB_PATH) / (1024 * 1024)\n",
    "print(f'\\nDuckDBファイル: {DB_PATH}')\n",
    "print(f'ファイルサイズ: {size_mb:.1f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 完了サマリー"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<cell_type>markdown</cell_type>## 5. データベース検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "実験用DuckDB 検証結果\n",
      "================================================================================\n",
      "\n",
      "ファイル: ../data/experiment_400k.duckdb\n",
      "サイズ: 6844.8 MB\n",
      "\n",
      "総レコード数: 400,000\n",
      "\n",
      "データセット別件数:\n",
      "  dataset lang    cnt\n",
      "  body_en   en 100000\n",
      "  body_ja   ja 100000\n",
      "titles_en   en 100000\n",
      "titles_ja   ja 100000\n",
      "\n",
      "インデックス:\n",
      "index_name table_name  is_unique\n",
      "  hnsw_idx  documents      False\n",
      "\n",
      "Embedding次元: 1024\n"
     ]
    }
   ],
   "source": [
    "# データベース検証\n",
    "import os\n",
    "\n",
    "# 再接続（読み取り専用）\n",
    "con = duckdb.connect(DB_PATH, read_only=True)\n",
    "con.execute(\"LOAD vss\")\n",
    "\n",
    "print('=' * 80)\n",
    "print('実験用DuckDB 検証結果')\n",
    "print('=' * 80)\n",
    "\n",
    "# ファイルサイズ\n",
    "size_mb = os.path.getsize(DB_PATH) / (1024 * 1024)\n",
    "print(f'\\nファイル: {DB_PATH}')\n",
    "print(f'サイズ: {size_mb:.1f} MB')\n",
    "\n",
    "# レコード数\n",
    "count = con.execute(\"SELECT COUNT(*) FROM documents\").fetchone()[0]\n",
    "print(f'\\n総レコード数: {count:,}')\n",
    "\n",
    "# データセット別件数\n",
    "print('\\nデータセット別件数:')\n",
    "result = con.execute(\"\"\"\n",
    "    SELECT dataset, lang, COUNT(*) as cnt \n",
    "    FROM documents \n",
    "    GROUP BY dataset, lang\n",
    "    ORDER BY dataset\n",
    "\"\"\").fetchdf()\n",
    "print(result.to_string(index=False))\n",
    "\n",
    "# インデックス確認\n",
    "print('\\nインデックス:')\n",
    "indexes = con.execute(\"SELECT index_name, table_name, is_unique FROM duckdb_indexes()\").fetchdf()\n",
    "print(indexes.to_string(index=False))\n",
    "\n",
    "# Embedding次元確認\n",
    "emb_dim = con.execute(\"SELECT len(embedding) FROM documents LIMIT 1\").fetchone()[0]\n",
    "print(f'\\nEmbedding次元: {emb_dim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "類似検索テスト\n",
      "================================================================================\n",
      "\n",
      "クエリ: id=300000, title=\"Category:成都のスポーツ競技大会...\"\n",
      "\n",
      "Top-5 類似結果:\n",
      "    id                    title lang dataset  similarity\n",
      "308723           Category:成都の歴史   ja body_ja    0.942124\n",
      "316256           Category:成都の大学   ja body_ja    0.936409\n",
      "318488 Category:ホーチミン市のスポーツ競技大会   ja body_ja    0.935011\n",
      "352566         Category:成都市の鉄道駅   ja body_ja    0.932598\n",
      "303197         Category:杭州のスポーツ   ja body_ja    0.930130\n",
      "\n",
      "検証完了\n"
     ]
    }
   ],
   "source": [
    "# 類似検索テスト\n",
    "print('\\n' + '=' * 80)\n",
    "print('類似検索テスト')\n",
    "print('=' * 80)\n",
    "\n",
    "# 日本語本文からランダムなドキュメントをクエリとして使用\n",
    "query_doc = con.execute(\"\"\"\n",
    "    SELECT id, title, embedding FROM documents WHERE dataset = 'body_ja' LIMIT 1\n",
    "\"\"\").fetchone()\n",
    "\n",
    "query_id, query_title, query_emb = query_doc\n",
    "print(f'\\nクエリ: id={query_id}, title=\"{query_title[:40]}...\"')\n",
    "\n",
    "result = con.execute(\"\"\"\n",
    "    SELECT id, title, lang, dataset,\n",
    "           array_cosine_similarity(embedding, ?::FLOAT[1024]) as similarity\n",
    "    FROM documents\n",
    "    WHERE id != ?\n",
    "    ORDER BY similarity DESC\n",
    "    LIMIT 5\n",
    "\"\"\", [list(query_emb), query_id]).fetchdf()\n",
    "\n",
    "print('\\nTop-5 類似結果:')\n",
    "print(result.to_string(index=False))\n",
    "\n",
    "con.close()\n",
    "print('\\n検証完了')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<cell_type>markdown</cell_type>## 検証結果サマリー\n",
    "\n",
    "### 基本情報\n",
    "\n",
    "| 項目 | 値 |\n",
    "|------|-----|\n",
    "| ファイル | `data/experiment_400k.duckdb` |\n",
    "| サイズ | 約6.8 GB |\n",
    "| 総レコード数 | 400,000件 |\n",
    "\n",
    "### データセット別件数\n",
    "\n",
    "| データセット | 言語 | 件数 |\n",
    "|-------------|------|------|\n",
    "| titles_en | en | 100,000 |\n",
    "| titles_ja | ja | 100,000 |\n",
    "| body_en | en | 100,000 |\n",
    "| body_ja | ja | 100,000 |\n",
    "\n",
    "### カラム構造\n",
    "\n",
    "| カラム | 型 | 説明 |\n",
    "|--------|-----|------|\n",
    "| id | INTEGER | 連番ID (0-399,999) |\n",
    "| original_id | INTEGER | 元Wikipedia記事ID |\n",
    "| title | VARCHAR | 記事タイトル |\n",
    "| lang | VARCHAR | 言語 (en/ja) |\n",
    "| dataset | VARCHAR | データセット名 |\n",
    "| text | VARCHAR | 元テキスト |\n",
    "| text_truncated | VARCHAR | 切り詰めテキスト (最大2000文字) |\n",
    "| embedding | FLOAT[1024] | E5-large埋め込み |\n",
    "\n",
    "### インデックス\n",
    "\n",
    "- **HNSW** (`hnsw_idx`) - cosine距離\n",
    "\n",
    "### 検証結果\n",
    "\n",
    "- 類似検索が正常に動作\n",
    "- 関連するドキュメントが上位に返される\n",
    "\n",
    "---\n",
    "\n",
    "**大規模実験の準備完了**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "実験用DuckDB作成完了\n",
      "================================================================================\n",
      "\n",
      "ファイル: ../data/experiment_400k.duckdb\n",
      "サイズ: 6844.8 MB\n",
      "\n",
      "テーブル: documents\n",
      "  - id: 連番ID (0-399,999)\n",
      "  - original_id: 元のWikipedia記事ID\n",
      "  - title: 記事タイトル\n",
      "  - lang: 言語 (en/ja)\n",
      "  - dataset: データセット名\n",
      "  - text: 元テキスト（タイトル+本文）\n",
      "  - text_truncated: 切り詰めテキスト（最大2000文字）\n",
      "  - embedding: E5-large埋め込み (1024次元)\n",
      "\n",
      "インデックス: HNSW (cosine)\n",
      "\n",
      "データ内訳:\n",
      "  - 英語タイトル: 100,000件\n",
      "  - 日本語タイトル: 100,000件\n",
      "  - 英語本文: 100,000件\n",
      "  - 日本語本文: 100,000件\n",
      "  - 合計: 400,000件\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('=' * 80)\n",
    "print('実験用DuckDB作成完了')\n",
    "print('=' * 80)\n",
    "print(f'''\n",
    "ファイル: {DB_PATH}\n",
    "サイズ: {size_mb:.1f} MB\n",
    "\n",
    "テーブル: documents\n",
    "  - id: 連番ID (0-399,999)\n",
    "  - original_id: 元のWikipedia記事ID\n",
    "  - title: 記事タイトル\n",
    "  - lang: 言語 (en/ja)\n",
    "  - dataset: データセット名\n",
    "  - text: 元テキスト（タイトル+本文）\n",
    "  - text_truncated: 切り詰めテキスト（最大{MAX_CHARS}文字）\n",
    "  - embedding: E5-large埋め込み (1024次元)\n",
    "\n",
    "インデックス: HNSW (cosine)\n",
    "\n",
    "データ内訳:\n",
    "  - 英語タイトル: 100,000件\n",
    "  - 日本語タイトル: 100,000件\n",
    "  - 英語本文: 100,000件\n",
    "  - 日本語本文: 100,000件\n",
    "  - 合計: 400,000件\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsh-cascade-poc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
