{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSH Cascade Search - 基本検証\n",
    "\n",
    "このノートブックでは、LSH (Locality Sensitive Hashing) を用いた3段階フィルタリング検索の基本動作を検証します。\n",
    "\n",
    "## 目次\n",
    "1. データの読み込みと確認\n",
    "2. SimHashの動作確認\n",
    "3. HNSW検索 vs LSH Cascade検索\n",
    "4. パラメータ比較（LSH-4, LSH-8, LSH-16）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from src.lsh import SimHashGenerator, chunk_hash, hamming_distance\n",
    "from src.db import VectorDatabase\n",
    "from src.pipeline import LSHCascadeSearcher, HNSWSearcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. データの読み込みと確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 10,000\n",
      "\n",
      "=== テーブルスキーマ ===\n",
      "column_name column_type null key default extra\n",
      "         id     INTEGER   NO PRI    None  None\n",
      "       text     VARCHAR  YES NaN    None  None\n",
      "     vector FLOAT[1024]  YES NaN    None  None\n",
      "    simhash     VARCHAR  YES NaN    None  None\n",
      " lsh_chunks   VARCHAR[]  YES NaN    None  None\n"
     ]
    }
   ],
   "source": [
    "# DuckDBからデータを読み込み\n",
    "db_path = Path('../data/sample_vectors_v2.duckdb')  # 手元のWikipedaデータ（１万件）v2: 本文データ\n",
    "db = VectorDatabase(db_path=db_path)\n",
    "db.initialize()\n",
    "\n",
    "print(f'Total documents: {db.count():,}')\n",
    "\n",
    "# カラム情報を表示\n",
    "print('\\n=== テーブルスキーマ ===')\n",
    "schema = db.conn.execute('DESCRIBE documents').fetchdf()\n",
    "print(schema.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>vector</th>\n",
       "      <th>simhash</th>\n",
       "      <th>lsh_chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>{{中華圏の事物\\n| 画像=[[File:People's Literature Publ...</td>\n",
       "      <td>[-0.004686727, -0.01291483, -0.015869895, -0.0...</td>\n",
       "      <td>BB8A8C209B56AA58CD0385D188E61C1F</td>\n",
       "      <td>[c0_BB, c1_8A, c2_8C, c3_20, c4_9B, c5_56, c6_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>モレロス州&lt;noinclude&gt;\\n[[Category:Country aliasテンプレ...</td>\n",
       "      <td>[0.037693564, 0.014720356, -0.008648348, -0.06...</td>\n",
       "      <td>BBB9CE281B56AAF8CB1385598A960E1B</td>\n",
       "      <td>[c0_BB, c1_B9, c2_CE, c3_28, c4_1B, c5_56, c6_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>{{Infobox Musician\\n| Name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;= 碧井...</td>\n",
       "      <td>[0.027235618, 0.050126687, -0.013588801, -0.06...</td>\n",
       "      <td>B91B8C268316BB7ACA138FDBCA8C081B</td>\n",
       "      <td>[c0_B9, c1_1B, c2_8C, c3_26, c4_83, c5_16, c6_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  \\\n",
       "0   0  {{中華圏の事物\\n| 画像=[[File:People's Literature Publ...   \n",
       "1   1  モレロス州<noinclude>\\n[[Category:Country aliasテンプレ...   \n",
       "2   2  {{Infobox Musician\\n| Name                = 碧井...   \n",
       "\n",
       "                                              vector  \\\n",
       "0  [-0.004686727, -0.01291483, -0.015869895, -0.0...   \n",
       "1  [0.037693564, 0.014720356, -0.008648348, -0.06...   \n",
       "2  [0.027235618, 0.050126687, -0.013588801, -0.06...   \n",
       "\n",
       "                            simhash  \\\n",
       "0  BB8A8C209B56AA58CD0385D188E61C1F   \n",
       "1  BBB9CE281B56AAF8CB1385598A960E1B   \n",
       "2  B91B8C268316BB7ACA138FDBCA8C081B   \n",
       "\n",
       "                                          lsh_chunks  \n",
       "0  [c0_BB, c1_8A, c2_8C, c3_20, c4_9B, c5_56, c6_...  \n",
       "1  [c0_BB, c1_B9, c2_CE, c3_28, c4_1B, c5_56, c6_...  \n",
       "2  [c0_B9, c1_1B, c2_8C, c3_26, c4_83, c5_16, c6_...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# サンプルデータを確認\n",
    "df_sample = db.get_by_ids([0, 1, 2])\n",
    "df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # サンプルデータの確認（全カラム表示）\n",
    "# df_sample = db.get_by_ids([0, 1, 2])\n",
    "# print(df_sample)\n",
    "# print(\"=====\")\n",
    "\n",
    "# print('=== サンプルデータ (3行) ===')\n",
    "# for i, row in df_sample.iterrows():\n",
    "#     print(f'\\n--- Row {row[\"id\"]} ---')\n",
    "#     print(f'id: {row[\"id\"]}')\n",
    "#     print(f'text: {row[\"text\"]}')\n",
    "#     vec = row[\"vector\"]\n",
    "#     print(f'vector: [{vec[0]:.6f}, {vec[1]:.6f}, ...] (len={len(vec)})')\n",
    "#     print(f'simhash: {row[\"simhash\"]}')\n",
    "#     chunks = row[\"lsh_chunks\"]\n",
    "#     print(f'lsh_chunks: {chunks[:4]}... (len={len(chunks)})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SimHashの動作確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector shape: (1024,)\n",
      "SimHash (int): 249285014298977734719250992799632727071\n",
      "SimHash (hex): BB8A8C209B56AA58CD0385D188E61C1F\n"
     ]
    }
   ],
   "source": [
    "# SimHashGeneratorの初期化\n",
    "simhash_gen = SimHashGenerator(dim=1024, hash_bits=128, seed=42)\n",
    "\n",
    "# サンプルベクトルでSimHashを生成\n",
    "sample_vec = np.array(df_sample.iloc[0]['vector'], dtype=np.float32)\n",
    "sample_hash = simhash_gen.hash(sample_vec)\n",
    "\n",
    "print(f'Vector shape: {sample_vec.shape}')\n",
    "print(f'SimHash (int): {sample_hash}')\n",
    "print(f'SimHash (hex): {sample_hash:032X}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSH-4: ['c0_BB8A8C20', 'c1_9B56AA58', 'c2_CD0385D1', 'c3_88E61C1F']... (total 4 chunks)\n",
      "LSH-8: ['c0_BB8A', 'c1_8C20', 'c2_9B56', 'c3_AA58']... (total 8 chunks)\n",
      "LSH-16: ['c0_BB', 'c1_8A', 'c2_8C', 'c3_20']... (total 16 chunks)\n"
     ]
    }
   ],
   "source": [
    "# チャンク分割の確認\n",
    "for num_chunks in [4, 8, 16]:\n",
    "    chunks = chunk_hash(sample_hash, num_chunks)\n",
    "    print(f'LSH-{num_chunks}: {chunks[:4]}... (total {len(chunks)} chunks)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming distance: 22 / 128 bits\n",
      "Cosine similarity: 0.7644\n"
     ]
    }
   ],
   "source": [
    "# ハミング距離の確認\n",
    "vec1 = np.array(df_sample.iloc[0]['vector'], dtype=np.float32)\n",
    "vec2 = np.array(df_sample.iloc[1]['vector'], dtype=np.float32)\n",
    "\n",
    "hash1 = simhash_gen.hash(vec1)\n",
    "hash2 = simhash_gen.hash(vec2)\n",
    "\n",
    "dist = hamming_distance(hash1, hash2)\n",
    "cosine_sim = np.dot(vec1, vec2)\n",
    "\n",
    "print(f'Hamming distance: {dist} / 128 bits')\n",
    "print(f'Cosine similarity: {cosine_sim:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. HNSW検索 vs LSH Cascade検索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 検索器の初期化\n",
    "hnsw_searcher = HNSWSearcher(db)\n",
    "lsh_searcher = LSHCascadeSearcher(\n",
    "    db=db,\n",
    "    simhash_generator=simhash_gen,\n",
    "    num_chunks=16,\n",
    "    # step2_top_n=100,\n",
    "    step2_top_n=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== HNSW Results ===\n",
      "Time: 125.73 ms\n",
      "  id=0, score=1.0000, text={{中華圏の事物\n",
      "| 画像=[[File:People's ...\n",
      "  id=9283, score=0.9133, text={{出典の明記|date=2018年4月17日 (火) 06...\n",
      "  id=3671, score=0.9106, text={{中華圏の事物\n",
      "| 画像=[[ファイル:人民教育出版社 -...\n",
      "  id=6035, score=0.9081, text={{中華圏の事物\n",
      "| 画像=\n",
      "| 画像の説明=\n",
      "| 英文=\n",
      "...\n",
      "  id=9438, score=0.9081, text={{中華圏の事物\n",
      "| 画像=\n",
      "| 画像の説明=\n",
      "| 英文=\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# クエリベクトル（最初のドキュメントを使用）\n",
    "query_vec = np.array(df_sample.iloc[0]['vector'], dtype=np.float32)\n",
    "\n",
    "# HNSW検索\n",
    "hnsw_results, hnsw_time = hnsw_searcher.search(query_vec, top_k=10)\n",
    "print('=== HNSW Results ===')\n",
    "print(f'Time: {hnsw_time:.2f} ms')\n",
    "for r in hnsw_results[:5]:\n",
    "    print(f'  id={r.id}, score={r.score:.4f}, text={r.text[:30]}...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LSH Cascade Results ===\n",
      "Total time: 410.71 ms\n",
      "Step1 candidates: 9496\n",
      "Step2 candidates: 500\n",
      "\n",
      "  id=0, score=1.0000, text={{中華圏の事物\n",
      "| 画像=[[File:People's ...\n",
      "  id=9283, score=0.9133, text={{出典の明記|date=2018年4月17日 (火) 06...\n",
      "  id=3671, score=0.9106, text={{中華圏の事物\n",
      "| 画像=[[ファイル:人民教育出版社 -...\n",
      "  id=9438, score=0.9081, text={{中華圏の事物\n",
      "| 画像=\n",
      "| 画像の説明=\n",
      "| 英文=\n",
      "...\n",
      "  id=5861, score=0.9080, text={{中華圏の事物\n",
      "| 画像=\n",
      "| 画像の説明=\n",
      "| 英文=\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# LSH Cascade検索\n",
    "lsh_results, lsh_metrics = lsh_searcher.search(query_vec, top_k=10)\n",
    "print('=== LSH Cascade Results ===')\n",
    "print(f'Total time: {lsh_metrics.total_time_ms:.2f} ms')\n",
    "print(f'Step1 candidates: {lsh_metrics.step1_candidates}')\n",
    "print(f'Step2 candidates: {lsh_metrics.step2_candidates}')\n",
    "print()\n",
    "for r in lsh_results[:5]:\n",
    "    print(f'  id={r.id}, score={r.score:.4f}, text={r.text[:30]}...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@10: 0.60\n"
     ]
    }
   ],
   "source": [
    "# Recall計算\n",
    "hnsw_ids = set(r.id for r in hnsw_results)\n",
    "lsh_ids = set(r.id for r in lsh_results)\n",
    "recall = len(hnsw_ids & lsh_ids) / len(hnsw_ids)\n",
    "\n",
    "print(f'Recall@10: {recall:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total docs: 10000\n",
      "Step1 candidates: 9496\n",
      "Step1 reduction: 95.0%\n"
     ]
    }
   ],
   "source": [
    "print(f'Total docs: {lsh_metrics.total_docs}')\n",
    "print(f'Step1 candidates: {lsh_metrics.step1_candidates}')\n",
    "print(f'Step1 reduction: {lsh_metrics.step1_candidates / lsh_metrics.total_docs * 100:.1f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ハミング距離とコサイン類似度の相関係数: -0.364\n",
      "\n",
      "=== HNSW正解10件のハミング距離順位 ===\n",
      "id=0: ハミング距離=0.0, 順位=1.0/9496, コサイン類似度=1.0000\n",
      "id=7680: ハミング距離=11.0, 順位=2.0/9496, コサイン類似度=0.9048\n",
      "id=7585: ハミング距離=20.0, 順位=552.0/9496, コサイン類似度=0.8889\n",
      "id=9283: ハミング距離=15.0, 順位=13.0/9496, コサイン類似度=0.9133\n",
      "id=4064: ハミング距離=22.0, 順位=1646.0/9496, コサイン類似度=0.8856\n",
      "id=5861: ハミング距離=18.0, 順位=154.0/9496, コサイン類似度=0.9080\n",
      "id=6035: ハミング距離=20.0, 順位=709.0/9496, コサイン類似度=0.9081\n",
      "id=3671: ハミング距離=13.0, 順位=4.0/9496, コサイン類似度=0.9106\n",
      "id=2459: ハミング距離=24.0, 順位=3284.0/9496, コサイン類似度=0.8835\n",
      "id=9438: ハミング距離=17.0, 順位=78.0/9496, コサイン類似度=0.9081\n"
     ]
    }
   ],
   "source": [
    "# Step1の候補に対して分析\n",
    "query_hash = simhash_gen.hash(query_vec)\n",
    "query_chunks = chunk_hash(query_hash, 16)\n",
    "candidates = db.search_lsh_chunks(query_chunks)\n",
    "\n",
    "results = []\n",
    "for _, row in candidates.iterrows():\n",
    "    doc_hash = int(row['simhash'], 16)\n",
    "    doc_vec = np.array(row['vector'], dtype=np.float32)\n",
    "    \n",
    "    ham_dist = hamming_distance(query_hash, doc_hash)\n",
    "    cos_sim = np.dot(query_vec, doc_vec)\n",
    "    \n",
    "    results.append({\n",
    "        'id': row['id'],\n",
    "        'hamming_dist': ham_dist,\n",
    "        'cosine_sim': cos_sim,\n",
    "    })\n",
    "\n",
    "df_analysis = pd.DataFrame(results)\n",
    "\n",
    "# 相関係数\n",
    "corr = df_analysis['hamming_dist'].corr(df_analysis['cosine_sim'])\n",
    "print(f'ハミング距離とコサイン類似度の相関係数: {corr:.3f}')\n",
    "\n",
    "# HNSW正解のハミング距離順位を確認\n",
    "df_sorted_ham = df_analysis.sort_values('hamming_dist')\n",
    "df_sorted_ham['ham_rank'] = range(1, len(df_sorted_ham) + 1)\n",
    "\n",
    "print('\\n=== HNSW正解10件のハミング距離順位 ===')\n",
    "# hnsw_resultsから動的に取得（セル12で定義済み）\n",
    "hnsw_ids = set(r.id for r in hnsw_results)\n",
    "for doc_id in hnsw_ids:\n",
    "    if doc_id in df_sorted_ham['id'].values:\n",
    "        row = df_sorted_ham[df_sorted_ham['id'] == doc_id].iloc[0]\n",
    "        print(f'id={doc_id}: ハミング距離={row[\"hamming_dist\"]}, 順位={row[\"ham_rank\"]}/{len(df_sorted_ham)}, コサイン類似度={row[\"cosine_sim\"]:.4f}')\n",
    "    else:\n",
    "        print(f'id={doc_id}: Step1候補に含まれていない')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HNSW結果のID: {0, 7680, 7585, 9283, 4064, 5861, 6035, 3671, 2459, 9438}\n",
      "Step1候補に含まれるHNSW正解: 10/10\n",
      "含まれているID: {0, 7680, 7585, 9283, 4064, 5861, 6035, 3671, 2459, 9438}\n",
      "含まれていないID: set()\n"
     ]
    }
   ],
   "source": [
    "# HNSWの正解ID\n",
    "hnsw_ids = set(r.id for r in hnsw_results)\n",
    "print(f'HNSW結果のID: {hnsw_ids}')\n",
    "\n",
    "# Step1の候補ID\n",
    "query_chunks = chunk_hash(simhash_gen.hash(query_vec), 16)\n",
    "candidates = db.search_lsh_chunks(query_chunks)\n",
    "candidate_ids = set(candidates['id'])\n",
    "\n",
    "# 重複確認\n",
    "overlap = hnsw_ids & candidate_ids\n",
    "print(f'Step1候補に含まれるHNSW正解: {len(overlap)}/10')\n",
    "print(f'含まれているID: {overlap}')\n",
    "print(f'含まれていないID: {hnsw_ids - candidate_ids}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Query ID: 857, Text: {{基礎情報 アナウンサー\n",
      "|名前=翠川 秋子\n",
      "|ふりがな=...\n",
      "==================================================\n",
      "Step1候補数: 8783\n",
      "相関係数: -0.321\n",
      "HNSW正解がStep1に含まれる: 10/10\n",
      "HNSW正解がTop100に含まれる: 2/10\n",
      "HNSW正解がTop500に含まれる: 4/10\n",
      "\n",
      "==================================================\n",
      "Query ID: 4385, Text: {{Infobox military conflict\n",
      "| ...\n",
      "==================================================\n",
      "Step1候補数: 8990\n",
      "相関係数: -0.400\n",
      "HNSW正解がStep1に含まれる: 9/10\n",
      "HNSW正解がTop100に含まれる: 4/10\n",
      "HNSW正解がTop500に含まれる: 6/10\n",
      "\n",
      "==================================================\n",
      "Query ID: 1428, Text: Flag of Queretaro.svg<noinclud...\n",
      "==================================================\n",
      "Step1候補数: 9531\n",
      "相関係数: -0.655\n",
      "HNSW正解がStep1に含まれる: 10/10\n",
      "HNSW正解がTop100に含まれる: 8/10\n",
      "HNSW正解がTop500に含まれる: 10/10\n",
      "\n",
      "==================================================\n",
      "Query ID: 6672, Text: {{軍隊資料\n",
      "|名称 = 第33装甲旅団\n",
      "|画像 = [[F...\n",
      "==================================================\n",
      "Step1候補数: 8941\n",
      "相関係数: -0.408\n",
      "HNSW正解がStep1に含まれる: 10/10\n",
      "HNSW正解がTop100に含まれる: 5/10\n",
      "HNSW正解がTop500に含まれる: 9/10\n",
      "\n",
      "==================================================\n",
      "Query ID: 4367, Text: {{Pathnav|主題別分類|・・|日本の不動産業|日本の...\n",
      "==================================================\n",
      "Step1候補数: 9210\n",
      "相関係数: -0.392\n",
      "HNSW正解がStep1に含まれる: 10/10\n",
      "HNSW正解がTop100に含まれる: 3/10\n",
      "HNSW正解がTop500に含まれる: 4/10\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# ランダムに5件のクエリを選択\n",
    "random.seed(123)\n",
    "all_docs = db.get_all()\n",
    "query_indices = random.sample(range(len(all_docs)), 5)\n",
    "\n",
    "for idx in query_indices:\n",
    "    query_row = all_docs.iloc[idx]\n",
    "    query_vec = np.array(query_row['vector'], dtype=np.float32)\n",
    "    query_id = query_row['id']\n",
    "    \n",
    "    print(f'\\n{\"=\"*50}')\n",
    "    print(f'Query ID: {query_id}, Text: {query_row[\"text\"][:30]}...')\n",
    "    print(f'{\"=\"*50}')\n",
    "    \n",
    "    # HNSW検索\n",
    "    hnsw_results, _ = hnsw_searcher.search(query_vec, top_k=10)\n",
    "    hnsw_ids = set(r.id for r in hnsw_results)\n",
    "    \n",
    "    # Step1候補取得\n",
    "    query_hash = simhash_gen.hash(query_vec)\n",
    "    query_chunks = chunk_hash(query_hash, 16)\n",
    "    candidates = db.search_lsh_chunks(query_chunks)\n",
    "    \n",
    "    # 相関分析\n",
    "    results = []\n",
    "    for _, row in candidates.iterrows():\n",
    "        doc_hash = int(row['simhash'], 16)\n",
    "        doc_vec = np.array(row['vector'], dtype=np.float32)\n",
    "        ham_dist = hamming_distance(query_hash, doc_hash)\n",
    "        cos_sim = np.dot(query_vec, doc_vec)\n",
    "        results.append({'id': row['id'], 'hamming_dist': ham_dist, 'cosine_sim': cos_sim})\n",
    "    \n",
    "    df_analysis = pd.DataFrame(results)\n",
    "    corr = df_analysis['hamming_dist'].corr(df_analysis['cosine_sim'])\n",
    "    \n",
    "    # HNSW正解の順位確認\n",
    "    df_sorted = df_analysis.sort_values('hamming_dist')\n",
    "    df_sorted['ham_rank'] = range(1, len(df_sorted) + 1)\n",
    "    \n",
    "    ranks_in_100 = 0\n",
    "    ranks_in_500 = 0\n",
    "    for doc_id in hnsw_ids:\n",
    "        if doc_id in df_sorted['id'].values:\n",
    "            rank = df_sorted[df_sorted['id'] == doc_id]['ham_rank'].values[0]\n",
    "            if rank <= 100:\n",
    "                ranks_in_100 += 1\n",
    "            if rank <= 500:\n",
    "                ranks_in_500 += 1\n",
    "    \n",
    "    print(f'Step1候補数: {len(candidates)}')\n",
    "    print(f'相関係数: {corr:.3f}')\n",
    "    print(f'HNSW正解がStep1に含まれる: {len(hnsw_ids & set(candidates[\"id\"]))}/10')\n",
    "    print(f'HNSW正解がTop100に含まれる: {ranks_in_100}/10')\n",
    "    print(f'HNSW正解がTop500に含まれる: {ranks_in_500}/10')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 0, Text: {{中華圏の事物\n",
      "| 画像=[[File:People's ...\n",
      "\n",
      "Seed=42: 相関係数=-0.366, Top100=5/10, Top500=6/10\n",
      "Seed=123: 相関係数=-0.362, Top100=4/10, Top500=5/10\n",
      "Seed=456: 相関係数=-0.296, Top100=6/10, Top500=9/10\n",
      "Seed=789: 相関係数=-0.354, Top100=3/10, Top500=5/10\n",
      "Seed=1000: 相関係数=-0.393, Top100=8/10, Top500=10/10\n"
     ]
    }
   ],
   "source": [
    "# 複数のシード値で比較\n",
    "seeds = [42, 123, 456, 789, 1000]\n",
    "\n",
    "# 固定のクエリを使用（最初のドキュメント）\n",
    "query_row = all_docs.iloc[0]\n",
    "query_vec = np.array(query_row['vector'], dtype=np.float32)\n",
    "\n",
    "print(f'Query ID: {query_row[\"id\"]}, Text: {query_row[\"text\"][:30]}...')\n",
    "print()\n",
    "\n",
    "for seed in seeds:\n",
    "    # 新しいシードでSimHashGeneratorを作成\n",
    "    test_gen = SimHashGenerator(dim=1024, hash_bits=128, seed=seed)\n",
    "    \n",
    "    # クエリのハッシュとチャンク\n",
    "    query_hash = test_gen.hash(query_vec)\n",
    "    \n",
    "    # 全ドキュメントのハッシュを再計算して分析\n",
    "    results = []\n",
    "    for _, row in all_docs.iterrows():\n",
    "        doc_vec = np.array(row['vector'], dtype=np.float32)\n",
    "        doc_hash = test_gen.hash(doc_vec)\n",
    "        \n",
    "        ham_dist = hamming_distance(query_hash, doc_hash)\n",
    "        cos_sim = np.dot(query_vec, doc_vec)\n",
    "        \n",
    "        results.append({\n",
    "            'id': row['id'],\n",
    "            'hamming_dist': ham_dist,\n",
    "            'cosine_sim': cos_sim,\n",
    "        })\n",
    "    \n",
    "    df_analysis = pd.DataFrame(results)\n",
    "    corr = df_analysis['hamming_dist'].corr(df_analysis['cosine_sim'])\n",
    "    \n",
    "    # HNSW正解の順位確認\n",
    "    hnsw_results, _ = hnsw_searcher.search(query_vec, top_k=10)\n",
    "    hnsw_ids = set(r.id for r in hnsw_results)\n",
    "    \n",
    "    df_sorted = df_analysis.sort_values('hamming_dist')\n",
    "    df_sorted['ham_rank'] = range(1, len(df_sorted) + 1)\n",
    "    \n",
    "    ranks_in_100 = sum(1 for doc_id in hnsw_ids \n",
    "                       if df_sorted[df_sorted['id'] == doc_id]['ham_rank'].values[0] <= 100)\n",
    "    ranks_in_500 = sum(1 for doc_id in hnsw_ids \n",
    "                       if df_sorted[df_sorted['id'] == doc_id]['ham_rank'].values[0] <= 500)\n",
    "    \n",
    "    print(f'Seed={seed}: 相関係数={corr:.3f}, Top100={ranks_in_100}/10, Top500={ranks_in_500}/10')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. パラメータ比較（LSH-4, LSH-8, LSH-16）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunks</th>\n",
       "      <th>bits_per_chunk</th>\n",
       "      <th>avg_recall</th>\n",
       "      <th>avg_latency_ms</th>\n",
       "      <th>avg_candidates</th>\n",
       "      <th>reduction_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.577625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.625032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>0.48</td>\n",
       "      <td>311.887399</td>\n",
       "      <td>8971.6</td>\n",
       "      <td>0.10284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chunks  bits_per_chunk  avg_recall  avg_latency_ms  avg_candidates  \\\n",
       "0       4              32        0.00        9.577625             0.0   \n",
       "1       8              16        0.00       11.625032             0.0   \n",
       "2      16               8        0.48      311.887399          8971.6   \n",
       "\n",
       "   reduction_rate  \n",
       "0         1.00000  \n",
       "1         1.00000  \n",
       "2         0.10284  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 複数クエリでの比較\n",
    "import random\n",
    "\n",
    "# ランダムに10件のクエリを選択\n",
    "random.seed(42)\n",
    "all_docs = db.get_all()\n",
    "query_indices = random.sample(range(len(all_docs)), 10)\n",
    "\n",
    "results_summary = []\n",
    "\n",
    "for num_chunks in [4, 8, 16]:\n",
    "    searcher = LSHCascadeSearcher(\n",
    "        db=db,\n",
    "        simhash_generator=simhash_gen,\n",
    "        num_chunks=num_chunks,\n",
    "        step2_top_n=100,\n",
    "    )\n",
    "    \n",
    "    recalls = []\n",
    "    latencies = []\n",
    "    candidates = []\n",
    "    \n",
    "    for idx in query_indices:\n",
    "        query_vec = np.array(all_docs.iloc[idx]['vector'], dtype=np.float32)\n",
    "        \n",
    "        # HNSW baseline\n",
    "        hnsw_results, _ = hnsw_searcher.search(query_vec, top_k=10)\n",
    "        hnsw_ids = set(r.id for r in hnsw_results)\n",
    "        \n",
    "        # LSH search\n",
    "        lsh_results, metrics = searcher.search(query_vec, top_k=10)\n",
    "        lsh_ids = set(r.id for r in lsh_results)\n",
    "        \n",
    "        recall = len(hnsw_ids & lsh_ids) / len(hnsw_ids) if hnsw_ids else 0\n",
    "        recalls.append(recall)\n",
    "        latencies.append(metrics.total_time_ms)\n",
    "        candidates.append(metrics.step1_candidates)\n",
    "    \n",
    "    results_summary.append({\n",
    "        'chunks': num_chunks,\n",
    "        'bits_per_chunk': 128 // num_chunks,\n",
    "        'avg_recall': np.mean(recalls),\n",
    "        'avg_latency_ms': np.mean(latencies),\n",
    "        'avg_candidates': np.mean(candidates),\n",
    "        'reduction_rate': 1 - np.mean(candidates) / len(all_docs),\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results_summary)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 0\n",
      "Query SimHash: BB8A8C209B56AA58CD0385D188E61C1F\n",
      "\n",
      "=== 4チャンク ===\n",
      "Query chunks: {'c0_BB8A8C20', 'c3_88E61C1F', 'c2_CD0385D1', 'c1_9B56AA58'}\n",
      "Doc chunks: {'c0_BB8A8C20', 'c3_88E61C1F', 'c2_CD0385D1', 'c1_9B56AA58'}\n",
      "一致: {'c0_BB8A8C20', 'c3_88E61C1F', 'c2_CD0385D1', 'c1_9B56AA58'}\n",
      "最初の100件中のマッチ数: 1\n",
      "\n",
      "=== 8チャンク ===\n",
      "Query chunks: {'c0_BB8A', 'c6_88E6', 'c5_85D1', 'c2_9B56', 'c3_AA58', 'c1_8C20', 'c4_CD03', 'c7_1C1F'}\n",
      "Doc chunks: {'c0_BB8A', 'c6_88E6', 'c5_85D1', 'c2_9B56', 'c3_AA58', 'c1_8C20', 'c4_CD03', 'c7_1C1F'}\n",
      "一致: {'c0_BB8A', 'c6_88E6', 'c5_85D1', 'c2_9B56', 'c3_AA58', 'c1_8C20', 'c4_CD03', 'c7_1C1F'}\n",
      "最初の100件中のマッチ数: 22\n",
      "\n",
      "=== 16チャンク ===\n",
      "Query chunks: {'c5_56', 'c0_BB', 'c4_9B', 'c12_88', 'c13_E6', 'c1_8A', 'c11_D1', 'c7_58', 'c14_1C', 'c15_1F', 'c9_03', 'c8_CD', 'c3_20', 'c2_8C', 'c10_85', 'c6_AA'}\n",
      "Doc chunks: {'c5_56', 'c0_BB', 'c4_9B', 'c12_88', 'c13_E6', 'c1_8A', 'c11_D1', 'c7_58', 'c14_1C', 'c15_1F', 'c9_03', 'c8_CD', 'c3_20', 'c2_8C', 'c10_85', 'c6_AA'}\n",
      "一致: {'c5_56', 'c0_BB', 'c13_E6', 'c4_9B', 'c12_88', 'c1_8A', 'c11_D1', 'c7_58', 'c14_1C', 'c15_1F', 'c2_8C', 'c8_CD', 'c3_20', 'c9_03', 'c10_85', 'c6_AA'}\n",
      "最初の100件中のマッチ数: 94\n"
     ]
    }
   ],
   "source": [
    "# デバッグ: 単一クエリで4チャンクのマッチングを確認\n",
    "query_idx = 0\n",
    "query_vec = np.array(all_docs.iloc[query_idx]['vector'], dtype=np.float32)\n",
    "query_hash = simhash_gen.hash(query_vec)\n",
    "\n",
    "print(f'Query ID: {all_docs.iloc[query_idx][\"id\"]}')\n",
    "print(f'Query SimHash: {query_hash:032X}')\n",
    "\n",
    "for num_chunks in [4, 8, 16]:\n",
    "    query_chunks = set(chunk_hash(query_hash, num_chunks))\n",
    "    print(f'\\n=== {num_chunks}チャンク ===')\n",
    "    print(f'Query chunks: {query_chunks}')\n",
    "    \n",
    "    # 自分自身とマッチするか確認\n",
    "    doc_hash = int(all_docs.iloc[query_idx]['simhash'], 16)\n",
    "    doc_chunks = set(chunk_hash(doc_hash, num_chunks))\n",
    "    print(f'Doc chunks: {doc_chunks}')\n",
    "    print(f'一致: {query_chunks & doc_chunks}')\n",
    "    \n",
    "    # 候補数をカウント\n",
    "    match_count = 0\n",
    "    for _, row in all_docs.head(100).iterrows():\n",
    "        doc_hash = int(row['simhash'], 16)\n",
    "        doc_chunks = set(chunk_hash(doc_hash, num_chunks))\n",
    "        if query_chunks & doc_chunks:\n",
    "            match_count += 1\n",
    "    print(f'最初の100件中のマッチ数: {match_count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# データベース接続を閉じる\n",
    "db.close()\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. v2データセット評価レポート\n\n### 主要結果\n\n| 指標 | 値 | 評価 |\n|------|-----|------|\n| **Recall@10** | 0.60 | 改善の余地あり |\n| **相関係数** | -0.364 | 弱い（理論値は-1.0に近いはず） |\n| **Step1正解含有率** | 10/10 | 良好 |\n| **Step1削減率** | 5%のみ（9496/10000残存） | 課題 |\n\n### 根本的な問題\n\n**高次元空間でのコサイン類似度の集中現象**\n- ランダムペアの類似度: 平均0.775、標準偏差0.030\n- 全データが0.65〜1.0の狭い範囲に集中\n- この狭い範囲での区別にSimHash 128bitでは不十分\n\n### 良い点\n\n1. **Step1で正解が漏れていない**（10/10含有）\n\n2. **一部のクエリは高性能**\n   - Query 1428 (Template): Top500に10/10含有、相関 -0.655\n   - Query 6672 (装甲旅団): Top500に9/10含有\n\n### 課題\n\n1. **ハミング距離順位のばらつき**\n   ```\n   id=3671: 順位4    → 良好\n   id=2459: 順位3284 → 問題\n   id=4064: 順位1646 → 問題\n   ```\n\n2. **シードによる変動が大きい**\n   ```\n   Seed=1000: Top500=10/10\n   Seed=789:  Top500=5/10\n   ```\n\n3. **ビット数を増やしても限界がある**（02_v2_lsh_accuracy_analysis.ipynbより）\n   - 128bit: -0.366\n   - 256bit: -0.514\n   - 512bit: -0.566（頭打ち傾向）\n\n### 次のステップ候補\n\n| 優先度 | アプローチ | 期待効果 |\n|--------|-----------|---------|\n| 1 | `step2_top_n`を1000〜2000に増加 | Recall向上（簡単だが対症療法） |\n| 2 | 複数シードのアンサンブル | 安定性向上 |\n| 3 | ハッシュビット数256bitに増加 | 相関 -0.364 → -0.514 程度 |\n| - | **根本解決**: 異なるLSH手法の検討 | 要調査 |"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsh-cascade-poc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}