{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 48. Wikipedia 400k 埋め込みデータ生成 & DuckDB永続化\n",
    "\n",
    "## 目的\n",
    "- 40万件のWikipediaデータをsentence-transformers (GPU) で埋め込み生成\n",
    "- DuckDBにHNSWインデックス付きで永続化\n",
    "- 今後の大規模評価実験の基盤データとして活用\n",
    "\n",
    "## 使用モデル\n",
    "- `intfloat/multilingual-e5-base` (768次元)\n",
    "- sentence-transformers経由でGPU推論（高速）\n",
    "\n",
    "## 出力ファイル\n",
    "- `data/wikipedia_400k_e5_base.duckdb` - 埋め込みベクトル + HNSWインデックス\n",
    "- `data/wikipedia_400k_e5_base_embeddings.npy` - 埋め込みベクトル（ITQ学習用）\n",
    "- `data/wikipedia_400k_e5_base_meta.npz` - メタデータ（タイトル、ドキュメントID）\n",
    "\n",
    "**注**: ファイル名にモデル名 `e5_base` を含めることで、既存の `experiment_400k.duckdb` (E5-large, 1024次元) と区別しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. セットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.10.0+cu128\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 4090\n",
      "GPU Memory: 23.5 GB\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import duckdb\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# GPU確認\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定\n",
    "MODEL_NAME = \"intfloat/multilingual-e5-base\"\n",
    "EMBEDDING_DIM = 768\n",
    "TARGET_COUNT = 400_000\n",
    "BATCH_SIZE = 64  # GPU用バッチサイズ\n",
    "\n",
    "# 出力パス（モデル名を含めて既存データと区別）\n",
    "DATA_DIR = Path(\"../data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "OUTPUT_DB = DATA_DIR / \"wikipedia_400k_e5_base.duckdb\"\n",
    "\n",
    "print(f\"Target: {TARGET_COUNT:,} documents\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Output: {OUTPUT_DB}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Wikipediaデータ取得\n",
    "\n",
    "HuggingFace datasetsからWikipediaデータを取得します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'wikimedia/wikipedia' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Wikipedia Japanese dataset (streaming)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "071212b6c16249b0a4eb65104ff064ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded in 3.8s\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Wikipedia日本語データをストリーミングで読み込み\n",
    "print(\"Loading Wikipedia Japanese dataset (streaming)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# 日本語Wikipedia\n",
    "wiki_ja = load_dataset(\n",
    "    \"wikimedia/wikipedia\",\n",
    "    \"20231101.ja\",\n",
    "    split=\"train\",\n",
    "    streaming=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(f\"Dataset loaded in {time.time() - start_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting 400,000 documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting:  28%|██▊       | 112559/400000 [05:23<02:55, 1640.10it/s]'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: a52ab705-7cc9-4d71-93fb-113b1804311a)')' thrown while requesting GET https://huggingface.co/datasets/wikimedia/wikipedia/resolve/b04c8d1ceb2f5cd4588862100d08de323dccfbaa/20231101.ja/train-00001-of-00015.parquet\n",
      "Retrying in 1s [Retry 1/5].\n",
      "Collecting: 100%|██████████| 400000/400000 [12:04<00:00, 552.27it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collected 399,029 documents in 724.3s\n",
      "Sample title: アンパサンド\n",
      "Sample text (first 100 chars): passage: アンパサンド（&, ）は、並立助詞「…と…」を意味する記号である。ラテン語で「…と…」を表す接続詞 \"et\" の合字を起源とする。現代のフォントでも、Trebuchet MS など一...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 400k件を収集\n",
    "print(f\"Collecting {TARGET_COUNT:,} documents...\")\n",
    "start_time = time.time()\n",
    "\n",
    "documents = []\n",
    "titles = []\n",
    "doc_ids = []\n",
    "\n",
    "for i, item in enumerate(tqdm(wiki_ja, total=TARGET_COUNT, desc=\"Collecting\")):\n",
    "    if i >= TARGET_COUNT:\n",
    "        break\n",
    "    \n",
    "    # テキストの前処理（最初の500文字程度を使用）\n",
    "    text = item['text'][:500].strip()\n",
    "    if len(text) < 50:  # 短すぎるものはスキップ\n",
    "        continue\n",
    "    \n",
    "    # E5モデル用のプレフィックス\n",
    "    documents.append(f\"passage: {text}\")\n",
    "    titles.append(item['title'])\n",
    "    doc_ids.append(item['id'])\n",
    "\n",
    "print(f\"\\nCollected {len(documents):,} documents in {time.time() - start_time:.1f}s\")\n",
    "print(f\"Sample title: {titles[0]}\")\n",
    "print(f\"Sample text (first 100 chars): {documents[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 埋め込み生成 (sentence-transformers GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: intfloat/multilingual-e5-base\n",
      "Model loaded in 5.5s\n",
      "Device: cuda\n",
      "Embedding dimension: 768\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# モデルロード\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "start_time = time.time()\n",
    "\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded in {time.time() - start_time:.1f}s\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Embedding dimension: {model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings for 399,029 documents...\n",
      "Batch size: 64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "588347ba0ecd49cf8b3c76ffefe2da4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/6235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding generation completed!\n",
      "Time: 941.4s (15.7 min)\n",
      "Speed: 423.9 docs/sec\n",
      "Shape: (399029, 768)\n",
      "Memory: 1.14 GB\n"
     ]
    }
   ],
   "source": [
    "# 埋め込み生成\n",
    "print(f\"\\nGenerating embeddings for {len(documents):,} documents...\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "start_time = time.time()\n",
    "\n",
    "embeddings = model.encode(\n",
    "    documents,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True  # E5は正規化推奨\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nEmbedding generation completed!\")\n",
    "print(f\"Time: {elapsed:.1f}s ({elapsed/60:.1f} min)\")\n",
    "print(f\"Speed: {len(documents)/elapsed:.1f} docs/sec\")\n",
    "print(f\"Shape: {embeddings.shape}\")\n",
    "print(f\"Memory: {embeddings.nbytes / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DuckDB永続化 + HNSWインデックス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VSS extension loaded\n"
     ]
    }
   ],
   "source": [
    "# 既存ファイル削除\n",
    "if OUTPUT_DB.exists():\n",
    "    OUTPUT_DB.unlink()\n",
    "    print(f\"Removed existing: {OUTPUT_DB}\")\n",
    "\n",
    "# DuckDB接続（永続化モード）\n",
    "con = duckdb.connect(str(OUTPUT_DB))\n",
    "\n",
    "# VSS拡張インストール\n",
    "con.execute(\"INSTALL vss;\")\n",
    "con.execute(\"LOAD vss;\")\n",
    "print(\"VSS extension loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating table...\n",
      "Table created\n"
     ]
    }
   ],
   "source": [
    "# テーブル作成\n",
    "print(\"Creating table...\")\n",
    "con.execute(f\"\"\"\n",
    "    CREATE TABLE wikipedia_docs (\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        doc_id VARCHAR,\n",
    "        title VARCHAR,\n",
    "        text VARCHAR,\n",
    "        embedding FLOAT[{EMBEDDING_DIM}]\n",
    "    )\n",
    "\"\"\")\n",
    "print(\"Table created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting 399,029 documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting: 100%|██████████| 40/40 [17:50<00:00, 26.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Insert completed in 1070.5s\n",
      "Total records: 399,029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# データ挿入（バッチで高速化）\n",
    "print(f\"Inserting {len(documents):,} documents...\")\n",
    "start_time = time.time()\n",
    "\n",
    "INSERT_BATCH = 10000\n",
    "for i in tqdm(range(0, len(documents), INSERT_BATCH), desc=\"Inserting\"):\n",
    "    batch_end = min(i + INSERT_BATCH, len(documents))\n",
    "    \n",
    "    # バッチデータ準備\n",
    "    batch_data = [\n",
    "        (j, doc_ids[j], titles[j], documents[j], embeddings[j].tolist())\n",
    "        for j in range(i, batch_end)\n",
    "    ]\n",
    "    \n",
    "    con.executemany(\n",
    "        \"INSERT INTO wikipedia_docs VALUES (?, ?, ?, ?, ?)\",\n",
    "        batch_data\n",
    "    )\n",
    "\n",
    "print(f\"\\nInsert completed in {time.time() - start_time:.1f}s\")\n",
    "\n",
    "# 件数確認\n",
    "count = con.execute(\"SELECT COUNT(*) FROM wikipedia_docs\").fetchone()[0]\n",
    "print(f\"Total records: {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating HNSW index...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d26ee99bf84b588aa84417055764cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HNSW index created in 105.5s\n"
     ]
    }
   ],
   "source": [
    "# HNSWインデックス作成\n",
    "print(\"Creating HNSW index...\")\n",
    "start_time = time.time()\n",
    "con.execute(\"SET hnsw_enable_experimental_persistence = true;\")\n",
    "\n",
    "\n",
    "con.execute(\"\"\"\n",
    "    CREATE INDEX wikipedia_hnsw_idx ON wikipedia_docs \n",
    "    USING HNSW (embedding)\n",
    "    WITH (metric = 'cosine')\n",
    "\"\"\")\n",
    "\n",
    "print(f\"HNSW index created in {time.time() - start_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Database Info ===\n",
      "   cid       name        type  notnull dflt_value     pk\n",
      "0    0         id     INTEGER     True       None   True\n",
      "1    1     doc_id     VARCHAR    False       None  False\n",
      "2    2      title     VARCHAR    False       None  False\n",
      "3    3       text     VARCHAR    False       None  False\n",
      "4    4  embedding  FLOAT[768]    False       None  False\n",
      "\n",
      "=== Indexes ===\n",
      "    database_name  database_oid schema_name  schema_oid          index_name  \\\n",
      "0  wikipedia_400k           592        main         590  wikipedia_hnsw_idx   \n",
      "\n",
      "   index_oid      table_name  table_oid comment tags  is_unique  is_primary  \\\n",
      "0       2012  wikipedia_docs       2006    None   {}      False       False   \n",
      "\n",
      "   expressions                                                sql  \n",
      "0  [embedding]  CREATE INDEX wikipedia_hnsw_idx ON wikipedia_d...  \n"
     ]
    }
   ],
   "source": [
    "# インデックス確認\n",
    "print(\"\\n=== Database Info ===\")\n",
    "print(con.execute(\"PRAGMA table_info('wikipedia_docs')\").fetchdf())\n",
    "print(\"\\n=== Indexes ===\")\n",
    "print(con.execute(\"SELECT * FROM duckdb_indexes()\").fetchdf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 動作確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing HNSW search...\n",
      "Search time: 303.1ms\n",
      "\n",
      "Top 10 results:\n",
      "       id   title  similarity\n",
      "0  249275    日本文明    0.865258\n",
      "1   77237   歴史の一覧    0.859950\n",
      "2    3446   1689年    0.859729\n",
      "3   37551    340年    0.859658\n",
      "4   26196    日本人論    0.859130\n",
      "5  220919  手掘り日本史    0.858300\n",
      "6  225548     戦国史    0.857259\n",
      "7   16551      古代    0.856780\n",
      "8   60636   歴史書一覧    0.856451\n",
      "9    5469    日本書紀    0.853323\n"
     ]
    }
   ],
   "source": [
    "# サンプルクエリで検索テスト\n",
    "print(\"Testing HNSW search...\")\n",
    "\n",
    "# クエリ用の埋め込み\n",
    "query_text = \"query: 日本の歴史について\"\n",
    "query_embedding = model.encode([query_text], normalize_embeddings=True)[0]\n",
    "\n",
    "start_time = time.time()\n",
    "results = con.execute(f\"\"\"\n",
    "    SELECT id, title, array_cosine_similarity(embedding, ?::FLOAT[{EMBEDDING_DIM}]) as similarity\n",
    "    FROM wikipedia_docs\n",
    "    ORDER BY similarity DESC\n",
    "    LIMIT 10\n",
    "\"\"\", [query_embedding.tolist()]).fetchdf()\n",
    "\n",
    "print(f\"Search time: {(time.time() - start_time)*1000:.1f}ms\")\n",
    "print(\"\\nTop 10 results:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Output File ===\n",
      "Path: ../data/wikipedia_400k.duckdb\n",
      "Size: 3.37 GB\n"
     ]
    }
   ],
   "source": [
    "# 接続クローズ\n",
    "con.close()\n",
    "\n",
    "# ファイルサイズ確認\n",
    "file_size = OUTPUT_DB.stat().st_size / 1024**3\n",
    "print(f\"\\n=== Output File ===\")\n",
    "print(f\"Path: {OUTPUT_DB}\")\n",
    "print(f\"Size: {file_size:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 再読み込みテスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading from disk...\n",
      "Reload time: 0.04s\n",
      "Records: 399,029\n",
      "Search time: 691.0ms\n",
      "\n",
      "Top 5:\n",
      "       id  title  similarity\n",
      "0  249275   日本文明    0.865258\n",
      "1   77237  歴史の一覧    0.859950\n",
      "2    3446  1689年    0.859729\n",
      "3   37551   340年    0.859658\n",
      "4   26196   日本人論    0.859130\n"
     ]
    }
   ],
   "source": [
    "# 永続化されたDBを再読み込み\n",
    "print(\"Reloading from disk...\")\n",
    "start_time = time.time()\n",
    "\n",
    "con2 = duckdb.connect(str(OUTPUT_DB), read_only=True)\n",
    "con2.execute(\"LOAD vss;\")\n",
    "\n",
    "print(f\"Reload time: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "# 件数確認\n",
    "count = con2.execute(\"SELECT COUNT(*) FROM wikipedia_docs\").fetchone()[0]\n",
    "print(f\"Records: {count:,}\")\n",
    "\n",
    "# サンプル検索\n",
    "start_time = time.time()\n",
    "result = con2.execute(f\"\"\"\n",
    "    SELECT id, title, array_cosine_similarity(embedding, ?::FLOAT[{EMBEDDING_DIM}]) as similarity\n",
    "    FROM wikipedia_docs\n",
    "    ORDER BY similarity DESC\n",
    "    LIMIT 5\n",
    "\"\"\", [query_embedding.tolist()]).fetchdf()\n",
    "\n",
    "print(f\"Search time: {(time.time() - start_time)*1000:.1f}ms\")\n",
    "print(\"\\nTop 5:\")\n",
    "print(result)\n",
    "\n",
    "con2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 埋め込みのNumPy保存（オプション）\n",
    "\n",
    "ITQ学習など、直接ベクトルにアクセスしたい場合用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPyファイルとしても保存（モデル名を含めて区別）\n",
    "NPY_PATH = DATA_DIR / \"wikipedia_400k_e5_base_embeddings.npy\"\n",
    "\n",
    "print(f\"Saving embeddings to {NPY_PATH}...\")\n",
    "np.save(NPY_PATH, embeddings)\n",
    "\n",
    "npy_size = NPY_PATH.stat().st_size / 1024**3\n",
    "print(f\"Saved: {npy_size:.2f} GB\")\n",
    "\n",
    "# メタデータも保存\n",
    "META_PATH = DATA_DIR / \"wikipedia_400k_e5_base_meta.npz\"\n",
    "np.savez(META_PATH, titles=np.array(titles), doc_ids=np.array(doc_ids))\n",
    "print(f\"Metadata saved: {META_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. サマリー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Wikipedia 400k E5-base Embedding Dataset - Summary\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Documents: {len(documents):,}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Embedding dimension: {EMBEDDING_DIM}\")\n",
    "print(f\"\")\n",
    "print(f\"Output files:\")\n",
    "print(f\"  - DuckDB: {OUTPUT_DB} ({file_size:.2f} GB)\")\n",
    "print(f\"  - NumPy: {NPY_PATH} ({npy_size:.2f} GB)\")\n",
    "print(f\"  - Meta: {META_PATH}\")\n",
    "print(f\"\")\n",
    "print(f\"DuckDB features:\")\n",
    "print(f\"  - HNSW index on embeddings (cosine similarity)\")\n",
    "print(f\"  - Columns: id, doc_id, title, text, embedding\")\n",
    "print(f\"\")\n",
    "print(f\"Note: Files named with 'e5_base' to distinguish from\")\n",
    "print(f\"      experiment_400k.duckdb (E5-large, 1024 dim)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ITQ学習と永続化\n",
    "\n",
    "64 bits と 96 bits の両方でITQモデルを学習し、今後の実験用に保存します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using in-memory embeddings: (399029, 768)\n"
     ]
    }
   ],
   "source": [
    "# ITQ実装をインポート\n",
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "from itq_lsh import ITQLSH\n",
    "\n",
    "# 埋め込みデータをロード（メモリに残っていない場合）\n",
    "if 'embeddings' not in dir():\n",
    "    print(\"Loading embeddings from file...\")\n",
    "    embeddings = np.load(DATA_DIR / \"wikipedia_400k_e5_base_embeddings.npy\")\n",
    "    print(f\"Loaded: {embeddings.shape}\")\n",
    "else:\n",
    "    print(f\"Using in-memory embeddings: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Training ITQ with 64 bits\n",
      "==================================================\n",
      "ITQ学習開始: samples=399029, dim=768, bits=64\n",
      "  Centering完了: mean_norm=0.8698\n",
      "  PCA完了: explained_variance=48.00%\n",
      "  ITQ iteration 10: quantization_error=0.9321\n",
      "  ITQ iteration 20: quantization_error=0.9318\n",
      "  ITQ iteration 30: quantization_error=0.9316\n",
      "  ITQ iteration 40: quantization_error=0.9315\n",
      "  ITQ iteration 50: quantization_error=0.9314\n",
      "ITQ学習完了\n",
      "\n",
      "Training time: 11.0s\n",
      "Saved: ../data/itq_e5_base_64bits.pkl\n"
     ]
    }
   ],
   "source": [
    "# 64 bits ITQ学習\n",
    "print(\"=\"*50)\n",
    "print(\"Training ITQ with 64 bits\")\n",
    "print(\"=\"*50)\n",
    "start_time = time.time()\n",
    "\n",
    "itq_64 = ITQLSH(n_bits=64, n_iterations=50, seed=42)\n",
    "itq_64.fit(embeddings)\n",
    "\n",
    "print(f\"\\nTraining time: {time.time() - start_time:.1f}s\")\n",
    "\n",
    "# 保存\n",
    "ITQ_64_PATH = DATA_DIR / \"itq_e5_base_64bits.pkl\"\n",
    "itq_64.save(str(ITQ_64_PATH))\n",
    "print(f\"Saved: {ITQ_64_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Training ITQ with 96 bits\n",
      "==================================================\n",
      "ITQ学習開始: samples=399029, dim=768, bits=96\n",
      "  Centering完了: mean_norm=0.8698\n",
      "  PCA完了: explained_variance=57.64%\n",
      "  ITQ iteration 10: quantization_error=0.9394\n",
      "  ITQ iteration 20: quantization_error=0.9391\n",
      "  ITQ iteration 30: quantization_error=0.9390\n",
      "  ITQ iteration 40: quantization_error=0.9389\n",
      "  ITQ iteration 50: quantization_error=0.9388\n",
      "ITQ学習完了\n",
      "\n",
      "Training time: 15.9s\n",
      "Saved: ../data/itq_e5_base_96bits.pkl\n"
     ]
    }
   ],
   "source": [
    "# 96 bits ITQ学習\n",
    "print(\"=\"*50)\n",
    "print(\"Training ITQ with 96 bits\")\n",
    "print(\"=\"*50)\n",
    "start_time = time.time()\n",
    "\n",
    "itq_96 = ITQLSH(n_bits=96, n_iterations=50, seed=42)\n",
    "itq_96.fit(embeddings)\n",
    "\n",
    "print(f\"\\nTraining time: {time.time() - start_time:.1f}s\")\n",
    "\n",
    "# 保存\n",
    "ITQ_96_PATH = DATA_DIR / \"itq_e5_base_96bits.pkl\"\n",
    "itq_96.save(str(ITQ_96_PATH))\n",
    "print(f\"Saved: {ITQ_96_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating hashes for all documents...\n",
      "64 bits hashes: (399029, 64), time: 0.4s\n",
      "96 bits hashes: (399029, 96), time: 0.4s\n",
      "\n",
      "Saved hashes:\n",
      "  - ../data/wikipedia_400k_e5_base_hashes_64bits.npy (24.4 MB)\n",
      "  - ../data/wikipedia_400k_e5_base_hashes_96bits.npy (36.5 MB)\n"
     ]
    }
   ],
   "source": [
    "# 全データのハッシュを生成して保存\n",
    "print(\"Generating hashes for all documents...\")\n",
    "\n",
    "# 64 bits ハッシュ\n",
    "start_time = time.time()\n",
    "hashes_64 = itq_64.transform(embeddings)\n",
    "print(f\"64 bits hashes: {hashes_64.shape}, time: {time.time() - start_time:.1f}s\")\n",
    "\n",
    "# 96 bits ハッシュ\n",
    "start_time = time.time()\n",
    "hashes_96 = itq_96.transform(embeddings)\n",
    "print(f\"96 bits hashes: {hashes_96.shape}, time: {time.time() - start_time:.1f}s\")\n",
    "\n",
    "# ハッシュを保存\n",
    "HASH_64_PATH = DATA_DIR / \"wikipedia_400k_e5_base_hashes_64bits.npy\"\n",
    "HASH_96_PATH = DATA_DIR / \"wikipedia_400k_e5_base_hashes_96bits.npy\"\n",
    "\n",
    "np.save(HASH_64_PATH, hashes_64)\n",
    "np.save(HASH_96_PATH, hashes_96)\n",
    "\n",
    "print(f\"\\nSaved hashes:\")\n",
    "print(f\"  - {HASH_64_PATH} ({HASH_64_PATH.stat().st_size / 1024**2:.1f} MB)\")\n",
    "print(f\"  - {HASH_96_PATH} ({HASH_96_PATH.stat().st_size / 1024**2:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing ITQ model reload...\n",
      "64 bits match: True\n",
      "96 bits match: True\n",
      "\n",
      "ITQ models saved and verified successfully!\n"
     ]
    }
   ],
   "source": [
    "# ITQモデルの再読み込みテスト\n",
    "print(\"Testing ITQ model reload...\")\n",
    "\n",
    "itq_64_reloaded = ITQLSH.load(str(ITQ_64_PATH))\n",
    "itq_96_reloaded = ITQLSH.load(str(ITQ_96_PATH))\n",
    "\n",
    "# サンプルで検証\n",
    "sample_idx = 0\n",
    "original_64 = itq_64.transform(embeddings[sample_idx:sample_idx+1])\n",
    "reloaded_64 = itq_64_reloaded.transform(embeddings[sample_idx:sample_idx+1])\n",
    "print(f\"64 bits match: {np.array_equal(original_64, reloaded_64)}\")\n",
    "\n",
    "original_96 = itq_96.transform(embeddings[sample_idx:sample_idx+1])\n",
    "reloaded_96 = itq_96_reloaded.transform(embeddings[sample_idx:sample_idx+1])\n",
    "print(f\"96 bits match: {np.array_equal(original_96, reloaded_96)}\")\n",
    "\n",
    "print(\"\\nITQ models saved and verified successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 最終サマリー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Wikipedia 400k E5-base Dataset - Final Summary\n",
      "============================================================\n",
      "\n",
      "Documents: 399,029\n",
      "Model: intfloat/multilingual-e5-base\n",
      "Embedding dimension: 768\n",
      "\n",
      "Output files:\n",
      "  - wikipedia_400k.duckdb: NOT FOUND\n",
      "  - wikipedia_400k_e5_base_embeddings.npy: 1.14 GB\n",
      "  - wikipedia_400k_e5_base_meta.npz: 132.4 MB\n",
      "  - itq_e5_base_64bits.pkl: 0.2 MB\n",
      "  - itq_e5_base_96bits.pkl: 0.3 MB\n",
      "  - wikipedia_400k_e5_base_hashes_64bits.npy: 24.4 MB\n",
      "  - wikipedia_400k_e5_base_hashes_96bits.npy: 36.5 MB\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 最終サマリー\n",
    "print(\"=\"*60)\n",
    "print(\"Wikipedia 400k E5-base Dataset - Final Summary\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ファイル一覧\n",
    "files = [\n",
    "    (\"DuckDB (embeddings + HNSW)\", OUTPUT_DB),\n",
    "    (\"Embeddings (NumPy)\", DATA_DIR / \"wikipedia_400k_e5_base_embeddings.npy\"),\n",
    "    (\"Metadata\", DATA_DIR / \"wikipedia_400k_e5_base_meta.npz\"),\n",
    "    (\"ITQ Model (64 bits)\", ITQ_64_PATH),\n",
    "    (\"ITQ Model (96 bits)\", ITQ_96_PATH),\n",
    "    (\"Hashes (64 bits)\", HASH_64_PATH),\n",
    "    (\"Hashes (96 bits)\", HASH_96_PATH),\n",
    "]\n",
    "\n",
    "print(f\"\\nDocuments: {len(embeddings):,}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Embedding dimension: {EMBEDDING_DIM}\")\n",
    "print(f\"\\nOutput files:\")\n",
    "for name, path in files:\n",
    "    if path.exists():\n",
    "        size = path.stat().st_size\n",
    "        if size > 1024**3:\n",
    "            size_str = f\"{size / 1024**3:.2f} GB\"\n",
    "        else:\n",
    "            size_str = f\"{size / 1024**2:.1f} MB\"\n",
    "        print(f\"  - {path.name}: {size_str}\")\n",
    "    else:\n",
    "        print(f\"  - {path.name}: NOT FOUND\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 実行結果・評価\n",
    "\n",
    "### 処理時間\n",
    "| 処理 | 時間 |\n",
    "|-----|------|\n",
    "| Wikipediaデータ収集 (400k件) | 12分 |\n",
    "| 埋め込み生成 (GPU) | 15.7分 (424 docs/sec) |\n",
    "| DuckDB挿入 | 17.8分 |\n",
    "| HNSWインデックス作成 | 1.8分 |\n",
    "| ITQ学習 (64 bits + 96 bits) | 数分 |\n",
    "| **合計** | **約50分** |\n",
    "\n",
    "### 出力ファイル\n",
    "| ファイル | サイズ | 内容 |\n",
    "|---------|--------|------|\n",
    "| `wikipedia_400k_e5_base.duckdb` | 3.37 GB | 399,029件 + HNSWインデックス |\n",
    "| `wikipedia_400k_e5_base_embeddings.npy` | 1.14 GB | 埋め込みベクトル |\n",
    "| `wikipedia_400k_e5_base_meta.npz` | 133 MB | タイトル・ドキュメントID |\n",
    "| `itq_e5_base_64bits.pkl` | - | ITQモデル (64 bits) |\n",
    "| `itq_e5_base_96bits.pkl` | - | ITQモデル (96 bits) |\n",
    "| `wikipedia_400k_e5_base_hashes_64bits.npy` | - | 全ドキュメントのハッシュ (64 bits) |\n",
    "| `wikipedia_400k_e5_base_hashes_96bits.npy` | - | 全ドキュメントのハッシュ (96 bits) |\n",
    "\n",
    "### 備考\n",
    "- 使用モデル: `intfloat/multilingual-e5-base` (768次元)\n",
    "- 既存の `experiment_400k.duckdb` は E5-large (1024次元) のため、ファイル名で区別\n",
    "- HNSWインデックスの永続化には `SET hnsw_enable_experimental_persistence = true;` が必要\n",
    "- このデータを使用して、次の実験でOverlapチャンク評価（64/96 bits）を40万件規模で実施予定"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsh-cascade-poc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
