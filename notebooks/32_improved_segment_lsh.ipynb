{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 32. セグメント分割LSH改善\n",
    "\n",
    "## 目的\n",
    "\n",
    "実験30でセグメント完全一致方式を試したが、Recall@10が最大72%と低かった。\n",
    "本実験では、以下の改善手法を検証する。\n",
    "\n",
    "## 背景（実験30の問題点）\n",
    "\n",
    "- 32bitセグメントの完全一致が厳しすぎて良い候補を逃す\n",
    "- GT Top-10の平均ハミング距離は約22bit\n",
    "- 距離2以内に含まれるGTはわずか10.4%\n",
    "\n",
    "## 検証する4つの改善手法\n",
    "\n",
    "1. **オーバーラップセグメント**: N-gram的スライディングでビット列をずらす\n",
    "2. **Multi-Index方式**: 複数の分割パターンでOR結合\n",
    "3. **部分一致方式**: ポップカウント閾値で候補選択\n",
    "4. **ハイブリッド方式**: 上記の組み合わせ\n",
    "\n",
    "## 比較対象\n",
    "\n",
    "- 2段階検索（候補2000件）: Recall@10 = 91%\n",
    "- Seg16完全一致（実験30）: Recall@10 = 72%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "from src.itq_lsh import ITQLSH, hamming_distance_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# パス設定\n",
    "DB_PATH = '../data/experiment_400k.duckdb'\n",
    "ITQ_MODEL_PATH = '../data/itq_model.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. データ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データ読み込み中...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35b92edb261a4bc0a7611b15731f8563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "読み込み完了: 400,000件\n",
      "埋め込み shape: (400000, 1024)\n"
     ]
    }
   ],
   "source": [
    "# データ読み込み\n",
    "print('データ読み込み中...')\n",
    "conn = duckdb.connect(DB_PATH, read_only=True)\n",
    "\n",
    "result = conn.execute(\"\"\"\n",
    "    SELECT id, embedding\n",
    "    FROM documents\n",
    "    ORDER BY id\n",
    "\"\"\").fetchall()\n",
    "conn.close()\n",
    "\n",
    "doc_ids = np.array([r[0] for r in result])\n",
    "embeddings = np.array([r[1] for r in result], dtype=np.float32)\n",
    "\n",
    "print(f'読み込み完了: {len(doc_ids):,}件')\n",
    "print(f'埋め込み shape: {embeddings.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITQモデル読み込み中...\n",
      "ITQハッシュ計算中...\n",
      "ハッシュ shape: (400000, 128)\n"
     ]
    }
   ],
   "source": [
    "# ITQモデル読み込みとハッシュ計算\n",
    "print('ITQモデル読み込み中...')\n",
    "itq = ITQLSH.load(ITQ_MODEL_PATH)\n",
    "\n",
    "print('ITQハッシュ計算中...')\n",
    "hashes = itq.transform(embeddings)\n",
    "print(f'ハッシュ shape: {hashes.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. 共通ユーティリティ関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ground_truth(query_embedding, all_embeddings, top_k=20):\n",
    "    \"\"\"\n",
    "    ブルートフォースでGround Truthを計算\n",
    "    \"\"\"\n",
    "    query_norm = norm(query_embedding)\n",
    "    all_norms = norm(all_embeddings, axis=1)\n",
    "    cosines = (all_embeddings @ query_embedding) / (all_norms * query_norm + 1e-10)\n",
    "    top_indices = np.argsort(cosines)[-top_k:][::-1]\n",
    "    return top_indices\n",
    "\n",
    "\n",
    "def cosine_rerank(query_embedding, candidate_indices, all_embeddings, top_k):\n",
    "    \"\"\"\n",
    "    コサイン類似度でリランキング\n",
    "    \"\"\"\n",
    "    if len(candidate_indices) == 0:\n",
    "        return np.array([], dtype=np.int64)\n",
    "    \n",
    "    candidate_embeddings = all_embeddings[candidate_indices]\n",
    "    query_norm = norm(query_embedding)\n",
    "    candidate_norms = norm(candidate_embeddings, axis=1)\n",
    "    cosine_scores = (candidate_embeddings @ query_embedding) / (candidate_norms * query_norm + 1e-10)\n",
    "    \n",
    "    top_k = min(top_k, len(candidate_indices))\n",
    "    top_idx_in_candidates = np.argsort(cosine_scores)[-top_k:][::-1]\n",
    "    return candidate_indices[top_idx_in_candidates]\n",
    "\n",
    "\n",
    "def evaluate_method(test_query_indices, predicted_results, ground_truths, top_k_values=[5, 10, 20]):\n",
    "    \"\"\"\n",
    "    Recall評価\n",
    "    \"\"\"\n",
    "    recalls = {k: [] for k in top_k_values}\n",
    "    \n",
    "    for qi in test_query_indices:\n",
    "        gt = ground_truths[qi]\n",
    "        pred = predicted_results[qi]\n",
    "        \n",
    "        for k in top_k_values:\n",
    "            gt_set = set(gt[:k])\n",
    "            pred_set = set(pred[:k]) if len(pred) >= k else set(pred)\n",
    "            recalls[k].append(len(gt_set & pred_set) / k)\n",
    "    \n",
    "    return {f'recall@{k}': np.mean(v) for k, v in recalls.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "テストクエリ数: 100\n",
      "Ground Truth計算中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GT計算: 100%|██████████| 100/100 [00:33<00:00,  3.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完了\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# テスト用クエリを準備\n",
    "rng = np.random.default_rng(42)\n",
    "n_test_queries = 100\n",
    "test_query_indices = rng.choice(len(embeddings), n_test_queries, replace=False)\n",
    "\n",
    "print(f'テストクエリ数: {n_test_queries}')\n",
    "\n",
    "# Ground Truth計算\n",
    "print('Ground Truth計算中...')\n",
    "ground_truths = {}\n",
    "for qi in tqdm(test_query_indices, desc='GT計算'):\n",
    "    ground_truths[qi] = compute_ground_truth(embeddings[qi], embeddings, top_k=20)\n",
    "print('完了')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. ベースライン: 2段階検索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_stage_search(query_embedding, query_hash, all_hashes, all_embeddings, candidates, top_k):\n",
    "    \"\"\"\n",
    "    従来の2段階検索（LSH→コサイン）\n",
    "    \"\"\"\n",
    "    distances = hamming_distance_batch(query_hash, all_hashes)\n",
    "    candidate_indices = np.argsort(distances)[:candidates]\n",
    "    return cosine_rerank(query_embedding, candidate_indices, all_embeddings, top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ベースライン（2段階検索）評価中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "候補2000: 100%|██████████| 100/100 [00:04<00:00, 23.21it/s]\n",
      "候補5000: 100%|██████████| 100/100 [00:04<00:00, 20.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ベースライン結果:\n",
      "   method  candidates  recall@5  recall@10  recall@20  avg_time_ms\n",
      "2段階(2000)        2000     0.916      0.913     0.8775    42.817116\n",
      "2段階(5000)        5000     0.952      0.960     0.9530    47.901416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ベースライン評価\n",
    "print('ベースライン（2段階検索）評価中...')\n",
    "\n",
    "baseline_results = []\n",
    "for candidates in [2000, 5000]:\n",
    "    predicted = {}\n",
    "    times = []\n",
    "    \n",
    "    for qi in tqdm(test_query_indices, desc=f'候補{candidates}'):\n",
    "        t0 = time.time()\n",
    "        predicted[qi] = two_stage_search(\n",
    "            embeddings[qi], hashes[qi], hashes, embeddings,\n",
    "            candidates=candidates, top_k=20\n",
    "        )\n",
    "        times.append((time.time() - t0) * 1000)\n",
    "    \n",
    "    recalls = evaluate_method(test_query_indices, predicted, ground_truths)\n",
    "    baseline_results.append({\n",
    "        'method': f'2段階({candidates})',\n",
    "        'candidates': candidates,\n",
    "        **recalls,\n",
    "        'avg_time_ms': np.mean(times)\n",
    "    })\n",
    "\n",
    "df_baseline = pd.DataFrame(baseline_results)\n",
    "print('\\nベースライン結果:')\n",
    "print(df_baseline.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. 手法1: オーバーラップセグメント（N-gram的スライディング）\n",
    "\n",
    "ビット列をスライドさせて重複するセグメントを生成し、OR条件で候補を収集。\n",
    "\n",
    "```\n",
    "128bit ハッシュ\n",
    "├─ セグメント0: bit[0:8]\n",
    "├─ セグメント1: bit[4:12]   ← 4bitずらし\n",
    "├─ セグメント2: bit[8:16]\n",
    "├─ ...\n",
    "└─ セグメント30: bit[120:128]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_to_overlap_segments(hash_array, segment_width=8, stride=4):\n",
    "    \"\"\"\n",
    "    オーバーラップセグメントを生成\n",
    "    \n",
    "    Args:\n",
    "        hash_array: (n_docs, 128) のハッシュ配列\n",
    "        segment_width: セグメント幅（ビット数）\n",
    "        stride: スライド幅（ビット数）\n",
    "    \n",
    "    Returns:\n",
    "        segments: (n_docs, n_segments) の整数配列\n",
    "    \"\"\"\n",
    "    n_docs, n_bits = hash_array.shape\n",
    "    n_segments = (n_bits - segment_width) // stride + 1\n",
    "    \n",
    "    segments = []\n",
    "    for i in range(n_segments):\n",
    "        start = i * stride\n",
    "        end = start + segment_width\n",
    "        segment_bits = hash_array[:, start:end]\n",
    "        \n",
    "        # ビットを整数に変換\n",
    "        powers = 2 ** np.arange(segment_width - 1, -1, -1)\n",
    "        segment_int = np.sum(segment_bits * powers, axis=1)\n",
    "        segments.append(segment_int)\n",
    "    \n",
    "    return np.column_stack(segments), n_segments\n",
    "\n",
    "\n",
    "def build_overlap_index(segments):\n",
    "    \"\"\"\n",
    "    オーバーラップセグメントのインデックスを構築\n",
    "    \"\"\"\n",
    "    n_docs, n_segments = segments.shape\n",
    "    index = {i: defaultdict(list) for i in range(n_segments)}\n",
    "    \n",
    "    for doc_idx in range(n_docs):\n",
    "        for seg_idx in range(n_segments):\n",
    "            seg_value = int(segments[doc_idx, seg_idx])\n",
    "            index[seg_idx][seg_value].append(doc_idx)\n",
    "    \n",
    "    return index\n",
    "\n",
    "\n",
    "def overlap_segment_search(query_segments, segment_index, n_segments):\n",
    "    \"\"\"\n",
    "    オーバーラップセグメントで候補を検索（OR条件）\n",
    "    \"\"\"\n",
    "    candidates = set()\n",
    "    \n",
    "    for seg_idx in range(n_segments):\n",
    "        seg_value = int(query_segments[seg_idx])\n",
    "        if seg_value in segment_index[seg_idx]:\n",
    "            candidates.update(segment_index[seg_idx][seg_value])\n",
    "    \n",
    "    return np.array(list(candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "オーバーラップセグメントインデックス構築中...\n",
      "  width=8, stride=4: 31セグメント, 平均バケット: 1562.5件\n",
      "  width=8, stride=2: 61セグメント, 平均バケット: 1562.5件\n",
      "  width=16, stride=8: 15セグメント, 平均バケット: 8.7件\n",
      "  width=16, stride=4: 29セグメント, 平均バケット: 8.7件\n"
     ]
    }
   ],
   "source": [
    "# オーバーラップセグメントのインデックス構築\n",
    "print('オーバーラップセグメントインデックス構築中...')\n",
    "\n",
    "overlap_configs = [\n",
    "    (8, 4),   # 8bit幅、4bitストライド → 31セグメント\n",
    "    (8, 2),   # 8bit幅、2bitストライド → 61セグメント\n",
    "    (16, 8),  # 16bit幅、8bitストライド → 15セグメント\n",
    "    (16, 4),  # 16bit幅、4bitストライド → 29セグメント\n",
    "]\n",
    "\n",
    "overlap_data = {}\n",
    "\n",
    "for width, stride in overlap_configs:\n",
    "    key = (width, stride)\n",
    "    segments, n_seg = hash_to_overlap_segments(hashes, width, stride)\n",
    "    index = build_overlap_index(segments)\n",
    "    \n",
    "    # 統計\n",
    "    bucket_sizes = [len(docs) for seg_idx in index for docs in index[seg_idx].values()]\n",
    "    \n",
    "    overlap_data[key] = {\n",
    "        'segments': segments,\n",
    "        'index': index,\n",
    "        'n_segments': n_seg,\n",
    "        'avg_bucket_size': np.mean(bucket_sizes),\n",
    "    }\n",
    "    \n",
    "    print(f'  width={width}, stride={stride}: {n_seg}セグメント, 平均バケット: {np.mean(bucket_sizes):.1f}件')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "オーバーラップセグメント評価中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "w=8,s=4: 100%|██████████| 100/100 [00:02<00:00, 48.39it/s]\n",
      "w=8,s=2: 100%|██████████| 100/100 [00:02<00:00, 34.76it/s]\n",
      "w=16,s=8: 100%|██████████| 100/100 [00:00<00:00, 371.83it/s]\n",
      "w=16,s=4: 100%|██████████| 100/100 [00:00<00:00, 365.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "オーバーラップセグメント結果:\n",
      "       method  width  stride  n_segments  avg_candidates  max_candidates  recall@5  recall@10  recall@20  avg_time_ms\n",
      " Overlap(8/4)      8       4          31        63764.47           96144     0.904      0.900     0.8660    20.518231\n",
      " Overlap(8/2)      8       2          61        86749.84          114777     0.916      0.916     0.8760    28.582323\n",
      "Overlap(16/8)     16       8          15         2383.70           21686     0.514      0.458     0.3875     2.669718\n",
      "Overlap(16/4)     16       4          29         3308.23           27454     0.576      0.520     0.4515     2.717373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# オーバーラップセグメント評価\n",
    "print('\\nオーバーラップセグメント評価中...')\n",
    "\n",
    "overlap_results = []\n",
    "\n",
    "for (width, stride), data in overlap_data.items():\n",
    "    segments = data['segments']\n",
    "    index = data['index']\n",
    "    n_seg = data['n_segments']\n",
    "    \n",
    "    predicted = {}\n",
    "    candidate_counts = []\n",
    "    times = []\n",
    "    \n",
    "    for qi in tqdm(test_query_indices, desc=f'w={width},s={stride}'):\n",
    "        t0 = time.time()\n",
    "        \n",
    "        # Step 1: セグメントで候補収集\n",
    "        candidates = overlap_segment_search(segments[qi], index, n_seg)\n",
    "        candidate_counts.append(len(candidates))\n",
    "        \n",
    "        # Step 2: ハミング距離でTop-2000に絞る\n",
    "        if len(candidates) > 2000:\n",
    "            dists = hamming_distance_batch(hashes[qi], hashes[candidates])\n",
    "            top_idx = np.argsort(dists)[:2000]\n",
    "            candidates = candidates[top_idx]\n",
    "        \n",
    "        # Step 3: コサイン類似度でリランキング\n",
    "        predicted[qi] = cosine_rerank(embeddings[qi], candidates, embeddings, top_k=20)\n",
    "        \n",
    "        times.append((time.time() - t0) * 1000)\n",
    "    \n",
    "    recalls = evaluate_method(test_query_indices, predicted, ground_truths)\n",
    "    \n",
    "    overlap_results.append({\n",
    "        'method': f'Overlap({width}/{stride})',\n",
    "        'width': width,\n",
    "        'stride': stride,\n",
    "        'n_segments': n_seg,\n",
    "        'avg_candidates': np.mean(candidate_counts),\n",
    "        'max_candidates': np.max(candidate_counts),\n",
    "        **recalls,\n",
    "        'avg_time_ms': np.mean(times)\n",
    "    })\n",
    "\n",
    "df_overlap = pd.DataFrame(overlap_results)\n",
    "print('\\nオーバーラップセグメント結果:')\n",
    "print(df_overlap.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 手法2: Multi-Index方式\n",
    "\n",
    "複数の異なる分割パターンでインデックスを構築し、OR結合で候補を収集。\n",
    "\n",
    "```\n",
    "パターン1: [0-31, 32-63, 64-95, 96-127]  (4分割)\n",
    "パターン2: [16-47, 48-79, 80-111, ...]   (16bitオフセット)\n",
    "パターン3: [0-15, 16-31, ..., 112-127]   (8分割)\n",
    "パターン4: [8-23, 24-39, ...]            (8bitオフセット)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_segment_patterns():\n",
    "    \"\"\"\n",
    "    複数のセグメント分割パターンを定義\n",
    "    \"\"\"\n",
    "    patterns = []\n",
    "    \n",
    "    # パターン1: 4分割（32bit x 4）- オフセット0\n",
    "    patterns.append([(0, 32), (32, 64), (64, 96), (96, 128)])\n",
    "    \n",
    "    # パターン2: 4分割（32bit x 4）- オフセット16\n",
    "    patterns.append([(16, 48), (48, 80), (80, 112), (112, 128), (0, 16)])\n",
    "    \n",
    "    # パターン3: 8分割（16bit x 8）- オフセット0\n",
    "    patterns.append([(i*16, (i+1)*16) for i in range(8)])\n",
    "    \n",
    "    # パターン4: 8分割（16bit x 8）- オフセット8\n",
    "    patterns.append([(8, 24), (24, 40), (40, 56), (56, 72), \n",
    "                     (72, 88), (88, 104), (104, 120), (120, 128), (0, 8)])\n",
    "    \n",
    "    # パターン5: 16分割（8bit x 16）- オフセット0\n",
    "    patterns.append([(i*8, (i+1)*8) for i in range(16)])\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "\n",
    "def hash_to_pattern_segments(hash_array, pattern):\n",
    "    \"\"\"\n",
    "    指定パターンでセグメント化\n",
    "    \"\"\"\n",
    "    n_docs = hash_array.shape[0]\n",
    "    segments = []\n",
    "    \n",
    "    for start, end in pattern:\n",
    "        width = end - start\n",
    "        segment_bits = hash_array[:, start:end]\n",
    "        powers = 2 ** np.arange(width - 1, -1, -1)\n",
    "        segment_int = np.sum(segment_bits * powers, axis=1)\n",
    "        segments.append(segment_int)\n",
    "    \n",
    "    return np.column_stack(segments)\n",
    "\n",
    "\n",
    "def build_multi_index(hashes, patterns):\n",
    "    \"\"\"\n",
    "    複数パターンのインデックスを構築\n",
    "    \"\"\"\n",
    "    multi_index = []\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        segments = hash_to_pattern_segments(hashes, pattern)\n",
    "        n_segments = len(pattern)\n",
    "        \n",
    "        index = {i: defaultdict(list) for i in range(n_segments)}\n",
    "        for doc_idx in range(len(hashes)):\n",
    "            for seg_idx in range(n_segments):\n",
    "                seg_value = int(segments[doc_idx, seg_idx])\n",
    "                index[seg_idx][seg_value].append(doc_idx)\n",
    "        \n",
    "        multi_index.append({\n",
    "            'pattern': pattern,\n",
    "            'segments': segments,\n",
    "            'index': index,\n",
    "            'n_segments': n_segments\n",
    "        })\n",
    "    \n",
    "    return multi_index\n",
    "\n",
    "\n",
    "def multi_index_search(query_hash, multi_index):\n",
    "    \"\"\"\n",
    "    Multi-Index方式で候補を検索（全パターンのOR結合）\n",
    "    \"\"\"\n",
    "    candidates = set()\n",
    "    \n",
    "    for idx_data in multi_index:\n",
    "        pattern = idx_data['pattern']\n",
    "        index = idx_data['index']\n",
    "        n_segments = idx_data['n_segments']\n",
    "        \n",
    "        # クエリのセグメント化\n",
    "        query_segments = []\n",
    "        for start, end in pattern:\n",
    "            width = end - start\n",
    "            segment_bits = query_hash[start:end]\n",
    "            powers = 2 ** np.arange(width - 1, -1, -1)\n",
    "            seg_value = int(np.sum(segment_bits * powers))\n",
    "            query_segments.append(seg_value)\n",
    "        \n",
    "        # 候補検索\n",
    "        for seg_idx, seg_value in enumerate(query_segments):\n",
    "            if seg_value in index[seg_idx]:\n",
    "                candidates.update(index[seg_idx][seg_value])\n",
    "    \n",
    "    return np.array(list(candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Index構築中...\n",
      "パターン数: 5\n",
      "  パターン1: 4セグメント, 幅=[32, 32, 32, 32]\n",
      "  パターン2: 5セグメント, 幅=[32, 32, 32, 16, 16]\n",
      "  パターン3: 8セグメント, 幅=[16, 16, 16, 16, 16, 16, 16, 16]\n",
      "  パターン4: 9セグメント, 幅=[16, 16, 16, 16, 16, 16, 16, 8, 8]\n",
      "  パターン5: 16セグメント, 幅=[8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "構築完了\n"
     ]
    }
   ],
   "source": [
    "# Multi-Indexの構築\n",
    "print('Multi-Index構築中...')\n",
    "\n",
    "patterns = create_segment_patterns()\n",
    "print(f'パターン数: {len(patterns)}')\n",
    "for i, p in enumerate(patterns):\n",
    "    widths = [end - start for start, end in p]\n",
    "    print(f'  パターン{i+1}: {len(p)}セグメント, 幅={widths}')\n",
    "\n",
    "multi_index = build_multi_index(hashes, patterns)\n",
    "print('構築完了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Multi-Index評価中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1パターン: 100%|██████████| 100/100 [00:00<00:00, 4521.72it/s]\n",
      "2パターン: 100%|██████████| 100/100 [00:00<00:00, 1507.47it/s]\n",
      "3パターン: 100%|██████████| 100/100 [00:00<00:00, 616.18it/s]\n",
      "5パターン: 100%|██████████| 100/100 [00:01<00:00, 59.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Multi-Index結果:\n",
      "       method  n_patterns  avg_candidates  max_candidates  recall@5  recall@10  recall@20  avg_time_ms\n",
      "MultiIndex(1)           1          134.68            3518     0.238      0.134     0.0840     0.218928\n",
      "MultiIndex(2)           2          637.43           10111     0.288      0.204     0.1470     0.659337\n",
      "MultiIndex(3)           3         1724.71           19368     0.428      0.353     0.2935     1.611657\n",
      "MultiIndex(5)           5        41161.27           72431     0.874      0.867     0.8330    16.784585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Multi-Index評価（パターン数を変えて）\n",
    "print('\\nMulti-Index評価中...')\n",
    "\n",
    "multi_index_results = []\n",
    "\n",
    "for n_patterns in [1, 2, 3, 5]:\n",
    "    selected_index = multi_index[:n_patterns]\n",
    "    \n",
    "    predicted = {}\n",
    "    candidate_counts = []\n",
    "    times = []\n",
    "    \n",
    "    for qi in tqdm(test_query_indices, desc=f'{n_patterns}パターン'):\n",
    "        t0 = time.time()\n",
    "        \n",
    "        # Step 1: Multi-Indexで候補収集\n",
    "        candidates = multi_index_search(hashes[qi], selected_index)\n",
    "        candidate_counts.append(len(candidates))\n",
    "        \n",
    "        # Step 2: ハミング距離でTop-2000に絞る\n",
    "        if len(candidates) > 2000:\n",
    "            dists = hamming_distance_batch(hashes[qi], hashes[candidates])\n",
    "            top_idx = np.argsort(dists)[:2000]\n",
    "            candidates = candidates[top_idx]\n",
    "        \n",
    "        # Step 3: コサイン類似度でリランキング\n",
    "        predicted[qi] = cosine_rerank(embeddings[qi], candidates, embeddings, top_k=20)\n",
    "        \n",
    "        times.append((time.time() - t0) * 1000)\n",
    "    \n",
    "    recalls = evaluate_method(test_query_indices, predicted, ground_truths)\n",
    "    \n",
    "    multi_index_results.append({\n",
    "        'method': f'MultiIndex({n_patterns})',\n",
    "        'n_patterns': n_patterns,\n",
    "        'avg_candidates': np.mean(candidate_counts),\n",
    "        'max_candidates': np.max(candidate_counts),\n",
    "        **recalls,\n",
    "        'avg_time_ms': np.mean(times)\n",
    "    })\n",
    "\n",
    "df_multi_index = pd.DataFrame(multi_index_results)\n",
    "print('\\nMulti-Index結果:')\n",
    "print(df_multi_index.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. 手法3: 部分一致方式\n",
    "\n",
    "セグメント完全一致ではなく、ポップカウント（一致ビット数）閾値で候補を選択。\n",
    "\n",
    "```\n",
    "16bitセグメントで14bit以上一致（87.5%以上）なら候補とする\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_match_search(query_hash, all_hashes, segment_width=16, threshold_ratio=0.875):\n",
    "    \"\"\"\n",
    "    部分一致方式で候補を検索\n",
    "    \n",
    "    Args:\n",
    "        query_hash: (128,) のクエリハッシュ\n",
    "        all_hashes: (n_docs, 128) の全ハッシュ\n",
    "        segment_width: セグメント幅\n",
    "        threshold_ratio: 一致率の閾値（0.875 = 14/16）\n",
    "    \n",
    "    Returns:\n",
    "        candidates: いずれかのセグメントで閾値以上の一致があるドキュメント\n",
    "    \"\"\"\n",
    "    n_docs, n_bits = all_hashes.shape\n",
    "    n_segments = n_bits // segment_width\n",
    "    threshold = int(segment_width * threshold_ratio)\n",
    "    \n",
    "    candidates = set()\n",
    "    \n",
    "    for seg_idx in range(n_segments):\n",
    "        start = seg_idx * segment_width\n",
    "        end = start + segment_width\n",
    "        \n",
    "        query_seg = query_hash[start:end]\n",
    "        doc_segs = all_hashes[:, start:end]\n",
    "        \n",
    "        # ポップカウント（一致ビット数）\n",
    "        matches = np.sum(query_seg == doc_segs, axis=1)\n",
    "        \n",
    "        # 閾値以上の候補を追加\n",
    "        matching_docs = np.where(matches >= threshold)[0]\n",
    "        candidates.update(matching_docs)\n",
    "    \n",
    "    return np.array(list(candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell-22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "部分一致評価中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "部分一致: 100%|██████████| 5/5 [00:52<00:00, 10.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "部分一致結果:\n",
      "         method  width  threshold_ratio  threshold_bits  avg_candidates  max_candidates  recall@5  recall@10  recall@20  avg_time_ms\n",
      "Partial(16/14+)     16           0.8750              14        25700.22           62814     0.892      0.890     0.8460   116.752002\n",
      "Partial(16/13+)     16           0.8125              13        61886.61           85487     0.914      0.912     0.8760   127.959471\n",
      "Partial(16/12+)     16           0.7500              12       124642.54          153451     0.918      0.918     0.8785   147.736108\n",
      "Partial(32/28+)     32           0.8750              28         3692.63           27465     0.692      0.639     0.5680    65.064964\n",
      "Partial(32/26+)     32           0.8125              26        12455.29           50004     0.866      0.862     0.8195    70.536461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 部分一致評価\n",
    "print('部分一致評価中...')\n",
    "\n",
    "partial_match_configs = [\n",
    "    (16, 0.875),  # 16bit中14bit以上一致\n",
    "    (16, 0.8125), # 16bit中13bit以上一致\n",
    "    (16, 0.75),   # 16bit中12bit以上一致\n",
    "    (32, 0.875),  # 32bit中28bit以上一致\n",
    "    (32, 0.8125), # 32bit中26bit以上一致\n",
    "]\n",
    "\n",
    "partial_results = []\n",
    "\n",
    "for width, ratio in tqdm(partial_match_configs, desc='部分一致'):\n",
    "    predicted = {}\n",
    "    candidate_counts = []\n",
    "    times = []\n",
    "    \n",
    "    for qi in test_query_indices:\n",
    "        t0 = time.time()\n",
    "        \n",
    "        # Step 1: 部分一致で候補収集\n",
    "        candidates = partial_match_search(hashes[qi], hashes, width, ratio)\n",
    "        candidate_counts.append(len(candidates))\n",
    "        \n",
    "        # Step 2: ハミング距離でTop-2000に絞る\n",
    "        if len(candidates) > 2000:\n",
    "            dists = hamming_distance_batch(hashes[qi], hashes[candidates])\n",
    "            top_idx = np.argsort(dists)[:2000]\n",
    "            candidates = candidates[top_idx]\n",
    "        \n",
    "        # Step 3: コサイン類似度でリランキング\n",
    "        predicted[qi] = cosine_rerank(embeddings[qi], candidates, embeddings, top_k=20)\n",
    "        \n",
    "        times.append((time.time() - t0) * 1000)\n",
    "    \n",
    "    recalls = evaluate_method(test_query_indices, predicted, ground_truths)\n",
    "    threshold_bits = int(width * ratio)\n",
    "    \n",
    "    partial_results.append({\n",
    "        'method': f'Partial({width}/{threshold_bits}+)',\n",
    "        'width': width,\n",
    "        'threshold_ratio': ratio,\n",
    "        'threshold_bits': threshold_bits,\n",
    "        'avg_candidates': np.mean(candidate_counts),\n",
    "        'max_candidates': np.max(candidate_counts),\n",
    "        **recalls,\n",
    "        'avg_time_ms': np.mean(times)\n",
    "    })\n",
    "\n",
    "df_partial = pd.DataFrame(partial_results)\n",
    "print('\\n部分一致結果:')\n",
    "print(df_partial.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. 手法4: ハイブリッド方式\n",
    "\n",
    "オーバーラップセグメント + Multi-Index のOR結合で候補を最大化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(query_hash, overlap_data_item, multi_index, hamming_limit=2000):\n",
    "    \"\"\"\n",
    "    ハイブリッド検索\n",
    "    \n",
    "    Step 1: オーバーラップ + Multi-Index で候補収集（OR）\n",
    "    Step 2: ハミング距離でTop-N選択\n",
    "    \"\"\"\n",
    "    # オーバーラップセグメントで候補収集\n",
    "    segments = overlap_data_item['segments']\n",
    "    index = overlap_data_item['index']\n",
    "    n_seg = overlap_data_item['n_segments']\n",
    "    \n",
    "    candidates_overlap = overlap_segment_search(\n",
    "        segments[np.where(doc_ids == query_hash)[0][0]] if isinstance(query_hash, int) else \n",
    "        overlap_data_item['segments'][0],  # Placeholder\n",
    "        index, n_seg\n",
    "    )\n",
    "    \n",
    "    # Multi-Indexで候補収集\n",
    "    candidates_multi = multi_index_search(query_hash, multi_index)\n",
    "    \n",
    "    # OR結合\n",
    "    all_candidates = np.unique(np.concatenate([\n",
    "        candidates_overlap, candidates_multi\n",
    "    ]))\n",
    "    \n",
    "    return all_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cell-25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ハイブリッド評価中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hybrid: 100%|██████████| 100/100 [00:04<00:00, 22.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ハイブリッド結果:\n",
      "  候補数: 63764 (最大: 96144)\n",
      "  Recall@10: 90.3%\n",
      "  処理時間: 43.82ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ハイブリッド評価\n",
    "print('ハイブリッド評価中...')\n",
    "\n",
    "# オーバーラップ(8/4)とMulti-Index(5)を組み合わせ\n",
    "overlap_key = (8, 4)\n",
    "overlap_item = overlap_data[overlap_key]\n",
    "\n",
    "predicted = {}\n",
    "candidate_counts = []\n",
    "times = []\n",
    "\n",
    "for qi in tqdm(test_query_indices, desc='Hybrid'):\n",
    "    t0 = time.time()\n",
    "    \n",
    "    # Step 1: オーバーラップで候補収集\n",
    "    candidates_overlap = overlap_segment_search(\n",
    "        overlap_item['segments'][qi], \n",
    "        overlap_item['index'], \n",
    "        overlap_item['n_segments']\n",
    "    )\n",
    "    \n",
    "    # Step 2: Multi-Indexで候補収集\n",
    "    candidates_multi = multi_index_search(hashes[qi], multi_index)\n",
    "    \n",
    "    # OR結合\n",
    "    all_candidates = np.unique(np.concatenate([candidates_overlap, candidates_multi]))\n",
    "    candidate_counts.append(len(all_candidates))\n",
    "    \n",
    "    # Step 3: ハミング距離でTop-2000に絞る\n",
    "    if len(all_candidates) > 2000:\n",
    "        dists = hamming_distance_batch(hashes[qi], hashes[all_candidates])\n",
    "        top_idx = np.argsort(dists)[:2000]\n",
    "        all_candidates = all_candidates[top_idx]\n",
    "    \n",
    "    # Step 4: コサイン類似度でリランキング\n",
    "    predicted[qi] = cosine_rerank(embeddings[qi], all_candidates, embeddings, top_k=20)\n",
    "    \n",
    "    times.append((time.time() - t0) * 1000)\n",
    "\n",
    "recalls = evaluate_method(test_query_indices, predicted, ground_truths)\n",
    "\n",
    "hybrid_result = {\n",
    "    'method': 'Hybrid(Overlap+Multi)',\n",
    "    'avg_candidates': np.mean(candidate_counts),\n",
    "    'max_candidates': np.max(candidate_counts),\n",
    "    **recalls,\n",
    "    'avg_time_ms': np.mean(times)\n",
    "}\n",
    "\n",
    "print(f'\\nハイブリッド結果:')\n",
    "print(f'  候補数: {hybrid_result[\"avg_candidates\"]:.0f} (最大: {hybrid_result[\"max_candidates\"]})')\n",
    "print(f'  Recall@10: {hybrid_result[\"recall@10\"]*100:.1f}%')\n",
    "print(f'  処理時間: {hybrid_result[\"avg_time_ms\"]:.2f}ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. 最終比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全結果を統合\n",
    "all_results = []\n",
    "\n",
    "# ベースライン\n",
    "for r in baseline_results:\n",
    "    all_results.append(r)\n",
    "\n",
    "# オーバーラップ\n",
    "for r in overlap_results:\n",
    "    all_results.append(r)\n",
    "\n",
    "# Multi-Index\n",
    "for r in multi_index_results:\n",
    "    all_results.append(r)\n",
    "\n",
    "# 部分一致（代表的なもの）\n",
    "for r in partial_results[:3]:  # 16bit系のみ\n",
    "    all_results.append(r)\n",
    "\n",
    "# ハイブリッド\n",
    "all_results.append(hybrid_result)\n",
    "\n",
    "df_all = pd.DataFrame(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cell-28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "最終比較: セグメント分割LSH改善手法\n",
      "====================================================================================================\n",
      "\n",
      "              手法               |      候補数 |    R@5 |   R@10 |   R@20 |     Time\n",
      "----------------------------------------------------------------------------------------------------\n",
      "          2段階(5000)            |      nan |  95.2% |  96.0% |  95.3% |   47.90ms\n",
      "       Partial(16/12+)         |   124643 |  91.8% |  91.8% |  87.9% |  147.74ms\n",
      "         Overlap(8/2)          |    86750 |  91.6% |  91.6% |  87.6% |   28.58ms\n",
      "          2段階(2000)            |      nan |  91.6% |  91.3% |  87.8% |   42.82ms\n",
      "       Partial(16/13+)         |    61887 |  91.4% |  91.2% |  87.6% |  127.96ms\n",
      "    Hybrid(Overlap+Multi)      |    63764 |  91.2% |  90.3% |  86.2% |   43.82ms\n",
      "         Overlap(8/4)          |    63764 |  90.4% |  90.0% |  86.6% |   20.52ms\n",
      "       Partial(16/14+)         |    25700 |  89.2% |  89.0% |  84.6% |  116.75ms\n",
      "        MultiIndex(5)          |    41161 |  87.4% |  86.7% |  83.3% |   16.78ms\n",
      "        Overlap(16/4)          |     3308 |  57.6% |  52.0% |  45.2% |    2.72ms\n",
      "        Overlap(16/8)          |     2384 |  51.4% |  45.8% |  38.8% |    2.67ms\n",
      "        MultiIndex(3)          |     1725 |  42.8% |  35.3% |  29.3% |    1.61ms\n",
      "        MultiIndex(2)          |      637 |  28.8% |  20.4% |  14.7% |    0.66ms\n",
      "        MultiIndex(1)          |      135 |  23.8% |  13.4% |   8.4% |    0.22ms\n"
     ]
    }
   ],
   "source": [
    "# 最終比較表示\n",
    "print('=' * 100)\n",
    "print('最終比較: セグメント分割LSH改善手法')\n",
    "print('=' * 100)\n",
    "\n",
    "print(f'\\n{\"手法\":^30} | {\"候補数\":>8} | {\"R@5\":>6} | {\"R@10\":>6} | {\"R@20\":>6} | {\"Time\":>8}')\n",
    "print('-' * 100)\n",
    "\n",
    "# Recall@10でソート\n",
    "df_sorted = df_all.sort_values('recall@10', ascending=False)\n",
    "\n",
    "for _, row in df_sorted.iterrows():\n",
    "    method = row['method']\n",
    "    candidates = row.get('avg_candidates', row.get('candidates', '-'))\n",
    "    if isinstance(candidates, (int, float)):\n",
    "        candidates = f'{candidates:.0f}'\n",
    "    \n",
    "    print(f'{method:^30} | {candidates:>8} | '\n",
    "          f'{row[\"recall@5\"]*100:>5.1f}% | {row[\"recall@10\"]*100:>5.1f}% | {row[\"recall@20\"]*100:>5.1f}% | '\n",
    "          f'{row[\"avg_time_ms\"]:>7.2f}ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. 結論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cell-30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "結論\n",
      "================================================================================\n",
      "\n",
      "■ 実験目的\n",
      "  実験30のセグメント完全一致方式（Recall@10=72%）を改善する\n",
      "\n",
      "■ 最良の改善手法: Partial(16/12+)\n",
      "  Recall@10: 91.8%\n",
      "  候補数: 124643件\n",
      "  処理時間: 147.74ms\n",
      "\n",
      "■ ベースライン（2段階検索）との比較\n",
      "  2段階(2000): Recall@10 = 91.3%\n",
      "  改善手法: Recall@10 = 91.8%\n",
      "  差分: +0.5ポイント\n",
      "\n",
      "■ 各手法の評価\n",
      "\n",
      "  1. オーバーラップセグメント\n",
      "     - N-gram的スライディングで多くの候補を収集\n",
      "     - width=8, stride=4 で良好な結果\n",
      "     - セグメント数増加で候補数が増え、Recall向上\n",
      "\n",
      "  2. Multi-Index方式\n",
      "     - 複数パターンのOR結合で漏れを防ぐ\n",
      "     - パターン数増加でRecall向上するが、処理時間も増加\n",
      "\n",
      "  3. 部分一致方式\n",
      "     - 全件走査が必要で処理時間が長い\n",
      "     - 閾値の調整が難しい\n",
      "\n",
      "  4. ハイブリッド方式\n",
      "     - オーバーラップ + Multi-Index で最大の候補を収集\n",
      "     - Recall向上するが、処理時間とのトレードオフ\n",
      "\n",
      "■ 結論\n",
      "  セグメント分割方式は改善したが、2段階検索（全件ハミング距離計算）には及ばない。\n",
      "  理由: ハミング距離とコサイン類似度の相関が完全でないため、セグメント一致だけでは\n",
      "  良い候補を確実に捉えられない。\n",
      "\n",
      "  推奨: 引き続き2段階検索（ITQ LSH → コサイン）を採用。\n",
      "  セグメント方式は、データ件数が非常に大きい場合（数千万件以上）の\n",
      "  初段フィルタとして検討価値あり。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 最良の手法を特定\n",
    "best_row = df_sorted.iloc[0]\n",
    "best_improved = df_sorted[~df_sorted['method'].str.contains('2段階')].iloc[0]\n",
    "\n",
    "print('=' * 80)\n",
    "print('結論')\n",
    "print('=' * 80)\n",
    "\n",
    "print(f'''\n",
    "■ 実験目的\n",
    "  実験30のセグメント完全一致方式（Recall@10=72%）を改善する\n",
    "\n",
    "■ 最良の改善手法: {best_improved[\"method\"]}\n",
    "  Recall@10: {best_improved[\"recall@10\"]*100:.1f}%\n",
    "  候補数: {best_improved.get(\"avg_candidates\", \"-\"):.0f}件\n",
    "  処理時間: {best_improved[\"avg_time_ms\"]:.2f}ms\n",
    "\n",
    "■ ベースライン（2段階検索）との比較\n",
    "  2段階(2000): Recall@10 = {baseline_results[0][\"recall@10\"]*100:.1f}%\n",
    "  改善手法: Recall@10 = {best_improved[\"recall@10\"]*100:.1f}%\n",
    "  差分: {(best_improved[\"recall@10\"] - baseline_results[0][\"recall@10\"])*100:+.1f}ポイント\n",
    "\n",
    "■ 各手法の評価\n",
    "\n",
    "  1. オーバーラップセグメント\n",
    "     - N-gram的スライディングで多くの候補を収集\n",
    "     - width=8, stride=4 で良好な結果\n",
    "     - セグメント数増加で候補数が増え、Recall向上\n",
    "\n",
    "  2. Multi-Index方式\n",
    "     - 複数パターンのOR結合で漏れを防ぐ\n",
    "     - パターン数増加でRecall向上するが、処理時間も増加\n",
    "\n",
    "  3. 部分一致方式\n",
    "     - 全件走査が必要で処理時間が長い\n",
    "     - 閾値の調整が難しい\n",
    "\n",
    "  4. ハイブリッド方式\n",
    "     - オーバーラップ + Multi-Index で最大の候補を収集\n",
    "     - Recall向上するが、処理時間とのトレードオフ\n",
    "\n",
    "■ 結論\n",
    "  セグメント分割方式は改善したが、2段階検索（全件ハミング距離計算）には及ばない。\n",
    "  理由: ハミング距離とコサイン類似度の相関が完全でないため、セグメント一致だけでは\n",
    "  良い候補を確実に捉えられない。\n",
    "\n",
    "  推奨: 引き続き2段階検索（ITQ LSH → コサイン）を採用。\n",
    "  セグメント方式は、データ件数が非常に大きい場合（数千万件以上）の\n",
    "  初段フィルタとして検討価値あり。\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": "---\n\n## 10. 実験評価まとめ\n\n### 実験目的\n実験30のセグメント完全一致方式（Recall@10=72%）を改善し、DB負荷低減のための効果的な枝刈り手法を探る。\n\n### 検証した手法と結果\n\n| 手法 | 期待R@10 | 実験R@10 | 候補数 | 削減率 | 処理時間 | 評価 |\n|------|---------|---------|--------|--------|----------|------|\n| **Overlap(8/2)** | 80-85% | **91.6%** | 86,750 | 78% | 28.6ms | 期待以上 |\n| **Overlap(8/4)** | 80-85% | **90.0%** | 63,764 | 84% | 20.5ms | 期待以上 |\n| Multi-Index(5) | 78-83% | **86.7%** | 41,161 | 90% | 16.8ms | 期待以上 |\n| Partial(16/12+) | 75-82% | **91.8%** | 124,643 | 69% | 147.7ms | 期待以上だが遅い |\n| Hybrid | 85-90% | **90.3%** | 63,764 | 84% | 43.8ms | 期待通り |\n| 2段階(2000)参考 | - | 91.3% | 2,000 | 99.5% | 42.8ms | - |\n\n※削減率 = (400,000 - 候補数) / 400,000\n\n### 計算コストの観点からの評価\n\n| 手法 | Step1処理 | Step2処理 | Step3処理 | 総コスト評価 |\n|------|-----------|-----------|-----------|-------------|\n| **Overlap(8/4)** | インデックス引き（O(1)×31） | ハミング距離6.4万件 | コサイン2000件 | **低コスト** |\n| Overlap(8/2) | インデックス引き（O(1)×61） | ハミング距離8.7万件 | コサイン2000件 | 中コスト |\n| Multi-Index(5) | インデックス引き（O(1)×42） | ハミング距離4.1万件 | コサイン2000件 | **最低コスト** |\n| Partial(16/12+) | **全件走査8セグメント** | ハミング距離12.5万件 | コサイン2000件 | 高コスト |\n| 2段階検索 | **全件ハミング距離40万件** | - | コサイン2000件 | 中〜高コスト |\n\n### 枝刈りによるDB負荷低減の観点\n\n本実験の目的は、ベクトル検索のDB負荷を低減することである。以下の観点で妥協点を整理する。\n\n#### シナリオ別推奨設定\n\n| シナリオ | 推奨手法 | R@10 | 候補削減率 | 理由 |\n|---------|---------|------|-----------|------|\n| **精度重視** | Overlap(8/2) | 91.6% | 78% | 2段階と同等精度、処理高速 |\n| **バランス** | Overlap(8/4) | 90.0% | 84% | 精度-1.3pt、処理時間半減 |\n| **コスト重視** | Multi-Index(5) | 86.7% | 90% | 精度-4.6pt、候補数最小 |\n\n#### 妥協点の考察\n\n1. **Overlap(8/4)が最も有力な妥協点**\n   - Recall@10 = 90.0%（2段階91.3%との差は1.3pt）\n   - 候補数63,764件（84%削減）\n   - 処理時間20.5ms（2段階42.8msの約半分）\n   - インデックス引きのみで全件走査不要\n\n2. **Multi-Index(5)は超大規模DB向け**\n   - Recall@10 = 86.7%（精度を4.6pt犠牲にして90%削減）\n   - 数千万〜億件規模で検討価値あり\n   - インデックスサイズは増加するが、検索は高速\n\n3. **部分一致方式は不採用**\n   - 精度は高いが全件走査が必要\n   - 処理時間が2段階検索の3倍以上\n   - 枝刈りの意味がない\n\n### 結論\n\n1. **実験30からの大幅改善を達成**\n   - Seg16完全一致: 72% → Overlap(8/4): 90%（+18pt）\n   - N-gram的アプローチが有効であることを確認\n\n2. **2段階検索に匹敵する精度を達成**\n   - Overlap(8/2): 91.6% vs 2段階: 91.3%\n   - 全件ハミング距離計算を回避しつつ同等精度\n\n3. **推奨: Overlap(8/4)を初段フィルタとして採用**\n   - 40万件 → 6.4万件（84%削減）\n   - Recall@10 = 90%を維持\n   - DBから読み込むベクトル数を大幅削減可能\n\n4. **超大規模DB（数千万件以上）ではMulti-Index(5)も検討**\n   - 90%の候補削減でDB負荷を大幅軽減\n   - 精度は86.7%だが、用途によっては許容範囲\n\n### 次のステップ\n\n- Overlap(8/4)を本番実装に組み込む\n- DuckDBでのセグメントインデックス構築と検索の実装\n- 実際のクエリ（外部入力）での評価"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsh-cascade-poc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}