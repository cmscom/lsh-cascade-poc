{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query-Passage差分を考慮した超平面実験\n",
    "\n",
    "## 背景\n",
    "\n",
    "e5-largeモデルでは、検索時に以下のプレフィックスを使い分ける：\n",
    "- ドキュメント: `passage: {text}`\n",
    "- クエリ: `query: {text}`\n",
    "\n",
    "これにより検索精度が向上するが、LSHの超平面は `passage:` 同士の差分から生成されているため、\n",
    "`query:` 埋め込みとの相関が弱くなっている可能性がある。\n",
    "\n",
    "## 実験内容\n",
    "\n",
    "### アプローチ1: Query-Passage差分超平面\n",
    "同じテキストの `query:` と `passage:` 埋め込みの差分ベクトルを超平面に混ぜる。\n",
    "これにより query→passage 方向の変換を直接捉える。\n",
    "\n",
    "### アプローチ2: クエリもpassage:で埋め込み\n",
    "検索時もpassage:プレフィックスを使う。e5の推奨とは異なるが、\n",
    "同じ空間にマップされるためLSHの相関が保たれる可能性がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from src.lsh import SimHashGenerator, hamming_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. データとモデルの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DuckDBに接続\n",
    "con = duckdb.connect('../data/experiment_400k.duckdb', read_only=True)\n",
    "\n",
    "# データ件数を確認\n",
    "result = con.execute(\"SELECT dataset, COUNT(*) as cnt FROM documents GROUP BY dataset ORDER BY dataset\").fetchall()\n",
    "print('データセット別件数:')\n",
    "for dataset, cnt in result:\n",
    "    print(f'  {dataset}: {cnt:,}件')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E5モデルを読み込み\n",
    "print('E5モデルを読み込み中...')\n",
    "model = SentenceTransformer('intfloat/multilingual-e5-large')\n",
    "print('読み込み完了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全データを読み込み（評価用）\n",
    "print('データ読み込み中...')\n",
    "\n",
    "datasets = ['body_en', 'body_ja', 'titles_en', 'titles_ja']\n",
    "all_embeddings = {}\n",
    "all_texts = {}\n",
    "all_ids = {}\n",
    "\n",
    "for dataset in tqdm(datasets, desc='データセット'):\n",
    "    df = con.execute(f\"\"\"\n",
    "        SELECT id, text, embedding \n",
    "        FROM documents \n",
    "        WHERE dataset = '{dataset}'\n",
    "        ORDER BY id\n",
    "    \"\"\").fetchdf()\n",
    "    \n",
    "    embeddings = np.array(df['embedding'].tolist(), dtype=np.float32)\n",
    "    \n",
    "    all_embeddings[dataset] = embeddings\n",
    "    all_texts[dataset] = df['text'].values\n",
    "    all_ids[dataset] = df['id'].values\n",
    "    \n",
    "    print(f'  {dataset}: {len(embeddings):,}件')\n",
    "\n",
    "print('\\n読み込み完了')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Query-Passage差分の分析\n",
    "\n",
    "まず、同じテキストに対する `query:` と `passage:` 埋め込みの差を確認する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# サンプルテキストで query と passage の埋め込み差を確認\n",
    "sample_texts = [\n",
    "    '東京',\n",
    "    '人工知能',\n",
    "    'Tokyo is the capital of Japan',\n",
    "    '日本の首都である東京は、世界最大の都市圏を持つ大都市です。',\n",
    "    'Artificial intelligence is transforming industries worldwide.',\n",
    "]\n",
    "\n",
    "print('Query vs Passage 埋め込みの比較')\n",
    "print('=' * 80)\n",
    "\n",
    "for text in sample_texts:\n",
    "    query_emb = model.encode(f'query: {text}', normalize_embeddings=False)\n",
    "    passage_emb = model.encode(f'passage: {text}', normalize_embeddings=False)\n",
    "    \n",
    "    # コサイン類似度\n",
    "    cos_sim = np.dot(query_emb, passage_emb) / (norm(query_emb) * norm(passage_emb))\n",
    "    \n",
    "    # L2距離\n",
    "    l2_dist = norm(query_emb - passage_emb)\n",
    "    \n",
    "    # 差分ベクトルのノルム\n",
    "    diff_norm = norm(query_emb - passage_emb)\n",
    "    \n",
    "    print(f'\\nテキスト: {text[:50]}...' if len(text) > 50 else f'\\nテキスト: {text}')\n",
    "    print(f'  cos_sim(query, passage): {cos_sim:.4f}')\n",
    "    print(f'  L2距離: {l2_dist:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. アプローチ1: Query-Passage差分超平面\n",
    "\n",
    "超平面に query→passage 方向の差分ベクトルを混ぜる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query_passage_diff_hyperplanes(\n",
    "    texts: list,\n",
    "    model: SentenceTransformer,\n",
    "    num_hyperplanes: int,\n",
    "    seed: int = 42\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    同じテキストの query: と passage: 埋め込みの差分を超平面として生成\n",
    "    \n",
    "    Args:\n",
    "        texts: テキストリスト\n",
    "        model: SentenceTransformerモデル\n",
    "        num_hyperplanes: 生成する超平面数\n",
    "        seed: 乱数シード\n",
    "    \n",
    "    Returns:\n",
    "        超平面 (num_hyperplanes, dim)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    \n",
    "    # テキストをサンプリング\n",
    "    selected_indices = rng.choice(len(texts), min(num_hyperplanes, len(texts)), replace=False)\n",
    "    selected_texts = [texts[i] for i in selected_indices]\n",
    "    \n",
    "    # query と passage の埋め込みを生成\n",
    "    query_texts = [f'query: {t}' for t in selected_texts]\n",
    "    passage_texts = [f'passage: {t}' for t in selected_texts]\n",
    "    \n",
    "    query_embs = model.encode(query_texts, normalize_embeddings=False, show_progress_bar=True)\n",
    "    passage_embs = model.encode(passage_texts, normalize_embeddings=False, show_progress_bar=True)\n",
    "    \n",
    "    # 差分ベクトルを計算（query - passage）\n",
    "    diff_vectors = query_embs - passage_embs\n",
    "    \n",
    "    # 正規化\n",
    "    norms = np.linalg.norm(diff_vectors, axis=1, keepdims=True)\n",
    "    hyperplanes = diff_vectors / norms\n",
    "    \n",
    "    return hyperplanes.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_sampled_hyperplanes(\n",
    "    embeddings: np.ndarray,\n",
    "    num_hyperplanes: int,\n",
    "    seed: int = 42\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    データの差分ベクトルから超平面を生成（従来手法）\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    hyperplanes = []\n",
    "    \n",
    "    for _ in range(num_hyperplanes):\n",
    "        i, j = rng.choice(len(embeddings), 2, replace=False)\n",
    "        diff = embeddings[i] - embeddings[j]\n",
    "        diff = diff / np.linalg.norm(diff)\n",
    "        hyperplanes.append(diff)\n",
    "    \n",
    "    return np.array(hyperplanes, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# サンプルテキストを準備（body_jaから300件）\n",
    "rng = np.random.default_rng(42)\n",
    "sample_indices = rng.choice(len(all_texts['body_ja']), 300, replace=False)\n",
    "sample_texts = [all_texts['body_ja'][i] for i in sample_indices]\n",
    "sample_embeddings = all_embeddings['body_ja'][sample_indices]\n",
    "\n",
    "print(f'サンプルテキスト数: {len(sample_texts)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 混合超平面パターンを定義\n",
    "# 128ビット中の配分：DataSampled / QueryPassageDiff / Random\n",
    "\n",
    "patterns = {\n",
    "    'Baseline_DS': (128, 0, 0),      # 従来のDataSampledのみ\n",
    "    'QP32': (96, 32, 0),              # DataSampled + QueryPassage差分32ビット\n",
    "    'QP64': (64, 64, 0),              # DataSampled + QueryPassage差分64ビット\n",
    "    'QP96': (32, 96, 0),              # DataSampled + QueryPassage差分96ビット\n",
    "    'QP128': (0, 128, 0),             # QueryPassage差分のみ\n",
    "    'QP64_R32': (32, 64, 32),         # 混合: DS32 + QP64 + Random32\n",
    "}\n",
    "\n",
    "print('実験パターン:')\n",
    "for name, (ds, qp, r) in patterns.items():\n",
    "    print(f'  {name}: DataSampled={ds}, QueryPassageDiff={qp}, Random={r}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各パターンの超平面を生成\n",
    "print('超平面を生成中...')\n",
    "print('=' * 80)\n",
    "\n",
    "hyperplanes_patterns = {}\n",
    "\n",
    "# まず各種の超平面を生成（最大数）\n",
    "print('\\n1. DataSampled超平面を生成...')\n",
    "hp_datasampled = generate_data_sampled_hyperplanes(sample_embeddings, 128, seed=42)\n",
    "print(f'  形状: {hp_datasampled.shape}')\n",
    "\n",
    "print('\\n2. QueryPassage差分超平面を生成...')\n",
    "hp_qp_diff = generate_query_passage_diff_hyperplanes(sample_texts, model, 128, seed=42)\n",
    "print(f'  形状: {hp_qp_diff.shape}')\n",
    "\n",
    "print('\\n3. ランダム超平面を生成...')\n",
    "rng = np.random.default_rng(42)\n",
    "hp_random = rng.standard_normal((128, 1024)).astype(np.float32)\n",
    "hp_random = hp_random / np.linalg.norm(hp_random, axis=1, keepdims=True)\n",
    "print(f'  形状: {hp_random.shape}')\n",
    "\n",
    "# パターンごとに組み合わせ\n",
    "print('\\n4. 各パターンを組み合わせ...')\n",
    "for name, (num_ds, num_qp, num_r) in patterns.items():\n",
    "    parts = []\n",
    "    if num_ds > 0:\n",
    "        parts.append(hp_datasampled[:num_ds])\n",
    "    if num_qp > 0:\n",
    "        parts.append(hp_qp_diff[:num_qp])\n",
    "    if num_r > 0:\n",
    "        parts.append(hp_random[:num_r])\n",
    "    \n",
    "    hyperplanes_patterns[name] = np.vstack(parts)\n",
    "    print(f'  {name}: {hyperplanes_patterns[name].shape}')\n",
    "\n",
    "print('\\n完了')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. アプローチ2: クエリもpassage:で埋め込み\n",
    "\n",
    "検索時にもpassage:プレフィックスを使用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 検索ワード（30件）\n",
    "search_queries = [\n",
    "    # 日本語 短文クエリ - 10件\n",
    "    ('東京', 'ja', 'short'),\n",
    "    ('人工知能', 'ja', 'short'),\n",
    "    ('日本の歴史', 'ja', 'short'),\n",
    "    ('プログラミング', 'ja', 'short'),\n",
    "    ('音楽', 'ja', 'short'),\n",
    "    ('環境問題', 'ja', 'short'),\n",
    "    ('宇宙探査', 'ja', 'short'),\n",
    "    ('経済学', 'ja', 'short'),\n",
    "    ('医療技術', 'ja', 'short'),\n",
    "    ('文学作品', 'ja', 'short'),\n",
    "    \n",
    "    # 日本語 曖昧文クエリ - 10件\n",
    "    ('最近話題になっている技術革新について知りたいのですが、何かありますか', 'ja', 'ambiguous'),\n",
    "    ('日本の伝統的な文化や芸術に関する情報を探しています', 'ja', 'ambiguous'),\n",
    "    ('環境に優しい持続可能な社会を実現するための取り組みとは', 'ja', 'ambiguous'),\n",
    "    ('健康的な生活を送るために必要なことは何でしょうか', 'ja', 'ambiguous'),\n",
    "    ('世界の政治情勢や国際関係についての最新動向を教えて', 'ja', 'ambiguous'),\n",
    "    ('子供の教育において大切にすべきポイントは何ですか', 'ja', 'ambiguous'),\n",
    "    ('スポーツやフィットネスに関するトレンドを知りたい', 'ja', 'ambiguous'),\n",
    "    ('美味しい料理のレシピや食文化についての情報', 'ja', 'ambiguous'),\n",
    "    ('旅行や観光に関するおすすめの場所はありますか', 'ja', 'ambiguous'),\n",
    "    ('ビジネスや起業に関する成功のヒントを教えてください', 'ja', 'ambiguous'),\n",
    "    \n",
    "    # 英語 短文クエリ - 5件\n",
    "    ('Tokyo', 'en', 'short'),\n",
    "    ('Artificial intelligence', 'en', 'short'),\n",
    "    ('World history', 'en', 'short'),\n",
    "    ('Programming', 'en', 'short'),\n",
    "    ('Climate change', 'en', 'short'),\n",
    "    \n",
    "    # 英語 曖昧文クエリ - 5件\n",
    "    ('I want to learn about recent technological innovations', 'en', 'ambiguous'),\n",
    "    ('Looking for information about traditional culture and arts', 'en', 'ambiguous'),\n",
    "    ('What are sustainable approaches to environmental protection', 'en', 'ambiguous'),\n",
    "    ('Tell me about the latest developments in space exploration', 'en', 'ambiguous'),\n",
    "    ('What are the key factors for business success and entrepreneurship', 'en', 'ambiguous'),\n",
    "]\n",
    "\n",
    "query_texts = [q[0] for q in search_queries]\n",
    "print(f'検索ワード数: {len(query_texts)}件')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 両方のプレフィックスで埋め込みを生成\n",
    "print('クエリ埋め込みを生成中...')\n",
    "\n",
    "# アプローチ1用: query: プレフィックス\n",
    "query_texts_with_query_prefix = [f'query: {text}' for text in query_texts]\n",
    "query_embeddings_query = model.encode(query_texts_with_query_prefix, normalize_embeddings=False)\n",
    "query_embeddings_query = query_embeddings_query.astype(np.float32)\n",
    "print(f'  query:プレフィックス: {query_embeddings_query.shape}')\n",
    "\n",
    "# アプローチ2用: passage: プレフィックス\n",
    "query_texts_with_passage_prefix = [f'passage: {text}' for text in query_texts]\n",
    "query_embeddings_passage = model.encode(query_texts_with_passage_prefix, normalize_embeddings=False)\n",
    "query_embeddings_passage = query_embeddings_passage.astype(np.float32)\n",
    "print(f'  passage:プレフィックス: {query_embeddings_passage.shape}')\n",
    "\n",
    "print('完了')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全データを統合\n",
    "print('全データを統合中...')\n",
    "\n",
    "all_embeddings_flat = np.vstack([all_embeddings[d] for d in datasets])\n",
    "all_datasets_flat = []\n",
    "all_ids_flat = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    all_datasets_flat.extend([dataset] * len(all_embeddings[dataset]))\n",
    "    all_ids_flat.extend(all_ids[dataset])\n",
    "\n",
    "print(f'統合完了: {all_embeddings_flat.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_recall(\n",
    "    query_embeddings: np.ndarray,\n",
    "    all_embeddings: np.ndarray,\n",
    "    hyperplanes: np.ndarray,\n",
    "    candidate_limit: int = 2000,\n",
    "    top_k: int = 10\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    クエリに対するRecall@kを評価\n",
    "    \n",
    "    Returns:\n",
    "        各クエリのRecall値リスト\n",
    "    \"\"\"\n",
    "    # SimHashGeneratorを作成して超平面を設定\n",
    "    gen = SimHashGenerator(dim=1024, hash_bits=128, seed=0, strategy='random')\n",
    "    gen.hyperplanes = hyperplanes\n",
    "    \n",
    "    # 全ドキュメントのハッシュを計算\n",
    "    all_hashes = gen.hash_batch(all_embeddings)\n",
    "    \n",
    "    # クエリのハッシュを計算\n",
    "    query_hashes = gen.hash_batch(query_embeddings)\n",
    "    \n",
    "    recalls = []\n",
    "    \n",
    "    for i in range(len(query_embeddings)):\n",
    "        query_emb = query_embeddings[i]\n",
    "        query_hash = query_hashes[i]\n",
    "        \n",
    "        # Ground Truth（コサイン類似度Top-k）\n",
    "        cos_sims = (all_embeddings @ query_emb) / (norm(all_embeddings, axis=1) * norm(query_emb))\n",
    "        gt_indices = set(np.argsort(cos_sims)[::-1][:top_k])\n",
    "        \n",
    "        # LSH候補（ハミング距離Top-candidate_limit）\n",
    "        distances = [(j, hamming_distance(h, query_hash)) for j, h in enumerate(all_hashes)]\n",
    "        distances.sort(key=lambda x: x[1])\n",
    "        candidates = set(idx for idx, _ in distances[:candidate_limit])\n",
    "        \n",
    "        recall = len(gt_indices & candidates) / top_k\n",
    "        recalls.append(recall)\n",
    "    \n",
    "    return recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評価実行\n",
    "print('評価を実行中...')\n",
    "print('=' * 100)\n",
    "\n",
    "candidate_limits = [1000, 2000, 5000]\n",
    "results = []\n",
    "\n",
    "for pattern_name, hyperplanes in tqdm(hyperplanes_patterns.items(), desc='パターン'):\n",
    "    for limit in candidate_limits:\n",
    "        # アプローチ1: query:プレフィックス + 各種超平面\n",
    "        recalls_query = evaluate_recall(\n",
    "            query_embeddings_query,\n",
    "            all_embeddings_flat,\n",
    "            hyperplanes,\n",
    "            candidate_limit=limit\n",
    "        )\n",
    "        \n",
    "        for i, (query_text, lang, query_type) in enumerate(search_queries):\n",
    "            results.append({\n",
    "                'pattern': pattern_name,\n",
    "                'query_prefix': 'query:',\n",
    "                'candidate_limit': limit,\n",
    "                'query': query_text,\n",
    "                'lang': lang,\n",
    "                'query_type': query_type,\n",
    "                'recall': recalls_query[i]\n",
    "            })\n",
    "        \n",
    "        # アプローチ2: passage:プレフィックス + Baseline_DSのみ\n",
    "        if pattern_name == 'Baseline_DS':\n",
    "            recalls_passage = evaluate_recall(\n",
    "                query_embeddings_passage,\n",
    "                all_embeddings_flat,\n",
    "                hyperplanes,\n",
    "                candidate_limit=limit\n",
    "            )\n",
    "            \n",
    "            for i, (query_text, lang, query_type) in enumerate(search_queries):\n",
    "                results.append({\n",
    "                    'pattern': 'Baseline_DS_PassageQuery',\n",
    "                    'query_prefix': 'passage:',\n",
    "                    'candidate_limit': limit,\n",
    "                    'query': query_text,\n",
    "                    'lang': lang,\n",
    "                    'query_type': query_type,\n",
    "                    'recall': recalls_passage[i]\n",
    "                })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print('\\n完了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果サマリー\n",
    "print('=' * 100)\n",
    "print('外部クエリ検索 Recall@10（30クエリ平均）')\n",
    "print('=' * 100)\n",
    "\n",
    "# パターン × 候補数別の平均Recall\n",
    "pivot = df_results.groupby(['pattern', 'candidate_limit'])['recall'].mean().unstack()\n",
    "\n",
    "# パターン順にソート\n",
    "pattern_order = ['Baseline_DS', 'Baseline_DS_PassageQuery', 'QP32', 'QP64', 'QP96', 'QP128', 'QP64_R32']\n",
    "pivot = pivot.reindex([p for p in pattern_order if p in pivot.index])\n",
    "\n",
    "print(f'\\n{\"パターン\":>30} | {\"クエリ接頭辞\":>12} | {\"1000件\":>10} | {\"2000件\":>10} | {\"5000件\":>10}')\n",
    "print('-' * 90)\n",
    "\n",
    "for pattern in pivot.index:\n",
    "    row = pivot.loc[pattern]\n",
    "    prefix = df_results[df_results['pattern'] == pattern]['query_prefix'].iloc[0]\n",
    "    print(f'{pattern:>30} | {prefix:>12} | {row[1000]:>10.1%} | {row[2000]:>10.1%} | {row[5000]:>10.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# クエリタイプ別の結果（候補2000件）\n",
    "print('\\n' + '=' * 100)\n",
    "print('クエリタイプ別 Recall@10（候補2000件）')\n",
    "print('=' * 100)\n",
    "\n",
    "subset = df_results[df_results['candidate_limit'] == 2000]\n",
    "\n",
    "print(f'\\n{\"パターン\":>30} | {\"JA短文\":>10} | {\"JA曖昧\":>10} | {\"EN短文\":>10} | {\"EN曖昧\":>10}')\n",
    "print('-' * 85)\n",
    "\n",
    "for pattern in [p for p in pattern_order if p in pivot.index]:\n",
    "    pattern_subset = subset[subset['pattern'] == pattern]\n",
    "    \n",
    "    ja_short = pattern_subset[(pattern_subset['lang'] == 'ja') & (pattern_subset['query_type'] == 'short')]['recall'].mean()\n",
    "    ja_amb = pattern_subset[(pattern_subset['lang'] == 'ja') & (pattern_subset['query_type'] == 'ambiguous')]['recall'].mean()\n",
    "    en_short = pattern_subset[(pattern_subset['lang'] == 'en') & (pattern_subset['query_type'] == 'short')]['recall'].mean()\n",
    "    en_amb = pattern_subset[(pattern_subset['lang'] == 'en') & (pattern_subset['query_type'] == 'ambiguous')]['recall'].mean()\n",
    "    \n",
    "    print(f'{pattern:>30} | {ja_short:>10.1%} | {ja_amb:>10.1%} | {en_short:>10.1%} | {en_amb:>10.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 詳細分析: クエリ「東京」でのハミング距離確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 「東京」クエリの詳細分析\n",
    "print('=' * 100)\n",
    "print('クエリ「東京」のGround Truth分析')\n",
    "print('=' * 100)\n",
    "\n",
    "test_idx = 0  # 「東京」\n",
    "\n",
    "# コサイン類似度を計算（query:プレフィックス使用）\n",
    "query_emb_q = query_embeddings_query[test_idx]\n",
    "cos_sims_q = (all_embeddings_flat @ query_emb_q) / (norm(all_embeddings_flat, axis=1) * norm(query_emb_q))\n",
    "gt_indices_q = np.argsort(cos_sims_q)[::-1][:10]\n",
    "\n",
    "# コサイン類似度を計算（passage:プレフィックス使用）\n",
    "query_emb_p = query_embeddings_passage[test_idx]\n",
    "cos_sims_p = (all_embeddings_flat @ query_emb_p) / (norm(all_embeddings_flat, axis=1) * norm(query_emb_p))\n",
    "gt_indices_p = np.argsort(cos_sims_p)[::-1][:10]\n",
    "\n",
    "print('\\nGround Truth比較:')\n",
    "print(f'  query:プレフィックス時のTop-10: {list(gt_indices_q)}')\n",
    "print(f'  passage:プレフィックス時のTop-10: {list(gt_indices_p)}')\n",
    "print(f'  共通件数: {len(set(gt_indices_q) & set(gt_indices_p))}/10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各パターンでのハミング距離\n",
    "print('\\n各パターンでのGT Top-10のハミング距離 (query:プレフィックス使用時):')\n",
    "print(f'{\"GT#\":>5} | {\"cos_sim\":>8} | {\"dataset\":>12} | ', end='')\n",
    "for pattern in ['Baseline_DS', 'QP64', 'QP128']:\n",
    "    print(f'{pattern:>15} | ', end='')\n",
    "print()\n",
    "print('-' * 90)\n",
    "\n",
    "for rank, idx in enumerate(gt_indices_q):\n",
    "    print(f'{rank+1:>5} | {cos_sims_q[idx]:>8.4f} | {all_datasets_flat[idx]:>12} | ', end='')\n",
    "    \n",
    "    for pattern in ['Baseline_DS', 'QP64', 'QP128']:\n",
    "        gen = SimHashGenerator(dim=1024, hash_bits=128, seed=0, strategy='random')\n",
    "        gen.hyperplanes = hyperplanes_patterns[pattern]\n",
    "        \n",
    "        doc_hash = gen.hash_batch(all_embeddings_flat[idx:idx+1])[0]\n",
    "        query_hash = gen.hash_batch(query_emb_q.reshape(1, -1))[0]\n",
    "        ham_dist = hamming_distance(doc_hash, query_hash)\n",
    "        print(f'{ham_dist:>15} | ', end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 結果を可視化\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "patterns_to_plot = [p for p in pattern_order if p in pivot.index]\n",
    "x = np.arange(len(patterns_to_plot))\n",
    "width = 0.25\n",
    "\n",
    "for i, limit in enumerate(candidate_limits):\n",
    "    values = [pivot.loc[p, limit] for p in patterns_to_plot]\n",
    "    ax.bar(x + i*width, values, width, label=f'{limit:,}件')\n",
    "\n",
    "ax.axhline(y=0.90, color='gray', linestyle=':', label='目標90%')\n",
    "ax.set_ylabel('Recall@10')\n",
    "ax.set_title('外部クエリ検索 Recall@10（30クエリ平均）')\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels(patterns_to_plot, rotation=45, ha='right')\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/18_query_passage_hyperplanes.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('グラフを data/18_query_passage_hyperplanes.png に保存しました')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 結論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 接続を閉じる\n",
    "con.close()\n",
    "print('DuckDB接続を閉じました')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結果サマリー\n",
    "\n",
    "### アプローチ1: Query-Passage差分超平面\n",
    "\n",
    "- query: と passage: の埋め込み差分を超平面に混ぜることで、query→passage方向の変換を捉える\n",
    "- 結果: [実験後に記入]\n",
    "\n",
    "### アプローチ2: クエリもpassage:で埋め込み\n",
    "\n",
    "- 検索時もpassage:プレフィックスを使用し、同じ空間にマップ\n",
    "- 結果: [実験後に記入]\n",
    "\n",
    "### 考察\n",
    "\n",
    "[実験後に記入]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
