{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19. Dual Prefix Search: passage/queryプレフィックスの使い分け実験\n",
    "\n",
    "## 背景\n",
    "\n",
    "e5-largeモデルでは、検索時に以下のプレフィックスを使い分けることが推奨されている：\n",
    "- ドキュメント埋め込み: `passage: {text}`\n",
    "- クエリ埋め込み: `query: {text}`\n",
    "\n",
    "この非対称設計により検索精度が向上するが、LSH（SimHash）との相性に問題があることが判明した。\n",
    "\n",
    "## 実験の目的\n",
    "\n",
    "1. `passage:` と `query:` プレフィックスによるベクトル変化を分析\n",
    "2. 2つのプレフィックスを使い分けた検索手法を検証\n",
    "3. **LSHにはpassage:、最終コサイン計算にはquery:を使う手法**の詳細検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.lsh import SimHashGenerator, hamming_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. データとモデルの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DuckDBに接続\n",
    "con = duckdb.connect('../data/experiment_400k.duckdb', read_only=True)\n",
    "\n",
    "# E5モデルを読み込み\n",
    "print('E5モデルを読み込み中...')\n",
    "model = SentenceTransformer('intfloat/multilingual-e5-large')\n",
    "print('完了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ読み込み\n",
    "print('データ読み込み中...')\n",
    "datasets = ['body_en', 'body_ja', 'titles_en', 'titles_ja']\n",
    "all_embeddings = {}\n",
    "all_texts = {}\n",
    "\n",
    "for dataset in tqdm(datasets, desc='データセット'):\n",
    "    df = con.execute(f\"\"\"\n",
    "        SELECT text, embedding\n",
    "        FROM documents\n",
    "        WHERE dataset = '{dataset}'\n",
    "        ORDER BY id\n",
    "    \"\"\").fetchdf()\n",
    "    \n",
    "    embeddings = np.array(df['embedding'].tolist(), dtype=np.float32)\n",
    "    all_embeddings[dataset] = embeddings\n",
    "    all_texts[dataset] = df['text'].values\n",
    "\n",
    "# 全データを統合\n",
    "all_embeddings_flat = np.vstack([all_embeddings[d] for d in datasets])\n",
    "all_datasets_flat = []\n",
    "for dataset in datasets:\n",
    "    all_datasets_flat.extend([dataset] * len(all_embeddings[dataset]))\n",
    "\n",
    "print(f'統合完了: {all_embeddings_flat.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Phase 1: passage: と query: のベクトル変化分析\n",
    "\n",
    "同じテキストに対して `passage:` と `query:` プレフィックスを付けた場合、\n",
    "埋め込みベクトルがどのように変化するかを分析する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# サンプルテキストを取得（各データセットから50件）\n",
    "sample_texts = []\n",
    "sample_metadata = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    df = con.execute(f\"\"\"\n",
    "        SELECT text, lang FROM documents\n",
    "        WHERE dataset = '{dataset}'\n",
    "        ORDER BY RANDOM()\n",
    "        LIMIT 50\n",
    "    \"\"\").fetchdf()\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        sample_texts.append(row['text'])\n",
    "        sample_metadata.append({\n",
    "            'dataset': dataset,\n",
    "            'lang': row['lang'],\n",
    "            'length': len(row['text']),\n",
    "            'type': 'body' if 'body' in dataset else 'title'\n",
    "        })\n",
    "\n",
    "print(f'サンプルテキスト数: {len(sample_texts)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 両方のプレフィックスで埋め込み生成\n",
    "print('埋め込み生成中...')\n",
    "passage_embs = model.encode([f'passage: {t}' for t in sample_texts], normalize_embeddings=False)\n",
    "query_embs = model.encode([f'query: {t}' for t in sample_texts], normalize_embeddings=False)\n",
    "print('完了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本統計\n",
    "cos_sims = []\n",
    "l2_dists = []\n",
    "diff_vectors = []\n",
    "\n",
    "for i in range(len(sample_texts)):\n",
    "    p_emb = passage_embs[i]\n",
    "    q_emb = query_embs[i]\n",
    "    \n",
    "    cos_sim = np.dot(p_emb, q_emb) / (norm(p_emb) * norm(q_emb))\n",
    "    l2_dist = norm(p_emb - q_emb)\n",
    "    diff = q_emb - p_emb\n",
    "    \n",
    "    cos_sims.append(cos_sim)\n",
    "    l2_dists.append(l2_dist)\n",
    "    diff_vectors.append(diff)\n",
    "\n",
    "diff_vectors = np.array(diff_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 1 結果: ベクトル変化の統計"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 70)\n",
    "print('Phase 1 結果: passage: と query: のベクトル変化')\n",
    "print('=' * 70)\n",
    "\n",
    "print(f'''\n",
    "■ 同じテキストの passage vs query 類似度\n",
    "  コサイン類似度:\n",
    "    平均: {np.mean(cos_sims):.4f}\n",
    "    標準偏差: {np.std(cos_sims):.4f}\n",
    "    最小: {np.min(cos_sims):.4f}\n",
    "    最大: {np.max(cos_sims):.4f}\n",
    "\n",
    "  L2距離:\n",
    "    平均: {np.mean(l2_dists):.4f}\n",
    "    標準偏差: {np.std(l2_dists):.4f}\n",
    "''')\n",
    "\n",
    "# タイプ別統計\n",
    "df_stats = pd.DataFrame({\n",
    "    'type': [m['type'] for m in sample_metadata],\n",
    "    'lang': [m['lang'] for m in sample_metadata],\n",
    "    'cos_sim': cos_sims,\n",
    "    'l2_dist': l2_dists\n",
    "})\n",
    "\n",
    "print('■ タイプ別（body vs title）:')\n",
    "for text_type in ['body', 'title']:\n",
    "    subset = df_stats[df_stats['type'] == text_type]\n",
    "    print(f'    {text_type}: cos_sim={subset[\"cos_sim\"].mean():.4f}, l2_dist={subset[\"l2_dist\"].mean():.4f}')\n",
    "\n",
    "print('\\n■ 言語別:')\n",
    "for lang in ['ja', 'en']:\n",
    "    subset = df_stats[df_stats['lang'] == lang]\n",
    "    print(f'    {lang}: cos_sim={subset[\"cos_sim\"].mean():.4f}, l2_dist={subset[\"l2_dist\"].mean():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 差分ベクトルの方向性分析\n",
    "diff_vectors_normalized = diff_vectors / np.linalg.norm(diff_vectors, axis=1, keepdims=True)\n",
    "\n",
    "# 差分ベクトル同士のコサイン類似度\n",
    "diff_cos_sims = []\n",
    "for i in range(len(diff_vectors_normalized)):\n",
    "    for j in range(i+1, len(diff_vectors_normalized)):\n",
    "        sim = np.dot(diff_vectors_normalized[i], diff_vectors_normalized[j])\n",
    "        diff_cos_sims.append(sim)\n",
    "\n",
    "print(f'''\n",
    "■ 差分ベクトル（query - passage）の方向性分析\n",
    "  差分ベクトル間のコサイン類似度:\n",
    "    平均: {np.mean(diff_cos_sims):.4f}\n",
    "    標準偏差: {np.std(diff_cos_sims):.4f}\n",
    "    → 値が低い = 方向性がランダムに近い\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 1 考察\n",
    "\n",
    "**発見事項:**\n",
    "1. 同じテキストでも `passage:` と `query:` では異なるベクトルになる（類似度≈0.93）\n",
    "2. **短文（title）の方が差が大きい**（類似度≈0.91 vs body≈0.95）\n",
    "3. 差分ベクトルの方向性は**ランダムに近い**（類似度≈0.19）\n",
    "4. 言語による差は小さい\n",
    "\n",
    "**示唆:**\n",
    "- `passage:` で埋め込んだドキュメントに対して `query:` で検索すると、LSHのハミング距離が最適でなくなる\n",
    "- 差分ベクトルに共通の方向性がないため、単純な線形変換では補正が難しい"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Phase 2: 検索手法の比較実験\n",
    "\n",
    "以下の5つの手法を比較:\n",
    "\n",
    "| 手法 | LSHプレフィックス | Ground Truthプレフィックス |\n",
    "|------|-------------------|---------------------------|\n",
    "| 1. Baseline_Query | query: | query: |\n",
    "| 2. Baseline_Passage | passage: | passage: |\n",
    "| 3. Hybrid | passage: | query: |\n",
    "| 4. MeanDiff_Transform | transformed | query: |\n",
    "| 5. Dual_Merge | query+passage | query: |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超平面を準備（DataSampled）\n",
    "rng = np.random.default_rng(42)\n",
    "sample_indices = rng.choice(len(all_embeddings['body_ja']), 300, replace=False)\n",
    "sample_embeddings = all_embeddings['body_ja'][sample_indices]\n",
    "\n",
    "hyperplanes = []\n",
    "for _ in range(128):\n",
    "    i, j = rng.choice(len(sample_embeddings), 2, replace=False)\n",
    "    diff = sample_embeddings[i] - sample_embeddings[j]\n",
    "    diff = diff / np.linalg.norm(diff)\n",
    "    hyperplanes.append(diff)\n",
    "hyperplanes = np.array(hyperplanes, dtype=np.float32)\n",
    "\n",
    "# SimHashGeneratorを設定\n",
    "gen = SimHashGenerator(dim=1024, hash_bits=128, seed=0, strategy='random')\n",
    "gen.hyperplanes = hyperplanes\n",
    "\n",
    "# 全ドキュメントのハッシュを計算\n",
    "print('SimHashを計算中...')\n",
    "all_hashes = gen.hash_batch(all_embeddings_flat)\n",
    "print(f'完了: {len(all_hashes)}件')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 検索クエリ\n",
    "search_queries = [\n",
    "    ('東京', 'ja', 'short'),\n",
    "    ('人工知能', 'ja', 'short'),\n",
    "    ('日本の歴史', 'ja', 'short'),\n",
    "    ('プログラミング', 'ja', 'short'),\n",
    "    ('音楽', 'ja', 'short'),\n",
    "    ('環境問題', 'ja', 'short'),\n",
    "    ('宇宙探査', 'ja', 'short'),\n",
    "    ('経済学', 'ja', 'short'),\n",
    "    ('医療技術', 'ja', 'short'),\n",
    "    ('文学作品', 'ja', 'short'),\n",
    "    ('最近話題になっている技術革新について知りたいのですが、何かありますか', 'ja', 'ambiguous'),\n",
    "    ('日本の伝統的な文化や芸術に関する情報を探しています', 'ja', 'ambiguous'),\n",
    "    ('環境に優しい持続可能な社会を実現するための取り組みとは', 'ja', 'ambiguous'),\n",
    "    ('健康的な生活を送るために必要なことは何でしょうか', 'ja', 'ambiguous'),\n",
    "    ('世界の政治情勢や国際関係についての最新動向を教えて', 'ja', 'ambiguous'),\n",
    "    ('子供の教育において大切にすべきポイントは何ですか', 'ja', 'ambiguous'),\n",
    "    ('スポーツやフィットネスに関するトレンドを知りたい', 'ja', 'ambiguous'),\n",
    "    ('美味しい料理のレシピや食文化についての情報', 'ja', 'ambiguous'),\n",
    "    ('旅行や観光に関するおすすめの場所はありますか', 'ja', 'ambiguous'),\n",
    "    ('ビジネスや起業に関する成功のヒントを教えてください', 'ja', 'ambiguous'),\n",
    "    ('Tokyo', 'en', 'short'),\n",
    "    ('Artificial intelligence', 'en', 'short'),\n",
    "    ('World history', 'en', 'short'),\n",
    "    ('Programming', 'en', 'short'),\n",
    "    ('Climate change', 'en', 'short'),\n",
    "    ('I want to learn about recent technological innovations', 'en', 'ambiguous'),\n",
    "    ('Looking for information about traditional culture and arts', 'en', 'ambiguous'),\n",
    "    ('What are sustainable approaches to environmental protection', 'en', 'ambiguous'),\n",
    "    ('Tell me about the latest developments in space exploration', 'en', 'ambiguous'),\n",
    "    ('What are the key factors for business success and entrepreneurship', 'en', 'ambiguous'),\n",
    "]\n",
    "\n",
    "query_texts = [q[0] for q in search_queries]\n",
    "print(f'検索クエリ数: {len(query_texts)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2 結果: 手法比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 事前に計算した結果（scripts/search_with_dual_prefix.py より）\n",
    "\n",
    "phase2_results = {\n",
    "    '1_Baseline_Query': {'lsh': 'query:', 'gt': 'query:', 1000: 0.243, 2000: 0.323, 5000: 0.450},\n",
    "    '2_Baseline_Passage': {'lsh': 'passage:', 'gt': 'passage:', 1000: 0.613, 2000: 0.693, 5000: 0.807},\n",
    "    '3_Hybrid_PassageLSH_QueryCos': {'lsh': 'passage:', 'gt': 'query:', 1000: 0.433, 2000: 0.500, 5000: 0.600},\n",
    "    '4_MeanDiff_Transform': {'lsh': 'transformed', 'gt': 'query:', 1000: 0.270, 2000: 0.353, 5000: 0.467},\n",
    "    '5_Dual_Merge': {'lsh': 'query+passage', 'gt': 'query:', 1000: 0.437, 2000: 0.517, 5000: 0.607},\n",
    "}\n",
    "\n",
    "print('=' * 90)\n",
    "print('Phase 2 結果: 外部クエリ検索 Recall@10（30クエリ平均）')\n",
    "print('=' * 90)\n",
    "print(f'{\"手法\":>35} | {\"LSH\":>12} | {\"GT\":>8} | {\"1000件\":>10} | {\"2000件\":>10} | {\"5000件\":>10}')\n",
    "print('-' * 90)\n",
    "\n",
    "for method, data in phase2_results.items():\n",
    "    print(f'{method:>35} | {data[\"lsh\"]:>12} | {data[\"gt\"]:>8} | {data[1000]:>10.1%} | {data[2000]:>10.1%} | {data[5000]:>10.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# クエリタイプ別（候補2000件）- 事前計算結果\n",
    "phase2_by_type = {\n",
    "    '1_Baseline_Query': {'ja_short': 0.34, 'ja_amb': 0.23, 'en_short': 0.58, 'en_amb': 0.22},\n",
    "    '2_Baseline_Passage': {'ja_short': 0.81, 'ja_amb': 0.63, 'en_short': 0.74, 'en_amb': 0.54},\n",
    "    '3_Hybrid_PassageLSH_QueryCos': {'ja_short': 0.61, 'ja_amb': 0.35, 'en_short': 0.70, 'en_amb': 0.38},\n",
    "    '4_MeanDiff_Transform': {'ja_short': 0.45, 'ja_amb': 0.18, 'en_short': 0.58, 'en_amb': 0.28},\n",
    "    '5_Dual_Merge': {'ja_short': 0.63, 'ja_amb': 0.40, 'en_short': 0.72, 'en_amb': 0.32},\n",
    "}\n",
    "\n",
    "print('\\n' + '=' * 85)\n",
    "print('クエリタイプ別 Recall@10（候補2000件）')\n",
    "print('=' * 85)\n",
    "print(f'{\"手法\":>35} | {\"JA短文\":>10} | {\"JA曖昧\":>10} | {\"EN短文\":>10} | {\"EN曖昧\":>10}')\n",
    "print('-' * 85)\n",
    "\n",
    "for method, data in phase2_by_type.items():\n",
    "    print(f'{method:>35} | {data[\"ja_short\"]:>10.1%} | {data[\"ja_amb\"]:>10.1%} | {data[\"en_short\"]:>10.1%} | {data[\"en_amb\"]:>10.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2 重要な発見: Ground Truthの不一致\n",
    "\n",
    "**query: と passage: で検索すると、正解（Top-10）自体が大きく変わる！**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 70)\n",
    "print('Ground Truth (Top-10) の一致率')\n",
    "print('=' * 70)\n",
    "print('''\n",
    "query: と passage: のGround Truth比較（事前計算結果）:\n",
    "\n",
    "  平均一致率: 3.4/10 件\n",
    "  最小一致率: 0/10 件\n",
    "  最大一致率: 7/10 件\n",
    "\n",
    "  クエリタイプ別:\n",
    "    短文クエリ: 3.0/10 件\n",
    "    曖昧クエリ: 3.9/10 件\n",
    "\n",
    "→ つまり、プレフィックスの選択で「正解」自体が変わってしまう\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2 考察\n",
    "\n",
    "| 手法 | Recall | メリット | デメリット |\n",
    "|------|--------|----------|------------|\n",
    "| Baseline_Query | 32.3% | e5の非対称検索を維持 | LSH精度が低い |\n",
    "| Baseline_Passage | 69.3% | LSH精度が高い | e5の非対称検索を放棄 |\n",
    "| **Hybrid** | **50.0%** | **両方のメリットを部分的に享受** | **中間的な性能** |\n",
    "| Dual_Merge | 51.7% | 候補を増やせる | 計算量2倍 |\n",
    "\n",
    "**結論:** Hybrid手法（LSH=passage:, GT=query:）は従来手法から+17.7pt改善"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Phase 3: Hybrid手法の詳細検証\n",
    "\n",
    "**検索フロー:**\n",
    "1. クエリに対して2つの埋め込みを生成:\n",
    "   - `passage: {query}` → LSH候補選択用\n",
    "   - `query: {query}` → 最終コサイン計算用\n",
    "2. LSH（ハミング距離）で候補を絞り込み（passage:埋め込み使用）\n",
    "3. 候補に対してコサイン類似度を計算（query:埋め込み使用）\n",
    "4. コサイン類似度でランキング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# クエリ埋め込みを生成（両方のプレフィックス）\n",
    "print('クエリ埋め込みを生成中...')\n",
    "query_embs_passage = model.encode([f'passage: {t}' for t in query_texts], normalize_embeddings=False).astype(np.float32)\n",
    "query_embs_query = model.encode([f'query: {t}' for t in query_texts], normalize_embeddings=False).astype(np.float32)\n",
    "print('完了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(\n",
    "    query_emb_passage: np.ndarray,  # LSH候補選択用（passage:プレフィックス）\n",
    "    query_emb_query: np.ndarray,    # コサイン計算用（query:プレフィックス）\n",
    "    all_embeddings: np.ndarray,\n",
    "    all_hashes: list,\n",
    "    gen: SimHashGenerator,\n",
    "    candidate_limit: int,\n",
    "    top_k: int = 10\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Hybrid検索: LSHにはpassage:、最終コサインにはquery:を使用\n",
    "    \n",
    "    Returns:\n",
    "        dict: {\n",
    "            'candidates': LSH候補インデックスのリスト,\n",
    "            'top_k_indices': コサイン類似度Top-kのインデックス,\n",
    "            'top_k_scores': コサイン類似度Top-kのスコア,\n",
    "            'gt_query': Ground Truth (query:プレフィックス),\n",
    "            'recall': Recall@k\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Step 1: LSHで候補選択（passage:埋め込み使用）\n",
    "    query_hash = gen.hash_batch(query_emb_passage.reshape(1, -1))[0]\n",
    "    distances = [(j, hamming_distance(h, query_hash)) for j, h in enumerate(all_hashes)]\n",
    "    distances.sort(key=lambda x: x[1])\n",
    "    candidates = [idx for idx, _ in distances[:candidate_limit]]\n",
    "    \n",
    "    # Step 2: 候補に対してコサイン類似度を計算（query:埋め込み使用）\n",
    "    candidate_embeddings = all_embeddings[candidates]\n",
    "    cos_sims = (candidate_embeddings @ query_emb_query) / (\n",
    "        norm(candidate_embeddings, axis=1) * norm(query_emb_query)\n",
    "    )\n",
    "    \n",
    "    # Step 3: コサイン類似度でランキング\n",
    "    sorted_indices = np.argsort(cos_sims)[::-1][:top_k]\n",
    "    top_k_indices = [candidates[i] for i in sorted_indices]\n",
    "    top_k_scores = [cos_sims[i] for i in sorted_indices]\n",
    "    \n",
    "    # Ground Truth（query:プレフィックスで全データから計算）\n",
    "    all_cos_sims = (all_embeddings @ query_emb_query) / (\n",
    "        norm(all_embeddings, axis=1) * norm(query_emb_query)\n",
    "    )\n",
    "    gt_query = set(np.argsort(all_cos_sims)[::-1][:top_k])\n",
    "    \n",
    "    # Recall計算\n",
    "    recall = len(set(top_k_indices) & gt_query) / top_k\n",
    "    \n",
    "    return {\n",
    "        'candidates': candidates,\n",
    "        'top_k_indices': top_k_indices,\n",
    "        'top_k_scores': top_k_scores,\n",
    "        'gt_query': gt_query,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid検索を全クエリで評価\n",
    "print('Hybrid検索を評価中...')\n",
    "\n",
    "candidate_limits = [500, 1000, 2000, 5000, 10000]\n",
    "hybrid_results = []\n",
    "\n",
    "for i, (query_text, lang, query_type) in enumerate(tqdm(search_queries, desc='クエリ')):\n",
    "    q_emb_p = query_embs_passage[i]\n",
    "    q_emb_q = query_embs_query[i]\n",
    "    \n",
    "    for limit in candidate_limits:\n",
    "        result = hybrid_search(\n",
    "            q_emb_p, q_emb_q,\n",
    "            all_embeddings_flat, all_hashes, gen,\n",
    "            candidate_limit=limit\n",
    "        )\n",
    "        \n",
    "        hybrid_results.append({\n",
    "            'query': query_text,\n",
    "            'lang': lang,\n",
    "            'query_type': query_type,\n",
    "            'candidate_limit': limit,\n",
    "            'recall': result['recall']\n",
    "        })\n",
    "\n",
    "df_hybrid = pd.DataFrame(hybrid_results)\n",
    "print('完了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid手法の詳細結果\n",
    "print('=' * 80)\n",
    "print('Phase 3 結果: Hybrid手法（LSH=passage:, Cos=query:）の詳細')\n",
    "print('=' * 80)\n",
    "\n",
    "pivot = df_hybrid.groupby('candidate_limit')['recall'].agg(['mean', 'std', 'min', 'max'])\n",
    "pivot.columns = ['平均', '標準偏差', '最小', '最大']\n",
    "\n",
    "print('\\n■ 候補数別 Recall@10')\n",
    "print(pivot.to_string(formatters={\n",
    "    '平均': '{:.1%}'.format,\n",
    "    '標準偏差': '{:.1%}'.format,\n",
    "    '最小': '{:.1%}'.format,\n",
    "    '最大': '{:.1%}'.format\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# クエリタイプ別\n",
    "print('\\n■ クエリタイプ別 Recall@10（候補2000件）')\n",
    "subset = df_hybrid[df_hybrid['candidate_limit'] == 2000]\n",
    "\n",
    "for qtype in ['short', 'ambiguous']:\n",
    "    for lang in ['ja', 'en']:\n",
    "        filtered = subset[(subset['query_type'] == qtype) & (subset['lang'] == lang)]\n",
    "        print(f'    {lang}_{qtype}: {filtered[\"recall\"].mean():.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 個別クエリの詳細分析（「東京」）\n",
    "print('\\n' + '=' * 80)\n",
    "print('詳細分析: クエリ「東京」')\n",
    "print('=' * 80)\n",
    "\n",
    "test_idx = 0  # 「東京」\n",
    "q_emb_p = query_embs_passage[test_idx]\n",
    "q_emb_q = query_embs_query[test_idx]\n",
    "\n",
    "result = hybrid_search(\n",
    "    q_emb_p, q_emb_q,\n",
    "    all_embeddings_flat, all_hashes, gen,\n",
    "    candidate_limit=2000\n",
    ")\n",
    "\n",
    "print(f'''\n",
    "クエリ: 「東京」\n",
    "候補数: 2000件\n",
    "\n",
    "Ground Truth (query:プレフィックス) Top-10:\n",
    "  インデックス: {sorted(result[\"gt_query\"])}\n",
    "\n",
    "Hybrid検索 Top-10:\n",
    "  インデックス: {result[\"top_k_indices\"]}\n",
    "  コサイン類似度: {[f\"{s:.4f}\" for s in result[\"top_k_scores\"]]}\n",
    "\n",
    "Recall@10: {result[\"recall\"]:.1%}\n",
    "一致件数: {len(set(result[\"top_k_indices\"]) & result[\"gt_query\"])}/10\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可視化: 候補数 vs Recall\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 全体平均\n",
    "ax1 = axes[0]\n",
    "pivot_mean = df_hybrid.groupby('candidate_limit')['recall'].mean()\n",
    "ax1.plot(pivot_mean.index, pivot_mean.values, 'bo-', linewidth=2, markersize=8)\n",
    "ax1.axhline(y=0.90, color='red', linestyle='--', alpha=0.7, label='Target 90%')\n",
    "ax1.set_xlabel('Candidate Limit')\n",
    "ax1.set_ylabel('Recall@10')\n",
    "ax1.set_title('Hybrid Search: Candidate Limit vs Recall (All Queries)')\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "\n",
    "# クエリタイプ別\n",
    "ax2 = axes[1]\n",
    "for qtype in ['short', 'ambiguous']:\n",
    "    for lang in ['ja', 'en']:\n",
    "        filtered = df_hybrid[(df_hybrid['query_type'] == qtype) & (df_hybrid['lang'] == lang)]\n",
    "        pivot_type = filtered.groupby('candidate_limit')['recall'].mean()\n",
    "        ax2.plot(pivot_type.index, pivot_type.values, 'o-', label=f'{lang}_{qtype}')\n",
    "\n",
    "ax2.axhline(y=0.90, color='red', linestyle='--', alpha=0.7, label='Target 90%')\n",
    "ax2.set_xlabel('Candidate Limit')\n",
    "ax2.set_ylabel('Recall@10')\n",
    "ax2.set_title('Hybrid Search: By Query Type')\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/19_hybrid_search_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('グラフを data/19_hybrid_search_results.png に保存しました')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Phase 3 実行結果: Hybrid検索の詳細検証\n\n`scripts/hybrid_search_detail.py` を実行した結果を以下に示す。\n\n## 候補数別 Recall@10（30クエリ平均）\n\n| 候補数 | 平均 | 標準偏差 | 最小 | 最大 |\n|--------|------|----------|------|------|\n| 500 | 36.7% | 20.7% | 10.0% | 90.0% |\n| 1000 | 43.3% | 22.5% | 10.0% | 90.0% |\n| 2000 | 50.0% | 23.8% | 10.0% | 100.0% |\n| 5000 | 60.0% | 22.6% | 10.0% | 100.0% |\n| 10000 | 68.0% | 21.6% | 10.0% | 100.0% |\n| 20000 | 75.0% | 20.1% | 10.0% | 100.0% |\n\n## クエリタイプ別 Recall@10\n\n| 候補数 | JA短文 | JA曖昧 | EN短文 | EN曖昧 |\n|--------|--------|--------|--------|--------|\n| 500 | 47.0% | 25.0% | 52.0% | 24.0% |\n| 1000 | 55.0% | 30.0% | 62.0% | 28.0% |\n| 2000 | 61.0% | 35.0% | 70.0% | 38.0% |\n| 5000 | 66.0% | 56.0% | 76.0% | 40.0% |\n| 10000 | 72.0% | 63.0% | 82.0% | 56.0% |\n| 20000 | 79.0% | 68.0% | 88.0% | 68.0% |\n\n## 代表的なクエリの詳細\n\n| クエリ | タイプ | 候補2000件 | 候補10000件 | GT内ハミング距離(平均/最大) |\n|--------|--------|------------|-------------|---------------------------|\n| 「東京」 | JA短文 | 40.0% | 40.0% | 29.7 / 43 |\n| 「最近話題に...」 | JA曖昧 | 20.0% | 100.0% | 27.3 / 31 |\n| 「Tokyo」 | EN短文 | 90.0% | 90.0% | 23.1 / 34 |\n| 「I want to learn...」 | EN曖昧 | 40.0% | 70.0% | 27.3 / 37 |\n\n## 90% Recall達成状況\n\n| 候補数 | 90%以上のクエリ数 |\n|--------|-------------------|\n| 500 | 1/30 |\n| 1000 | 2/30 |\n| 2000 | 4/30 |\n| 5000 | 4/30 |\n| 10000 | 5/30 |\n| 20000 | 8/30 |\n\n## 考察\n\n1. **短文クエリは曖昧クエリより性能が良い**: 候補2000件で短文61-70%、曖昧35-38%\n2. **英語クエリは日本語クエリより性能が良い傾向**\n3. **「東京」クエリの問題**: GT内のハミング距離が大きく（平均29.7、最大43）、候補を増やしても改善しない\n4. **90% Recall達成は困難**: 候補20000件（95%削減）でも8/30クエリしか達成できない"
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n# 結論\n\n## 実験結果サマリー\n\n### Phase 1: ベクトル変化の分析\n- 同じテキストでも `passage:` と `query:` で異なるベクトルになる（類似度≈0.93）\n- 短文（title）の方が差が大きい（類似度≈0.91 vs body≈0.95）\n- 差分ベクトルの方向性はランダムに近い（共通の変換方向がない）\n\n### Phase 2: 検索手法の比較（候補2000件）\n| 手法 | LSH | GT | Recall |\n|------|-----|-----|--------|\n| Baseline_Query（従来） | query: | query: | 32.3% |\n| Baseline_Passage | passage: | passage: | 69.3% |\n| **Hybrid** | **passage:** | **query:** | **50.0%** |\n| Dual_Merge | query+passage | query: | 51.7% |\n\n### Phase 3: Hybrid手法の詳細\n- **候補2000件**: Recall 50%（従来比 +17.7pt）\n- **候補10000件**: Recall 68%\n- **候補20000件**: Recall 75%\n- 90% Recall達成は困難（候補20000件でも8/30クエリのみ）\n\n## Hybrid手法の評価\n\n**メリット:**\n1. LSHの候補選択精度が向上（passage:空間で検索）\n2. 最終的なコサイン類似度はquery:プレフィックスでe5の非対称検索の恩恵を受ける\n3. 従来手法（Baseline_Query）から +17.7pt の改善\n\n**デメリット:**\n1. 2つの埋め込み生成が必要（計算コスト増）\n2. 90% Recallに達するには候補数を大幅に増やす必要がある\n3. 一部のクエリ（「東京」など）では候補を増やしても改善しない\n\n## 推奨事項\n\n| 用途 | 推奨手法 | Recall | 候補削減率 |\n|------|----------|--------|-----------|\n| LSH精度優先 | Baseline_Passage | 69.3% | 99.5% |\n| e5精度維持 | **Hybrid** | 50-68% | 97.5-99.5% |\n| 高Recall必須 | FAISS/HNSW検討 | - | - |\n\n## 根本的な課題\n\n**SimHashとe5の非対称設計は本質的に相性が悪い。**\n\n- SimHashは「同じ空間にあるベクトル同士のハミング距離」で近傍を探す\n- e5の `query:` / `passage:` は意図的に異なる空間にマッピング\n- Hybrid手法でも、クエリによっては改善が見込めないケースがある\n\n長期的には以下の選択肢を検討：\n1. **passage:統一**: e5の非対称検索を諦め、LSH精度を優先\n2. **Hybrid手法**: 候補数を増やして対応\n3. **別のANN手法**: FAISS（IVF/HNSW）など、コサイン類似度に最適化された手法",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 接続を閉じる\n",
    "con.close()\n",
    "print('完了')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}