{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSH Cascade 検索システム - 最終レポート\n",
    "\n",
    "## 概要\n",
    "\n",
    "本プロジェクトでは、大規模ベクトル検索の高速化を目的として、**ITQ-LSH（Iterative Quantization Locality Sensitive Hashing）** と **Overlapセグメントフィルタリング** を組み合わせたカスケード検索システムを開発・評価しました。\n",
    "\n",
    "### 最終推奨パラメータ\n",
    "\n",
    "```\n",
    "埋め込みモデル: intfloat/multilingual-e5-base (768次元)\n",
    "ITQビット数: 128 bits\n",
    "Overlap設定: width=8, stride=4 (31セグメント)\n",
    "```\n",
    "\n",
    "### 性能サマリー（40万件Wikipedia）\n",
    "\n",
    "| 指標 | 値 |\n",
    "|------|----|\n",
    "| 候補削減率 | 87.5% |\n",
    "| Recall@10 (limit=1000) | 83.7% |\n",
    "| ベースラインRecall | 88.8% |\n",
    "| Recall低下 | -5.1pt |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 実験ノートブック一覧\n",
    "\n",
    "### Phase 1: モデル・ビット数の評価 (44-46)\n",
    "\n",
    "| No. | ノートブック | 目的 | 主要な発見 |\n",
    "|-----|-------------|------|------------|\n",
    "| 44 | [44_e5_base_bits_evaluation.ipynb](44_e5_base_bits_evaluation.ipynb) | E5-baseでのITQビット数評価 | 64 bitsで99.6% Recall@10達成、ストレージ効率重視なら64 bits推奨 |\n",
    "| 45 | [45_e5_base_fastembed_compatibility.ipynb](45_e5_base_fastembed_compatibility.ipynb) | FastEmbed互換性検証 | FastEmbedはE5-base未対応、モデル変更時はITQ再学習必要 |\n",
    "| 46 | [46_fastembed_practical_speed.ipynb](46_fastembed_practical_speed.ipynb) | FastEmbed実用速度評価 | カスタム登録でE5シリーズ使用可能、CPU推論は遅い |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2: Overlap評価・最適化 (47-49)\n",
    "\n",
    "| No. | ノートブック | 目的 | 主要な発見 |\n",
    "|-----|-------------|------|------------|\n",
    "| 47 | [47_fastembed_e5base_optimization.ipynb](47_fastembed_e5base_optimization.ipynb) | FastEmbed最適化 + Overlap小規模評価 | 5000件ではOverlapフィルタ効果なし（全件ヒット） |\n",
    "| 48 | [48_wikipedia_400k_embedding.ipynb](48_wikipedia_400k_embedding.ipynb) | Wikipedia 40万件データセット構築 | E5-base埋め込み + ITQ(64/96/128 bits)学習・永続化 |\n",
    "| 49 | [49_overlap_evaluation_400k.ipynb](49_overlap_evaluation_400k.ipynb) | 40万件規模Overlap評価 | **128b + Overlap(8,4)** が最適バランス |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 3: 汎化性能評価 (data/notebooks/)\n",
    "\n",
    "ResOUデータ（研究報告約2,800件）を用いた汎化性能評価: (データは別途提供)\n",
    "\n",
    "| No. | ノートブック | 目的 | 主要な発見 |\n",
    "|-----|-------------|------|------------|\n",
    "| 01 | data/notebooks/01_prepare_resou_data.ipynb | ResOUデータ準備 | E5-largeで埋め込み生成、DuckDB保存 |\n",
    "| 02 | data/notebooks/02_itq_generalization_test.ipynb | ITQ汎化テスト | Wikipedia学習ITQの汎化性能検証 |\n",
    "| 03 | data/notebooks/03_cascade_search_validation.ipynb | カスケード検索検証 | 3段階検索の基本動作確認 |\n",
    "| 04 | data/notebooks/04_cascade_optimization.ipynb | E5-large最適化 | 外部クエリStep1 R@10=62%（低い） |\n",
    "| 05 | data/notebooks/05_cascade_e5base_128bits.ipynb | **E5-base最終評価** | 外部クエリStep1 R@10=82%（+20pt改善） |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 主要な技術的発見\n",
    "\n",
    "### 1. ITQビット数の選択\n",
    "\n",
    "| ビット数 | Recall@10 (40万件) | ストレージ | 推奨用途 |\n",
    "|----------|-------------------|-----------|----------|\n",
    "| 64 bits | 73.8% | 8 bytes | ストレージ重視 |\n",
    "| 96 bits | 82.5% | 12 bytes | バランス |\n",
    "| **128 bits** | **88.8%** | 16 bytes | **精度重視（推奨）** |\n",
    "\n",
    "**結論**: 128 bitsが最も高いRecallを達成。ストレージ増加（+4-8 bytes/doc）は許容範囲。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Overlapセグメント設定\n",
    "\n",
    "128 bitsでの各設定の比較:\n",
    "\n",
    "| 設定 | セグメント数 | 削減率 | Recall@1000 | 評価 |\n",
    "|------|-------------|--------|-------------|------|\n",
    "| **(8, 4)** | **31** | **87.5%** | **83.7%** | **推奨** |\n",
    "| (8, 2) | 61 | 82.2% | 86.0% | 高Recall |\n",
    "| (12, 6) | 15 | 99.2% | 44.6% | 非推奨 |\n",
    "| (16, 8) | 11 | 99.9% | 21.1% | 非推奨 |\n",
    "\n",
    "**結論**: \n",
    "- **Overlap(8, 4)** が削減率とRecallの最適バランス\n",
    "- セグメント幅12bit以上は削減率が高すぎてRecall大幅低下"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. E5-base vs E5-large\n",
    "\n",
    "ResOUデータでの汎化性能比較:\n",
    "\n",
    "| 指標 | E5-large (04) | E5-base (05) | 改善 |\n",
    "|------|--------------|--------------|------|\n",
    "| 外部クエリ Step1 R@10 | 62.0% | **82.0%** | **+20.0pt** |\n",
    "| 外部クエリ 最終R@10 | 62.0% | **79.5%** | **+17.5pt** |\n",
    "| 埋め込み次元 | 1024 | 768 | -25% |\n",
    "| ITQストレージ | 同じ | 同じ | - |\n",
    "\n",
    "**結論**: E5-baseの方が外部クエリに対する汎化性能が高い。次元数も小さく効率的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. データ規模の影響\n",
    "\n",
    "| データ規模 | Overlapフィルタ効果 | 推奨手法 |\n",
    "|-----------|---------------------|----------|\n",
    "| ~5,000件 | 効果なし（全件ヒット） | 2段階検索 |\n",
    "| ~3,000件 | 一部効果あり | 2段階検索 |\n",
    "| **40万件以上** | **効果大（87%削減）** | **3段階カスケード** |\n",
    "\n",
    "**結論**: Overlapフィルタは大規模データ（数十万件以上）で真価を発揮。小規模データでは2段階検索（全件ハミング距離→コサイン）が実用的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 永続化ファイル一覧\n",
    "\n",
    "### ITQモデル・ハッシュ（data/）\n",
    "\n",
    "```\n",
    "data/\n",
    "├── itq_e5_base_64bits.pkl          # ITQモデル (64 bits)\n",
    "├── itq_e5_base_96bits.pkl          # ITQモデル (96 bits)\n",
    "├── itq_e5_base_128bits.pkl         # ITQモデル (128 bits) ★推奨\n",
    "├── wikipedia_400k_e5_base.duckdb   # 埋め込み + HNSWインデックス (3.4GB)\n",
    "├── wikipedia_400k_e5_base_embeddings.npy  # 埋め込みベクトル (1.2GB)\n",
    "├── wikipedia_400k_e5_base_meta.npz        # メタデータ\n",
    "├── wikipedia_400k_e5_base_hashes_64bits.npy   # ハッシュ (25MB)\n",
    "├── wikipedia_400k_e5_base_hashes_96bits.npy   # ハッシュ (37MB)\n",
    "└── wikipedia_400k_e5_base_hashes_128bits.npy  # ハッシュ (49MB) ★推奨\n",
    "```\n",
    "\n",
    "### 使用方法\n",
    "\n",
    "```python\n",
    "from src.itq_lsh import ITQLSH\n",
    "\n",
    "# ITQモデルロード\n",
    "itq = ITQLSH.load('data/itq_e5_base_128bits.pkl')\n",
    "\n",
    "# 埋め込みからハッシュ生成\n",
    "hashes = itq.transform(embeddings)  # (n_samples, 128)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3段階カスケード検索アーキテクチャ\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                      Query Embedding                        │\n",
    "│                    (E5-base, 768次元)                       │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "                              │\n",
    "                              ▼\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│  Step 1: Overlap Segment Filter (OR条件)                    │\n",
    "│  - 128 bits → 31セグメント (8bit幅, 4bitストライド)         │\n",
    "│  - いずれかのセグメントが一致する文書を候補に               │\n",
    "│  - 削減率: 87.5% (399k → 50k)                               │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "                              │\n",
    "                              ▼\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│  Step 2: Hamming Distance Sort                              │\n",
    "│  - 候補文書のハミング距離を計算                             │\n",
    "│  - 上位N件に絞り込み (例: 1000件)                           │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "                              │\n",
    "                              ▼\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│  Step 3: Cosine Similarity Ranking                          │\n",
    "│  - 絞り込んだ候補でコサイン類似度を計算                     │\n",
    "│  - Top-K (例: 10件) を返却                                  │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 今後の展望\n",
    "\n",
    "### 1. 性能改善\n",
    "- Multi-probe LSH の導入（セグメント近傍も候補に含める）\n",
    "- ファジーセグメントマッチング（部分一致許容）\n",
    "- GPU並列化によるハミング距離計算の高速化\n",
    "\n",
    "### 2. スケーラビリティ\n",
    "- 分散インデックス対応\n",
    "- インクリメンタルなITQ更新\n",
    "- オンラインでのセグメントインデックス更新\n",
    "\n",
    "### 3. 実運用\n",
    "- FastAPI/gRPC サービス化\n",
    "- キャッシュ戦略の最適化\n",
    "- モニタリング・メトリクス収集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 参考文献\n",
    "\n",
    "1. **ITQ (Iterative Quantization)**\n",
    "   - Gong et al., \"Iterative Quantization: A Procrustean Approach to Learning Binary Codes\", CVPR 2011\n",
    "\n",
    "2. **E5 Embedding Models**\n",
    "   - Wang et al., \"Text Embeddings by Weakly-Supervised Contrastive Pre-training\", 2022\n",
    "   - [intfloat/multilingual-e5-base](https://huggingface.co/intfloat/multilingual-e5-base)\n",
    "\n",
    "3. **LSH (Locality Sensitive Hashing)**\n",
    "   - Indyk & Motwani, \"Approximate Nearest Neighbors: Towards Removing the Curse of Dimensionality\", STOC 1998"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
