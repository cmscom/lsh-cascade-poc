{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 35. 小規模データセットでのセグメント幅最適化\n",
    "\n",
    "## 背景\n",
    "\n",
    "実験34で判明した通り、データ規模が小さい場合（2,789件）ではOverlap(8,4)方式で\n",
    "外部クエリの再現率が低下する問題があった。これはバケットがスパースになるため。\n",
    "\n",
    "## 仮説\n",
    "\n",
    "セグメント幅を大きくする（16bit, 32bitなど）ことで、バケット数を減らし、\n",
    "各バケットに含まれるドキュメント数を増やせば、小規模データでも\n",
    "Overlap方式が有効に機能するのではないか。\n",
    "\n",
    "## 検証するパラメータ\n",
    "\n",
    "| 設定 | セグメント幅 | ストライド | セグメント数 | バケット数(理論) |\n",
    "|------|------------|-----------|-------------|----------------|\n",
    "| (8, 4) | 8bit | 4bit | 31 | 256 |\n",
    "| (16, 8) | 16bit | 8bit | 15 | 65,536 |\n",
    "| (32, 8) | 32bit | 8bit | 13 | 4.3B |\n",
    "| (32, 16) | 32bit | 16bit | 7 | 4.3B |\n",
    "\n",
    "## 実験方法\n",
    "\n",
    "1. 40万件から5000件をランダムサンプリング（5回）\n",
    "2. 各サンプルで異なるOverlap設定を評価\n",
    "3. 内部/外部クエリ両方で評価\n",
    "4. 結果を平均化して比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from src.itq_lsh import ITQLSH, hamming_distance_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パス設定\n",
    "DB_PATH = '../data/experiment_400k.duckdb'\n",
    "ITQ_MODEL_PATH = '../data/itq_model.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. データ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データ読み込み中...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8b27fe384634f3ab91adce484b9b494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "読み込み完了: 400,000件\n",
      "埋め込み shape: (400000, 1024)\n"
     ]
    }
   ],
   "source": [
    "# 全データ読み込み\n",
    "print('データ読み込み中...')\n",
    "conn = duckdb.connect(DB_PATH, read_only=True)\n",
    "\n",
    "result = conn.execute(\"\"\"\n",
    "    SELECT id, embedding\n",
    "    FROM documents\n",
    "    ORDER BY id\n",
    "\"\"\").fetchall()\n",
    "conn.close()\n",
    "\n",
    "all_doc_ids = np.array([r[0] for r in result])\n",
    "all_embeddings = np.array([r[1] for r in result], dtype=np.float32)\n",
    "\n",
    "n_all_docs = len(all_doc_ids)\n",
    "print(f'読み込み完了: {n_all_docs:,}件')\n",
    "print(f'埋め込み shape: {all_embeddings.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITQモデル読み込み中...\n",
      "ITQハッシュ計算中...\n",
      "ハッシュ shape: (400000, 128)\n"
     ]
    }
   ],
   "source": [
    "# ITQモデル読み込みとハッシュ計算\n",
    "print('ITQモデル読み込み中...')\n",
    "itq = ITQLSH.load(ITQ_MODEL_PATH)\n",
    "\n",
    "print('ITQハッシュ計算中...')\n",
    "all_hashes = itq.transform(all_embeddings)\n",
    "print(f'ハッシュ shape: {all_hashes.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E5モデル読み込み中...\n",
      "完了\n"
     ]
    }
   ],
   "source": [
    "# E5モデル読み込み\n",
    "print('E5モデル読み込み中...')\n",
    "model = SentenceTransformer('intfloat/multilingual-e5-large')\n",
    "print('完了')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. 共通ユーティリティ関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ground_truth(query_embedding, all_embeddings, top_k=10):\n",
    "    \"\"\"ブルートフォースでGround Truthを計算\"\"\"\n",
    "    query_norm = norm(query_embedding)\n",
    "    all_norms = norm(all_embeddings, axis=1)\n",
    "    cosines = (all_embeddings @ query_embedding) / (all_norms * query_norm + 1e-10)\n",
    "    top_indices = np.argsort(cosines)[-top_k:][::-1]\n",
    "    return top_indices\n",
    "\n",
    "\n",
    "def evaluate_recall(predicted, ground_truth, k=10):\n",
    "    \"\"\"Recall@k を計算\"\"\"\n",
    "    gt_set = set(ground_truth[:k])\n",
    "    pred_set = set(predicted[:k]) if len(predicted) >= k else set(predicted)\n",
    "    return len(gt_set & pred_set) / k\n",
    "\n",
    "\n",
    "def bits_to_int(bits_array):\n",
    "    \"\"\"バイナリ配列を整数に変換\"\"\"\n",
    "    if bits_array.ndim == 1:\n",
    "        bits_array = bits_array.reshape(1, -1)\n",
    "    n_bits = bits_array.shape[1]\n",
    "    powers = 2 ** np.arange(n_bits - 1, -1, -1)\n",
    "    return np.sum(bits_array * powers, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_to_overlap_segments(hash_array, segment_width=8, stride=4):\n",
    "    \"\"\"\n",
    "    オーバーラップセグメントを生成\n",
    "    \"\"\"\n",
    "    if hash_array.ndim == 1:\n",
    "        hash_array = hash_array.reshape(1, -1)\n",
    "    n_docs, n_bits = hash_array.shape\n",
    "    n_segments = (n_bits - segment_width) // stride + 1\n",
    "    \n",
    "    segments = []\n",
    "    for i in range(n_segments):\n",
    "        start = i * stride\n",
    "        end = start + segment_width\n",
    "        segment_bits = hash_array[:, start:end]\n",
    "        \n",
    "        # ビットを整数に変換\n",
    "        powers = 2 ** np.arange(segment_width - 1, -1, -1)\n",
    "        segment_int = np.sum(segment_bits * powers, axis=1)\n",
    "        segments.append(segment_int)\n",
    "    \n",
    "    return np.column_stack(segments), n_segments\n",
    "\n",
    "\n",
    "def build_overlap_index(segments):\n",
    "    \"\"\"\n",
    "    オーバーラップセグメントのインデックスを構築\n",
    "    \"\"\"\n",
    "    n_docs, n_segments = segments.shape\n",
    "    index = {i: defaultdict(list) for i in range(n_segments)}\n",
    "    \n",
    "    for doc_idx in range(n_docs):\n",
    "        for seg_idx in range(n_segments):\n",
    "            seg_value = int(segments[doc_idx, seg_idx])\n",
    "            index[seg_idx][seg_value].append(doc_idx)\n",
    "    \n",
    "    return index\n",
    "\n",
    "\n",
    "def overlap_segment_search(query_segments, segment_index, n_segments):\n",
    "    \"\"\"\n",
    "    Overlapセグメント一致による候補取得（OR条件）\n",
    "    \"\"\"\n",
    "    candidates = set()\n",
    "    \n",
    "    for seg_idx in range(n_segments):\n",
    "        seg_value = int(query_segments[seg_idx])\n",
    "        if seg_value in segment_index[seg_idx]:\n",
    "            candidates.update(segment_index[seg_idx][seg_value])\n",
    "    \n",
    "    return np.array(list(candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cascade_search(query_embedding, query_hash, query_segments,\n",
    "                   segment_index, n_segments, all_hashes, all_embeddings,\n",
    "                   step1_limit=2000, step2_limit=500, top_k=10):\n",
    "    \"\"\"\n",
    "    3段階カスケード検索\n",
    "    \"\"\"\n",
    "    # Step 1: Overlapセグメント一致\n",
    "    step1_candidates = overlap_segment_search(query_segments, segment_index, n_segments)\n",
    "    step1_raw_count = len(step1_candidates)\n",
    "    \n",
    "    # Step 1でstep1_limitを超えた場合はハミング距離で絞る\n",
    "    if len(step1_candidates) > step1_limit:\n",
    "        dists = hamming_distance_batch(query_hash, all_hashes[step1_candidates])\n",
    "        top_idx = np.argsort(dists)[:step1_limit]\n",
    "        step1_candidates = step1_candidates[top_idx]\n",
    "    \n",
    "    # Step 2: ハミング距離ソート\n",
    "    if len(step1_candidates) > step2_limit:\n",
    "        dists = hamming_distance_batch(query_hash, all_hashes[step1_candidates])\n",
    "        top_idx = np.argsort(dists)[:step2_limit]\n",
    "        step2_candidates = step1_candidates[top_idx]\n",
    "    else:\n",
    "        step2_candidates = step1_candidates\n",
    "    \n",
    "    # Step 3: コサイン類似度\n",
    "    if len(step2_candidates) > 0:\n",
    "        candidate_embs = all_embeddings[step2_candidates]\n",
    "        query_norm = norm(query_embedding)\n",
    "        candidate_norms = norm(candidate_embs, axis=1)\n",
    "        cosines = (candidate_embs @ query_embedding) / (candidate_norms * query_norm + 1e-10)\n",
    "        top_k_idx = np.argsort(cosines)[-top_k:][::-1]\n",
    "        top_k_indices = step2_candidates[top_k_idx]\n",
    "    else:\n",
    "        top_k_indices = np.array([])\n",
    "    \n",
    "    return {\n",
    "        'top_k_indices': top_k_indices,\n",
    "        'step1_raw_count': step1_raw_count,\n",
    "        'step1_count': len(step1_candidates),\n",
    "        'step2_count': len(step2_candidates),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. 外部クエリの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "外部クエリ数: 20\n"
     ]
    }
   ],
   "source": [
    "# 外部クエリ（幅広いトピックのキーワード）\n",
    "external_queries = [\n",
    "    # 日本語\n",
    "    '人工知能',\n",
    "    '機械学習',\n",
    "    '医療',\n",
    "    '環境問題',\n",
    "    '再生医療',\n",
    "    'ロボット',\n",
    "    '量子コンピュータ',\n",
    "    '脳科学',\n",
    "    'がん治療',\n",
    "    '気候変動',\n",
    "    # 英語\n",
    "    'artificial intelligence',\n",
    "    'machine learning',\n",
    "    'medical research',\n",
    "    'environmental science',\n",
    "    'regenerative medicine',\n",
    "    'robotics',\n",
    "    'quantum computing',\n",
    "    'neuroscience',\n",
    "    'cancer treatment',\n",
    "    'climate change',\n",
    "]\n",
    "\n",
    "print(f'外部クエリ数: {len(external_queries)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "外部クエリ埋め込み生成中...\n",
      "完了: (20, 1024)\n"
     ]
    }
   ],
   "source": [
    "# 外部クエリの埋め込みとITQハッシュ\n",
    "print('外部クエリ埋め込み生成中...')\n",
    "external_query_embs = model.encode(\n",
    "    [f'passage: {q}' for q in external_queries],\n",
    "    normalize_embeddings=False\n",
    ").astype(np.float32)\n",
    "\n",
    "external_query_hashes = itq.transform(external_query_embs)\n",
    "print(f'完了: {external_query_embs.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. サンプリングと評価関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_dataset(n_samples, seed):\n",
    "    \"\"\"\n",
    "    40万件から指定件数をサンプリング\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    sample_indices = rng.choice(n_all_docs, n_samples, replace=False)\n",
    "    sample_indices = np.sort(sample_indices)\n",
    "    \n",
    "    return {\n",
    "        'indices': sample_indices,\n",
    "        'embeddings': all_embeddings[sample_indices],\n",
    "        'hashes': all_hashes[sample_indices],\n",
    "        'n_docs': n_samples,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_overlap_config(sample_data, segment_width, stride,\n",
    "                            n_internal_queries=50,\n",
    "                            step1_limit=1000, step2_limit=200):\n",
    "    \"\"\"\n",
    "    特定のOverlap設定を評価\n",
    "    \"\"\"\n",
    "    embeddings = sample_data['embeddings']\n",
    "    hashes = sample_data['hashes']\n",
    "    n_docs = sample_data['n_docs']\n",
    "    \n",
    "    # Overlapインデックス構築\n",
    "    segments, n_segments = hash_to_overlap_segments(hashes, segment_width, stride)\n",
    "    segment_index = build_overlap_index(segments)\n",
    "    \n",
    "    # バケット統計\n",
    "    bucket_sizes = [len(docs) for seg_idx in segment_index for docs in segment_index[seg_idx].values()]\n",
    "    avg_bucket_size = np.mean(bucket_sizes) if bucket_sizes else 0\n",
    "    n_buckets = sum(len(segment_index[seg_idx]) for seg_idx in segment_index)\n",
    "    \n",
    "    # 内部クエリ準備\n",
    "    rng = np.random.default_rng(42)\n",
    "    internal_query_indices = rng.choice(n_docs, min(n_internal_queries, n_docs), replace=False)\n",
    "    \n",
    "    # 内部クエリのGround Truth\n",
    "    internal_gts = {}\n",
    "    for qi in internal_query_indices:\n",
    "        internal_gts[qi] = compute_ground_truth(embeddings[qi], embeddings, top_k=10)\n",
    "    \n",
    "    # 内部クエリ評価\n",
    "    internal_recalls = []\n",
    "    internal_step1_counts = []\n",
    "    \n",
    "    for qi in internal_query_indices:\n",
    "        q_segments = segments[qi]\n",
    "        result = cascade_search(\n",
    "            embeddings[qi], hashes[qi], q_segments,\n",
    "            segment_index, n_segments, hashes, embeddings,\n",
    "            step1_limit=step1_limit, step2_limit=step2_limit, top_k=10\n",
    "        )\n",
    "        recall = evaluate_recall(result['top_k_indices'], internal_gts[qi], k=10)\n",
    "        internal_recalls.append(recall)\n",
    "        internal_step1_counts.append(result['step1_raw_count'])\n",
    "    \n",
    "    # 外部クエリ評価\n",
    "    external_recalls = []\n",
    "    external_step1_counts = []\n",
    "    \n",
    "    for i, (q_emb, q_hash) in enumerate(zip(external_query_embs, external_query_hashes)):\n",
    "        # Ground Truth（このサンプルデータに対して）\n",
    "        gt = compute_ground_truth(q_emb, embeddings, top_k=10)\n",
    "        \n",
    "        # セグメント生成\n",
    "        q_segments, _ = hash_to_overlap_segments(q_hash.reshape(1, -1), segment_width, stride)\n",
    "        \n",
    "        result = cascade_search(\n",
    "            q_emb, q_hash, q_segments[0],\n",
    "            segment_index, n_segments, hashes, embeddings,\n",
    "            step1_limit=step1_limit, step2_limit=step2_limit, top_k=10\n",
    "        )\n",
    "        recall = evaluate_recall(result['top_k_indices'], gt, k=10)\n",
    "        external_recalls.append(recall)\n",
    "        external_step1_counts.append(result['step1_raw_count'])\n",
    "    \n",
    "    return {\n",
    "        'segment_width': segment_width,\n",
    "        'stride': stride,\n",
    "        'n_segments': n_segments,\n",
    "        'n_buckets': n_buckets,\n",
    "        'avg_bucket_size': avg_bucket_size,\n",
    "        'internal_recall@10': np.mean(internal_recalls),\n",
    "        'internal_avg_step1': np.mean(internal_step1_counts),\n",
    "        'external_recall@10': np.mean(external_recalls),\n",
    "        'external_avg_step1': np.mean(external_step1_counts),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 実験: 5000件データセット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "サンプルサイズ: [5000]\n",
      "サンプリング回数: 5\n",
      "Overlap設定数: 6\n"
     ]
    }
   ],
   "source": [
    "# 実験設定\n",
    "SAMPLE_SIZES = [5000]  # サンプルサイズ\n",
    "N_SAMPLES = 5  # サンプリング回数\n",
    "\n",
    "# Overlap設定\n",
    "OVERLAP_CONFIGS = [\n",
    "    (8, 4),    # 現行設定\n",
    "    (8, 2),    # より多いオーバーラップ\n",
    "    (16, 8),   # セグメント幅を2倍\n",
    "    (16, 4),   # 幅16、ストライド4\n",
    "    (32, 16),  # セグメント幅を4倍\n",
    "    (32, 8),   # 幅32、ストライド8\n",
    "]\n",
    "\n",
    "print(f'サンプルサイズ: {SAMPLE_SIZES}')\n",
    "print(f'サンプリング回数: {N_SAMPLES}')\n",
    "print(f'Overlap設定数: {len(OVERLAP_CONFIGS)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "5000件データセットでの実験\n",
      "================================================================================\n",
      "\n",
      "サンプル 1/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overlap設定: 100%|██████████| 6/6 [00:04<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "サンプル 2/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overlap設定: 100%|██████████| 6/6 [00:01<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "サンプル 3/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overlap設定: 100%|██████████| 6/6 [00:01<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "サンプル 4/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overlap設定: 100%|██████████| 6/6 [00:01<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "サンプル 5/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overlap設定: 100%|██████████| 6/6 [00:02<00:00,  2.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "完了\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 5000件での実験\n",
    "print('=' * 80)\n",
    "print('5000件データセットでの実験')\n",
    "print('=' * 80)\n",
    "\n",
    "results_5000 = []\n",
    "\n",
    "for sample_idx in range(N_SAMPLES):\n",
    "    print(f'\\nサンプル {sample_idx + 1}/{N_SAMPLES}...')\n",
    "    \n",
    "    # サンプルデータ作成\n",
    "    sample_data = create_sample_dataset(5000, seed=42 + sample_idx)\n",
    "    \n",
    "    for seg_width, stride in tqdm(OVERLAP_CONFIGS, desc='Overlap設定'):\n",
    "        result = evaluate_overlap_config(\n",
    "            sample_data, seg_width, stride,\n",
    "            n_internal_queries=50,\n",
    "            step1_limit=1000, step2_limit=200\n",
    "        )\n",
    "        result['sample_idx'] = sample_idx\n",
    "        result['n_docs'] = 5000\n",
    "        results_5000.append(result)\n",
    "\n",
    "df_5000 = pd.DataFrame(results_5000)\n",
    "print('\\n完了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果を集約\n",
    "df_5000_agg = df_5000.groupby(['segment_width', 'stride']).agg({\n",
    "    'n_segments': 'first',\n",
    "    'n_buckets': 'mean',\n",
    "    'avg_bucket_size': 'mean',\n",
    "    'internal_recall@10': ['mean', 'std'],\n",
    "    'internal_avg_step1': 'mean',\n",
    "    'external_recall@10': ['mean', 'std'],\n",
    "    'external_avg_step1': 'mean',\n",
    "}).round(3)\n",
    "\n",
    "# カラム名をフラット化\n",
    "df_5000_agg.columns = [\n",
    "    'n_segments', 'n_buckets', 'avg_bucket_size',\n",
    "    'internal_r10_mean', 'internal_r10_std',\n",
    "    'internal_step1',\n",
    "    'external_r10_mean', 'external_r10_std',\n",
    "    'external_step1'\n",
    "]\n",
    "df_5000_agg = df_5000_agg.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================================================================\n",
      "5000件データセット: Overlap設定比較\n",
      "========================================================================================================================\n",
      "\n",
      "Width | Stride | Segs |  Buckets |  BktSize |   Int R@10 |  Int Step1 |   Ext R@10 |  Ext Step1\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "  8.0 |    2.0 | 61.0 |    15559 |     19.6 |  89.8%±1.6 |       1096 |  94.3%±1.8★ |       1207\n",
      "  8.0 |    4.0 | 31.0 |     7906 |     19.6 |  86.5%±1.3 |        812 |  90.3%±2.8★ |        899\n",
      " 16.0 |    4.0 | 29.0 |   105122 |      1.4 |  36.1%±4.8 |         43 |  30.2%±4.1 |         38\n",
      " 16.0 |    8.0 | 15.0 |    54150 |      1.4 |  32.2%±4.2 |         31 |  22.2%±5.2 |         26\n",
      " 32.0 |    8.0 | 13.0 |    63028 |      1.0 |  13.6%±1.6 |          3 |   1.2%±0.8 |          0\n",
      " 32.0 |   16.0 |  7.0 |    33927 |      1.0 |  12.4%±1.0 |          3 |   0.6%±0.7 |          0\n"
     ]
    }
   ],
   "source": [
    "# 5000件結果表示\n",
    "print('\\n' + '=' * 120)\n",
    "print('5000件データセット: Overlap設定比較')\n",
    "print('=' * 120)\n",
    "\n",
    "print(f'\\n{\"Width\":>5} | {\"Stride\":>6} | {\"Segs\":>4} | {\"Buckets\":>8} | {\"BktSize\":>8} | '\n",
    "      f'{\"Int R@10\":>10} | {\"Int Step1\":>10} | {\"Ext R@10\":>10} | {\"Ext Step1\":>10}')\n",
    "print('-' * 120)\n",
    "\n",
    "for _, row in df_5000_agg.iterrows():\n",
    "    int_r10 = f'{row[\"internal_r10_mean\"]*100:.1f}%±{row[\"internal_r10_std\"]*100:.1f}'\n",
    "    ext_r10 = f'{row[\"external_r10_mean\"]*100:.1f}%±{row[\"external_r10_std\"]*100:.1f}'\n",
    "    ext_mark = '★' if row['external_r10_mean'] >= 0.80 else ''\n",
    "    \n",
    "    print(f'{row[\"segment_width\"]:>5} | {row[\"stride\"]:>6} | {row[\"n_segments\"]:>4} | '\n",
    "          f'{row[\"n_buckets\"]:>8.0f} | {row[\"avg_bucket_size\"]:>8.1f} | '\n",
    "          f'{int_r10:>10} | {row[\"internal_step1\"]:>10.0f} | '\n",
    "          f'{ext_r10:>10}{ext_mark} | {row[\"external_step1\"]:>10.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. 実験: 異なるデータサイズでの比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "異なるデータサイズでの比較\n",
      "================================================================================\n",
      "\n",
      "サイズ: 2,000件\n",
      "\n",
      "サイズ: 5,000件\n",
      "\n",
      "サイズ: 10,000件\n",
      "\n",
      "サイズ: 20,000件\n",
      "\n",
      "サイズ: 50,000件\n",
      "\n",
      "完了\n"
     ]
    }
   ],
   "source": [
    "# 複数サイズでの実験\n",
    "print('=' * 80)\n",
    "print('異なるデータサイズでの比較')\n",
    "print('=' * 80)\n",
    "\n",
    "SIZE_CONFIGS = [2000, 5000, 10000, 20000, 50000]\n",
    "N_SAMPLES_QUICK = 3  # 各サイズ3回サンプリング\n",
    "\n",
    "results_sizes = []\n",
    "\n",
    "for size in SIZE_CONFIGS:\n",
    "    print(f'\\nサイズ: {size:,}件')\n",
    "    \n",
    "    for sample_idx in range(N_SAMPLES_QUICK):\n",
    "        sample_data = create_sample_dataset(size, seed=100 + sample_idx)\n",
    "        \n",
    "        for seg_width, stride in OVERLAP_CONFIGS:\n",
    "            result = evaluate_overlap_config(\n",
    "                sample_data, seg_width, stride,\n",
    "                n_internal_queries=30,\n",
    "                step1_limit=min(1000, size // 5),\n",
    "                step2_limit=min(200, size // 25)\n",
    "            )\n",
    "            result['sample_idx'] = sample_idx\n",
    "            result['n_docs'] = size\n",
    "            results_sizes.append(result)\n",
    "\n",
    "df_sizes = pd.DataFrame(results_sizes)\n",
    "print('\\n完了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# サイズ別に集約\n",
    "df_sizes_agg = df_sizes.groupby(['n_docs', 'segment_width', 'stride']).agg({\n",
    "    'n_segments': 'first',\n",
    "    'avg_bucket_size': 'mean',\n",
    "    'internal_recall@10': 'mean',\n",
    "    'internal_avg_step1': 'mean',\n",
    "    'external_recall@10': 'mean',\n",
    "    'external_avg_step1': 'mean',\n",
    "}).round(3).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================================================================================================\n",
      "データサイズ別: Overlap設定比較\n",
      "============================================================================================================================================\n",
      "\n",
      "■ 2,000件\n",
      "Width | Stride | Segs |  BktSize | Int R@10 |  Int Step1 | Ext R@10 |  Ext Step1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "  8.0 |    2.0 | 61.0 |      8.1 |    86.0% |        431 |    90.2%★ |        484\n",
      "  8.0 |    4.0 | 31.0 |      8.1 |    82.4% |        319 |    87.2%★ |        361\n",
      " 16.0 |    4.0 | 29.0 |      1.2 |    31.1% |         17 |    24.5% |         14\n",
      " 16.0 |    8.0 | 15.0 |      1.2 |    26.9% |         12 |    18.7% |         10\n",
      " 32.0 |    8.0 | 13.0 |      1.0 |    12.4% |          2 |     0.7% |          0\n",
      " 32.0 |   16.0 |  7.0 |      1.0 |    12.1% |          2 |     0.3% |          0\n",
      "\n",
      "■ 5,000件\n",
      "Width | Stride | Segs |  BktSize | Int R@10 |  Int Step1 | Ext R@10 |  Ext Step1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "  8.0 |    2.0 | 61.0 |     19.6 |    90.1% |       1065 |    93.8%★ |       1216\n",
      "  8.0 |    4.0 | 31.0 |     19.6 |    85.6% |        782 |    89.8%★ |        903\n",
      " 16.0 |    4.0 | 29.0 |      1.4 |    34.3% |         39 |    29.3% |         38\n",
      " 16.0 |    8.0 | 15.0 |      1.4 |    29.6% |         29 |    23.0% |         27\n",
      " 32.0 |    8.0 | 13.0 |      1.0 |    12.2% |          2 |     1.3% |          0\n",
      " 32.0 |   16.0 |  7.0 |      1.0 |    11.6% |          2 |     0.8% |          0\n",
      "\n",
      "■ 10,000件\n",
      "Width | Stride | Segs |  BktSize | Int R@10 |  Int Step1 | Ext R@10 |  Ext Step1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "  8.0 |    2.0 | 61.0 |     39.1 |    88.2% |       2172 |    91.7%★ |       2408\n",
      "  8.0 |    4.0 | 31.0 |     39.1 |    86.2% |       1586 |    89.0%★ |       1797\n",
      " 16.0 |    4.0 | 29.0 |      1.6 |    35.8% |         69 |    32.0% |         76\n",
      " 16.0 |    8.0 | 15.0 |      1.6 |    28.8% |         50 |    25.2% |         53\n",
      " 32.0 |    8.0 | 13.0 |      1.0 |    12.3% |          4 |     0.8% |          0\n",
      " 32.0 |   16.0 |  7.0 |      1.0 |    11.9% |          4 |     0.8% |          0\n",
      "\n",
      "■ 20,000件\n",
      "Width | Stride | Segs |  BktSize | Int R@10 |  Int Step1 | Ext R@10 |  Ext Step1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "  8.0 |    2.0 | 61.0 |     78.1 |    84.2% |       4313 |    89.8%★ |       4856\n",
      "  8.0 |    4.0 | 31.0 |     78.1 |    82.8% |       3179 |    87.2%★ |       3614\n",
      " 16.0 |    4.0 | 29.0 |      1.9 |    37.3% |        151 |    38.0% |        153\n",
      " 16.0 |    8.0 | 15.0 |      1.9 |    32.3% |        106 |    30.5% |        107\n",
      " 32.0 |    8.0 | 13.0 |      1.1 |    11.6% |          4 |     2.3% |          1\n",
      " 32.0 |   16.0 |  7.0 |      1.1 |    11.4% |          3 |     1.5% |          1\n",
      "\n",
      "■ 50,000件\n",
      "Width | Stride | Segs |  BktSize | Int R@10 |  Int Step1 | Ext R@10 |  Ext Step1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "  8.0 |    2.0 | 61.0 |    195.3 |    76.4% |      10702 |    85.3%★ |      12079\n",
      "  8.0 |    4.0 | 31.0 |    195.3 |    76.4% |       7892 |    84.3%★ |       9006\n",
      " 16.0 |    4.0 | 29.0 |      2.7 |    39.0% |        406 |    41.0% |        385\n",
      " 16.0 |    8.0 | 15.0 |      2.7 |    33.4% |        297 |    35.3% |        269\n",
      " 32.0 |    8.0 | 13.0 |      1.1 |    15.0% |         22 |     4.2% |          2\n",
      " 32.0 |   16.0 |  7.0 |      1.1 |    14.7% |         17 |     3.2% |          1\n"
     ]
    }
   ],
   "source": [
    "# サイズ別結果表示\n",
    "print('\\n' + '=' * 140)\n",
    "print('データサイズ別: Overlap設定比較')\n",
    "print('=' * 140)\n",
    "\n",
    "for size in SIZE_CONFIGS:\n",
    "    print(f'\\n■ {size:,}件')\n",
    "    print(f'{\"Width\":>5} | {\"Stride\":>6} | {\"Segs\":>4} | {\"BktSize\":>8} | '\n",
    "          f'{\"Int R@10\":>8} | {\"Int Step1\":>10} | {\"Ext R@10\":>8} | {\"Ext Step1\":>10}')\n",
    "    print('-' * 100)\n",
    "    \n",
    "    df_size = df_sizes_agg[df_sizes_agg['n_docs'] == size]\n",
    "    \n",
    "    for _, row in df_size.iterrows():\n",
    "        ext_mark = '★' if row['external_recall@10'] >= 0.80 else ''\n",
    "        print(f'{row[\"segment_width\"]:>5} | {row[\"stride\"]:>6} | {row[\"n_segments\"]:>4} | '\n",
    "              f'{row[\"avg_bucket_size\"]:>8.1f} | '\n",
    "              f'{row[\"internal_recall@10\"]*100:>7.1f}% | {row[\"internal_avg_step1\"]:>10.0f} | '\n",
    "              f'{row[\"external_recall@10\"]*100:>7.1f}%{ext_mark} | {row[\"external_avg_step1\"]:>10.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. 2段階検索との比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_stage_search(query_embedding, query_hash, all_hashes, all_embeddings,\n",
    "                     step1_limit=500, top_k=10):\n",
    "    \"\"\"\n",
    "    2段階検索（全件ハミング距離→コサイン）\n",
    "    \"\"\"\n",
    "    # Step 1: 全件ハミング距離\n",
    "    dists = hamming_distance_batch(query_hash, all_hashes)\n",
    "    top_idx = np.argsort(dists)[:step1_limit]\n",
    "    \n",
    "    # Step 2: コサイン類似度\n",
    "    candidate_embs = all_embeddings[top_idx]\n",
    "    query_norm = norm(query_embedding)\n",
    "    candidate_norms = norm(candidate_embs, axis=1)\n",
    "    cosines = (candidate_embs @ query_embedding) / (candidate_norms * query_norm + 1e-10)\n",
    "    top_k_idx = np.argsort(cosines)[-top_k:][::-1]\n",
    "    \n",
    "    return top_idx[top_k_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "2段階検索との比較（5000件）\n",
      "================================================================================\n",
      "\n",
      "2段階検索（Step1=500件）:\n",
      "  内部R@10: 97.8%\n",
      "  外部R@10: 99.5%\n"
     ]
    }
   ],
   "source": [
    "# 2段階検索との比較（5000件）\n",
    "print('=' * 80)\n",
    "print('2段階検索との比較（5000件）')\n",
    "print('=' * 80)\n",
    "\n",
    "two_stage_results = []\n",
    "\n",
    "for sample_idx in range(3):\n",
    "    sample_data = create_sample_dataset(5000, seed=200 + sample_idx)\n",
    "    embeddings = sample_data['embeddings']\n",
    "    hashes = sample_data['hashes']\n",
    "    \n",
    "    # 内部クエリ\n",
    "    rng = np.random.default_rng(42)\n",
    "    internal_indices = rng.choice(5000, 50, replace=False)\n",
    "    \n",
    "    internal_recalls = []\n",
    "    for qi in internal_indices:\n",
    "        gt = compute_ground_truth(embeddings[qi], embeddings, top_k=10)\n",
    "        result = two_stage_search(embeddings[qi], hashes[qi], hashes, embeddings,\n",
    "                                  step1_limit=500, top_k=10)\n",
    "        internal_recalls.append(evaluate_recall(result, gt, k=10))\n",
    "    \n",
    "    # 外部クエリ\n",
    "    external_recalls = []\n",
    "    for q_emb, q_hash in zip(external_query_embs, external_query_hashes):\n",
    "        gt = compute_ground_truth(q_emb, embeddings, top_k=10)\n",
    "        result = two_stage_search(q_emb, q_hash, hashes, embeddings,\n",
    "                                  step1_limit=500, top_k=10)\n",
    "        external_recalls.append(evaluate_recall(result, gt, k=10))\n",
    "    \n",
    "    two_stage_results.append({\n",
    "        'sample_idx': sample_idx,\n",
    "        'internal_recall@10': np.mean(internal_recalls),\n",
    "        'external_recall@10': np.mean(external_recalls),\n",
    "    })\n",
    "\n",
    "df_two_stage = pd.DataFrame(two_stage_results)\n",
    "\n",
    "print(f'\\n2段階検索（Step1=500件）:')\n",
    "print(f'  内部R@10: {df_two_stage[\"internal_recall@10\"].mean()*100:.1f}%')\n",
    "print(f'  外部R@10: {df_two_stage[\"external_recall@10\"].mean()*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. 分析と考察"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "バケットサイズと再現率の関係分析\n",
      "================================================================================\n",
      "\n",
      "■ 外部R@10 >= 80%を達成した設定:\n",
      "  2,000.0件 | Overlap(8.0,2.0) | BktSize=8.1 | Ext R@10=90.2%\n",
      "  2,000.0件 | Overlap(8.0,4.0) | BktSize=8.1 | Ext R@10=87.2%\n",
      "  5,000.0件 | Overlap(8.0,2.0) | BktSize=19.6 | Ext R@10=93.8%\n",
      "  5,000.0件 | Overlap(8.0,4.0) | BktSize=19.6 | Ext R@10=89.8%\n",
      "  10,000.0件 | Overlap(8.0,2.0) | BktSize=39.1 | Ext R@10=91.7%\n",
      "  10,000.0件 | Overlap(8.0,4.0) | BktSize=39.1 | Ext R@10=89.0%\n",
      "  20,000.0件 | Overlap(8.0,2.0) | BktSize=78.1 | Ext R@10=89.8%\n",
      "  20,000.0件 | Overlap(8.0,4.0) | BktSize=78.1 | Ext R@10=87.2%\n",
      "  50,000.0件 | Overlap(8.0,2.0) | BktSize=195.3 | Ext R@10=85.3%\n",
      "  50,000.0件 | Overlap(8.0,4.0) | BktSize=195.3 | Ext R@10=84.3%\n",
      "\n",
      "■ バケットサイズと外部R@10の関係:\n",
      "  バケットサイズ >= 5: 平均外部R@10 = 88.8%\n",
      "  バケットサイズ >= 10: 平均外部R@10 = 88.9%\n",
      "  バケットサイズ >= 20: 平均外部R@10 = 87.9%\n",
      "  バケットサイズ >= 50: 平均外部R@10 = 86.7%\n",
      "  バケットサイズ >= 100: 平均外部R@10 = 84.8%\n"
     ]
    }
   ],
   "source": [
    "# バケットサイズとRecallの関係を分析\n",
    "print('=' * 80)\n",
    "print('バケットサイズと再現率の関係分析')\n",
    "print('=' * 80)\n",
    "\n",
    "# 外部R@10が80%以上の設定を抽出\n",
    "good_configs = df_sizes_agg[df_sizes_agg['external_recall@10'] >= 0.80]\n",
    "\n",
    "print('\\n■ 外部R@10 >= 80%を達成した設定:')\n",
    "if len(good_configs) > 0:\n",
    "    for _, row in good_configs.iterrows():\n",
    "        print(f'  {row[\"n_docs\"]:,}件 | Overlap({row[\"segment_width\"]},{row[\"stride\"]}) | '\n",
    "              f'BktSize={row[\"avg_bucket_size\"]:.1f} | Ext R@10={row[\"external_recall@10\"]*100:.1f}%')\n",
    "else:\n",
    "    print('  なし')\n",
    "\n",
    "# バケットサイズの目安を分析\n",
    "print('\\n■ バケットサイズと外部R@10の関係:')\n",
    "for bucket_threshold in [5, 10, 20, 50, 100]:\n",
    "    filtered = df_sizes_agg[df_sizes_agg['avg_bucket_size'] >= bucket_threshold]\n",
    "    if len(filtered) > 0:\n",
    "        avg_ext_r10 = filtered['external_recall@10'].mean()\n",
    "        print(f'  バケットサイズ >= {bucket_threshold}: 平均外部R@10 = {avg_ext_r10*100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "各Overlap設定の推奨データサイズ\n",
      "================================================================================\n",
      "\n",
      "■ Overlap(8,4):\n",
      "  セグメント数: 31\n",
      "  平均外部R@10: 87.5%\n",
      "  外部R@10>=80%の最小サイズ: 2000\n",
      "\n",
      "■ Overlap(8,2):\n",
      "  セグメント数: 61\n",
      "  平均外部R@10: 90.2%\n",
      "  外部R@10>=80%の最小サイズ: 2000\n",
      "\n",
      "■ Overlap(16,8):\n",
      "  セグメント数: 15\n",
      "  平均外部R@10: 26.5%\n",
      "  外部R@10>=80%の最小サイズ: N/A\n",
      "\n",
      "■ Overlap(16,4):\n",
      "  セグメント数: 29\n",
      "  平均外部R@10: 33.0%\n",
      "  外部R@10>=80%の最小サイズ: N/A\n",
      "\n",
      "■ Overlap(32,16):\n",
      "  セグメント数: 7\n",
      "  平均外部R@10: 1.3%\n",
      "  外部R@10>=80%の最小サイズ: N/A\n",
      "\n",
      "■ Overlap(32,8):\n",
      "  セグメント数: 13\n",
      "  平均外部R@10: 1.9%\n",
      "  外部R@10>=80%の最小サイズ: N/A\n"
     ]
    }
   ],
   "source": [
    "# 設定ごとの最適データサイズを分析\n",
    "print('\\n' + '=' * 80)\n",
    "print('各Overlap設定の推奨データサイズ')\n",
    "print('=' * 80)\n",
    "\n",
    "for seg_width, stride in OVERLAP_CONFIGS:\n",
    "    df_config = df_sizes_agg[\n",
    "        (df_sizes_agg['segment_width'] == seg_width) &\n",
    "        (df_sizes_agg['stride'] == stride)\n",
    "    ]\n",
    "    \n",
    "    # 外部R@10 >= 80%を達成する最小サイズ\n",
    "    good_sizes = df_config[df_config['external_recall@10'] >= 0.80]['n_docs']\n",
    "    min_size = good_sizes.min() if len(good_sizes) > 0 else 'N/A'\n",
    "    \n",
    "    # 全サイズでの平均R@10\n",
    "    avg_ext_r10 = df_config['external_recall@10'].mean()\n",
    "    \n",
    "    print(f'\\n■ Overlap({seg_width},{stride}):')\n",
    "    print(f'  セグメント数: {df_config[\"n_segments\"].iloc[0]}')\n",
    "    print(f'  平均外部R@10: {avg_ext_r10*100:.1f}%')\n",
    "    print(f'  外部R@10>=80%の最小サイズ: {min_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. 結論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "結論\n",
      "================================================================================\n",
      "\n",
      "■ 実験目的\n",
      "  小規模データ（5000件程度）でOverlapセグメント幅を調整することで、\n",
      "  バケットのスパース問題を解決できるか検証する。\n",
      "\n",
      "■ 5000件での最良Overlap設定\n",
      "  設定: Overlap(8,2)\n",
      "  セグメント数: 61\n",
      "  平均バケットサイズ: 19.6\n",
      "  内部R@10: 89.8%\n",
      "  外部R@10: 94.3%\n",
      "\n",
      "■ 2段階検索（ベースライン）\n",
      "  外部R@10: 99.5%\n",
      "\n",
      "■ 考察\n",
      "\n",
      "  ✓ Overlap(8,2)で外部R@10>=80%を達成\n",
      "  △ 2段階検索より劣るが、許容範囲内\n"
     ]
    }
   ],
   "source": [
    "print('=' * 80)\n",
    "print('結論')\n",
    "print('=' * 80)\n",
    "\n",
    "# 5000件での最良設定\n",
    "df_5000_sorted = df_5000_agg.sort_values('external_r10_mean', ascending=False)\n",
    "best_5000 = df_5000_sorted.iloc[0]\n",
    "\n",
    "# 2段階検索の結果\n",
    "two_stage_ext_r10 = df_two_stage['external_recall@10'].mean()\n",
    "\n",
    "print(f'''\n",
    "■ 実験目的\n",
    "  小規模データ（5000件程度）でOverlapセグメント幅を調整することで、\n",
    "  バケットのスパース問題を解決できるか検証する。\n",
    "\n",
    "■ 5000件での最良Overlap設定\n",
    "  設定: Overlap({int(best_5000['segment_width'])},{int(best_5000['stride'])})\n",
    "  セグメント数: {int(best_5000['n_segments'])}\n",
    "  平均バケットサイズ: {best_5000['avg_bucket_size']:.1f}\n",
    "  内部R@10: {best_5000['internal_r10_mean']*100:.1f}%\n",
    "  外部R@10: {best_5000['external_r10_mean']*100:.1f}%\n",
    "\n",
    "■ 2段階検索（ベースライン）\n",
    "  外部R@10: {two_stage_ext_r10*100:.1f}%\n",
    "\n",
    "■ 考察\n",
    "''')\n",
    "\n",
    "if best_5000['external_r10_mean'] >= 0.80:\n",
    "    print(f'  ✓ Overlap({int(best_5000[\"segment_width\"])},{int(best_5000[\"stride\"])})で外部R@10>=80%を達成')\n",
    "    if best_5000['external_r10_mean'] >= two_stage_ext_r10 * 0.95:\n",
    "        print(f'  ✓ 2段階検索と同等の性能を維持しながら、Step1での枝刈りが可能')\n",
    "    else:\n",
    "        print(f'  △ 2段階検索より劣るが、許容範囲内')\n",
    "else:\n",
    "    print(f'  △ 最良設定でも外部R@10<80%')\n",
    "    print(f'  → 5000件規模では2段階検索を推奨')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. 推奨設定まとめ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "データサイズ別推奨設定\n",
      "================================================================================\n",
      "\n",
      "     データ件数 |         推奨設定 |  Segs |  BktSize | Int R@10 | Ext R@10\n",
      "--------------------------------------------------------------------------------\n",
      "     2,000 |        (8,2) |    61 |      8.1 |    86.0% |    90.2%★\n",
      "     5,000 |        (8,2) |    61 |     19.6 |    90.1% |    93.8%★\n",
      "    10,000 |        (8,2) |    61 |     39.1 |    88.2% |    91.7%★\n",
      "    20,000 |        (8,2) |    61 |     78.1 |    84.2% |    89.8%★\n",
      "    50,000 |        (8,2) |    61 |    195.3 |    76.4% |    85.3%★\n"
     ]
    }
   ],
   "source": [
    "print('=' * 80)\n",
    "print('データサイズ別推奨設定')\n",
    "print('=' * 80)\n",
    "\n",
    "# 各サイズで最良の設定を特定\n",
    "recommendations = []\n",
    "\n",
    "for size in SIZE_CONFIGS:\n",
    "    df_size = df_sizes_agg[df_sizes_agg['n_docs'] == size]\n",
    "    best = df_size.sort_values('external_recall@10', ascending=False).iloc[0]\n",
    "    \n",
    "    recommendations.append({\n",
    "        'n_docs': size,\n",
    "        'best_config': f\"({int(best['segment_width'])},{int(best['stride'])})\",\n",
    "        'n_segments': int(best['n_segments']),\n",
    "        'avg_bucket_size': best['avg_bucket_size'],\n",
    "        'internal_r10': best['internal_recall@10'],\n",
    "        'external_r10': best['external_recall@10'],\n",
    "    })\n",
    "\n",
    "df_rec = pd.DataFrame(recommendations)\n",
    "\n",
    "print(f'\\n{\"データ件数\":>10} | {\"推奨設定\":>12} | {\"Segs\":>5} | {\"BktSize\":>8} | {\"Int R@10\":>8} | {\"Ext R@10\":>8}')\n",
    "print('-' * 80)\n",
    "\n",
    "for _, row in df_rec.iterrows():\n",
    "    ext_mark = '★' if row['external_r10'] >= 0.80 else ''\n",
    "    print(f'{row[\"n_docs\"]:>10,} | {row[\"best_config\"]:>12} | {row[\"n_segments\"]:>5} | '\n",
    "          f'{row[\"avg_bucket_size\"]:>8.1f} | {row[\"internal_r10\"]*100:>7.1f}% | '\n",
    "          f'{row[\"external_r10\"]*100:>7.1f}%{ext_mark}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 11. 実験評価まとめ\n\n### 重要な発見：仮説と異なる結果\n\n**当初の仮説**:\n- セグメント幅を大きくする（16bit, 32bit）とバケット数が減り、各バケットに含まれるドキュメントが増える\n- これにより小規模データでもOverlap方式が有効になる\n\n**実際の結果**:\n| 設定 | セグメント幅 | バケット数(5000件) | 平均バケットサイズ | 外部R@10 |\n|------|------------|-------------------|-------------------|----------|\n| **(8,2)** | 8bit | 15,559 | **19.6** | **94.3%** ◎ |\n| (8,4) | 8bit | 7,906 | 19.6 | 90.3% ★ |\n| (16,4) | 16bit | 105,122 | 1.4 | 30.2% ✗ |\n| (16,8) | 16bit | 54,150 | 1.4 | 22.2% ✗ |\n| (32,8) | 32bit | 63,028 | 1.0 | 1.2% ✗ |\n| (32,16) | 32bit | 33,927 | 1.0 | 0.6% ✗ |\n\n→ **仮説は間違いだった！** セグメント幅を大きくするとむしろ悪化する\n\n---\n\n### 仮説の誤り（重要）\n\n**当初の仮説**:\n> セグメント幅を大きくする（16bit, 32bit）とバケット数が減り、各バケットに含まれるドキュメント数が増える\n\n**この仮説は根本的に間違っていた**:\n\n| セグメント幅 | 理論上の最大バケット数 | 5000件での実際 | 平均バケットサイズ |\n|------------|---------------------|--------------|------------------|\n| 8bit | 256/セグメント | 256（飽和） | **19.5件** |\n| 16bit | 65,536/セグメント | 5,000（データ数で制限） | **1件** |\n| 32bit | 4.3B/セグメント | 5,000（データ数で制限） | **1件** |\n\n**誤りの原因**:\n- バケット数は「理論上の最大」であり、実際に使われるバケット数は**データ件数で制限される**\n- 16bit/32bitでは、バケット数が減るのではなく、**スパースなバケットが増える**\n- データ5000件を65,536バケットに分散 → 各バケットに平均0.076件\n\n---\n\n### なぜセグメント幅を大きくすると悪化するのか\n\n1. **完全一致の確率が激減**\n   - 8bitセグメント: クエリがヒットする確率 ≈ 5000/256 = **19.5件/バケット** → ほぼ必ずヒット\n   - 16bitセグメント: クエリがヒットする確率 ≈ 5000/65536 = **0.076件/バケット** → 7.6%の確率\n   - 32bitセグメント: クエリがヒットする確率 ≈ 5000/4.3B ≈ **0件** → ほぼヒットしない\n\n2. **OR検索でも救えない**\n   - 16bit × 15セグメント: 各セグメントのヒット率7.6% → OR検索でも候補が約26件しか集まらない\n   - 32bit × 13セグメント: 各セグメントのヒット率≈0% → OR検索でも候補が0件\n\n3. **正しい理解**\n   - 問題は「バケットがスパース」ではなく「**セグメント完全一致の確率が低すぎる**」\n   - セグメント幅が大きいほど、クエリと同じバケットに入るドキュメントが存在しない確率が上がる\n\n---\n\n### 最良設定: Overlap(8, 2)\n\n**5000件での推奨**: `Overlap(8, 2)`\n```\nセグメント幅: 8bit\nストライド: 2bit\nセグメント数: 61（(128-8)/2+1）\n平均バケットサイズ: 19.6件\n\nStep 1候補: 約1,100件（22%）\n削減率: 78%\n外部R@10: 94.3%\n```\n\n**なぜ(8,2)が(8,4)より良いか**:\n- ストライドを小さくする → セグメント数が増える（31→61）\n- セグメント数が多い → クエリのセグメントがヒットする確率が上がる\n- より多くのOR条件でカバーできる\n\n---\n\n### 2段階検索との比較\n\n| 方式 | 外部R@10 | Step1候補 | 計算量 |\n|------|---------|-----------|--------|\n| 2段階検索 | **99.5%** | 全件(5000) | O(N) |\n| Overlap(8,2) | 94.3% | ~1,100件 | O(セグメント数 × ルックアップ) |\n| Overlap(8,4) | 90.3% | ~900件 | O(セグメント数 × ルックアップ) |\n\n**トレードオフ**:\n- 2段階検索は精度最高だが、全件ハミング距離計算が必要\n- Overlap(8,2)は精度5%低下だが、78%の候補削減\n\n---\n\n### データサイズ別推奨設定\n\n| データ件数 | 推奨設定 | セグメント数 | 外部R@10 | コメント |\n|-----------|---------|-------------|----------|----------|\n| 2,000件 | (8,2) | 61 | 90.2% | ★3段階可能 |\n| 5,000件 | (8,2) | 61 | 93.8% | ★3段階推奨 |\n| 10,000件 | (8,2) | 61 | 91.7% | ★3段階推奨 |\n| 20,000件 | (8,4) or (8,2) | 31/61 | 87-90% | ★どちらでも可 |\n| 50,000件 | (8,4) | 31 | 84.3% | ★3段階推奨 |\n\n---\n\n### 結論\n\n1. **セグメント幅を大きくしても解決しない（仮説は棄却）**\n   - 16bit, 32bitセグメントは完全一致の確率が低すぎて候補が集まらない\n   - バケット数が減るのではなく、スパースなバケットが増えるだけ\n\n2. **ストライドを小さくして、セグメント数を増やすのが有効**\n   - Overlap(8,4) → Overlap(8,2)にすることで外部R@10が+4pt改善\n   - セグメント数: 31 → 61\n\n3. **5000件規模でも3段階カスケードは有効**\n   - Overlap(8,2)で外部R@10=94.3%を達成\n   - 2段階検索の99.5%より5%低いが、候補削減78%のメリット\n\n4. **小規模データでより精度が必要な場合は2段階検索を使用**\n   - 精度優先なら2段階検索（全件ハミング距離→コサイン）\n   - 速度優先ならOverlap(8,2)\n\n---\n\n### 次のステップ\n\n1. 本番データ（1.4万件）で Overlap(8,2) を検証\n2. さらにストライドを小さくする（8,1）の検証\n3. 処理時間の比較（3段階 vs 2段階）"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsh-cascade-poc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}