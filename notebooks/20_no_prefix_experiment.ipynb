{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20. プレフィックスなしの埋め込み実験\n",
    "\n",
    "## 背景\n",
    "\n",
    "e5-largeモデルでは、以下のプレフィックスを使い分けることが推奨されている：\n",
    "- ドキュメント: `passage: {text}`\n",
    "- クエリ: `query: {text}`\n",
    "\n",
    "しかし、LSH（SimHash）との相性問題から、**プレフィックスを使わない**埋め込みの効果を検証する。\n",
    "\n",
    "## 仮説\n",
    "\n",
    "プレフィックスを使わない場合、ドキュメントとクエリが同じ空間にマッピングされるため、\n",
    "LSHのハミング距離がコサイン類似度とより強く相関する可能性がある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Phase 1: 小規模実験（1000件）\n",
    "\n",
    "`scripts/no_prefix_experiment.py` を実行した結果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実験設定\n",
    "\n",
    "- **データ**: 1000件（各データセットから250件）\n",
    "- **候補数**: 50, 100, 200件（5-20%の候補を選択）\n",
    "- **クエリ**: 25件\n",
    "\n",
    "### 検証パターン\n",
    "\n",
    "| パターン | ドキュメント埋め込み | クエリ埋め込み |\n",
    "|----------|----------------------|----------------|\n",
    "| 現行 | passage: {text} | query: {text} |\n",
    "| A | passage: {text} | passage: {text} |\n",
    "| B | passage: {text} | {text} (なし) |\n",
    "| C | {text} (なし) | query: {text} |\n",
    "| D | {text} (なし) | passage: {text} |\n",
    "| **E** | **{text} (なし)** | **{text} (なし)** |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結果: Recall@10（候補100件 / 1000件中 = 90%削減）\n",
    "\n",
    "| パターン | Recall | 現行比 |\n",
    "|----------|--------|--------|\n",
    "| Doc=passage, Query=query (現行) | **36.8%** | - |\n",
    "| Doc=passage, Query=passage | **83.6%** | +46.8pt |\n",
    "| Doc=passage, Query=none | 57.2% | +20.4pt |\n",
    "| Doc=none, Query=query | 53.2% | +16.4pt |\n",
    "| Doc=none, Query=passage | 57.6% | +20.8pt |\n",
    "| **Doc=none, Query=none** | **68.0%** | **+31.2pt** |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## クエリタイプ別（候補100件）\n",
    "\n",
    "| パターン | JA短文 | JA曖昧 | EN短文 | EN曖昧 |\n",
    "|----------|--------|--------|--------|--------|\n",
    "| Doc=passage, Query=query (現行) | 44% | 38% | 24% | 34% |\n",
    "| Doc=none, Query=none | **68%** | **62%** | **72%** | **70%** |\n",
    "\n",
    "**英語クエリの改善が特に顕著:**\n",
    "- EN短文: 24% → 72%（+48pt）\n",
    "- EN曖昧: 34% → 70%（+36pt）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1 考察\n",
    "\n",
    "### 発見事項\n",
    "\n",
    "1. **プレフィックスなし（Doc=none, Query=none）は現行から+31.2%改善**\n",
    "   - 現行: 36.8% → プレフィックスなし: 68.0%\n",
    "\n",
    "2. **最良は Doc=passage, Query=passage（83.6%）**\n",
    "   - ただしこれはe5の非対称検索を完全に放棄する設定\n",
    "\n",
    "3. **プレフィックスなしは現実的な妥協点**\n",
    "   - e5の設計思想からは外れるが、LSHとの相性は大幅に改善\n",
    "   - 英語クエリで特に効果的\n",
    "\n",
    "### 理論的な解釈\n",
    "\n",
    "e5モデルの `query:` / `passage:` プレフィックスは、クエリとドキュメントを**意図的に異なる空間**にマッピングしている。\n",
    "これにより、コサイン類似度での検索精度が向上するが、**ハミング距離との相関が崩れる**。\n",
    "\n",
    "プレフィックスなしの場合、クエリとドキュメントが**同じ空間**にマッピングされるため、\n",
    "ハミング距離がコサイン類似度をより正確に近似できる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Phase 2: 大規模検証（40万件）\n",
    "\n",
    "`scripts/create_no_prefix_index.py` で40万件の埋め込みを再生成し、検証を行う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from src.lsh import SimHashGenerator, hamming_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DuckDBに接続\n",
    "con = duckdb.connect('../data/experiment_400k.duckdb', read_only=True)\n",
    "\n",
    "# テーブル確認\n",
    "tables = con.execute(\"SHOW TABLES\").fetchall()\n",
    "print('テーブル一覧:')\n",
    "for table in tables:\n",
    "    print(f'  - {table[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# プレフィックスなしの埋め込みを読み込み\n",
    "print('データ読み込み中...')\n",
    "datasets = ['body_en', 'body_ja', 'titles_en', 'titles_ja']\n",
    "embeddings_no_prefix = {}\n",
    "embeddings_with_prefix = {}\n",
    "\n",
    "for dataset in tqdm(datasets, desc='データセット'):\n",
    "    # プレフィックスなし\n",
    "    df_no = con.execute(f\"\"\"\n",
    "        SELECT embedding FROM documents_no_prefix\n",
    "        WHERE dataset = '{dataset}'\n",
    "        ORDER BY id\n",
    "    \"\"\").fetchdf()\n",
    "    embeddings_no_prefix[dataset] = np.array(df_no['embedding'].tolist(), dtype=np.float32)\n",
    "    \n",
    "    # プレフィックスあり（既存）\n",
    "    df_with = con.execute(f\"\"\"\n",
    "        SELECT embedding FROM documents\n",
    "        WHERE dataset = '{dataset}'\n",
    "        ORDER BY id\n",
    "    \"\"\").fetchdf()\n",
    "    embeddings_with_prefix[dataset] = np.array(df_with['embedding'].tolist(), dtype=np.float32)\n",
    "\n",
    "print('完了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全データを統合\n",
    "all_embs_no_prefix = np.vstack([embeddings_no_prefix[d] for d in datasets])\n",
    "all_embs_with_prefix = np.vstack([embeddings_with_prefix[d] for d in datasets])\n",
    "\n",
    "print(f'プレフィックスなし: {all_embs_no_prefix.shape}')\n",
    "print(f'プレフィックスあり: {all_embs_with_prefix.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E5モデルを読み込み\n",
    "print('E5モデルを読み込み中...')\n",
    "model = SentenceTransformer('intfloat/multilingual-e5-large')\n",
    "print('完了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 検索クエリ\n",
    "search_queries = [\n",
    "    ('東京', 'ja', 'short'),\n",
    "    ('人工知能', 'ja', 'short'),\n",
    "    ('日本の歴史', 'ja', 'short'),\n",
    "    ('プログラミング', 'ja', 'short'),\n",
    "    ('音楽', 'ja', 'short'),\n",
    "    ('環境問題', 'ja', 'short'),\n",
    "    ('宇宙探査', 'ja', 'short'),\n",
    "    ('経済学', 'ja', 'short'),\n",
    "    ('医療技術', 'ja', 'short'),\n",
    "    ('文学作品', 'ja', 'short'),\n",
    "    ('最近話題になっている技術革新について知りたいのですが', 'ja', 'ambiguous'),\n",
    "    ('日本の伝統的な文化や芸術に関する情報', 'ja', 'ambiguous'),\n",
    "    ('環境に優しい持続可能な社会を実現するための取り組み', 'ja', 'ambiguous'),\n",
    "    ('健康的な生活を送るために必要なこと', 'ja', 'ambiguous'),\n",
    "    ('世界の政治情勢や国際関係についての最新動向', 'ja', 'ambiguous'),\n",
    "    ('Tokyo', 'en', 'short'),\n",
    "    ('Artificial intelligence', 'en', 'short'),\n",
    "    ('World history', 'en', 'short'),\n",
    "    ('Programming', 'en', 'short'),\n",
    "    ('Climate change', 'en', 'short'),\n",
    "    ('Recent technological innovations and developments', 'en', 'ambiguous'),\n",
    "    ('Traditional culture and arts from around the world', 'en', 'ambiguous'),\n",
    "    ('Sustainable approaches to environmental protection', 'en', 'ambiguous'),\n",
    "    ('Latest developments in space exploration', 'en', 'ambiguous'),\n",
    "    ('Key factors for business success and entrepreneurship', 'en', 'ambiguous'),\n",
    "]\n",
    "\n",
    "query_texts = [q[0] for q in search_queries]\n",
    "print(f'検索クエリ数: {len(query_texts)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# クエリ埋め込みを生成\n",
    "print('クエリ埋め込みを生成中...')\n",
    "\n",
    "# プレフィックスなし\n",
    "query_embs_none = model.encode(query_texts, normalize_embeddings=False).astype(np.float32)\n",
    "\n",
    "# query:プレフィックス\n",
    "query_embs_query = model.encode(\n",
    "    [f'query: {t}' for t in query_texts],\n",
    "    normalize_embeddings=False\n",
    ").astype(np.float32)\n",
    "\n",
    "print('完了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超平面を生成（各パターン用）\n",
    "def generate_hyperplanes(embeddings, num_hyperplanes=128, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    sample_indices = rng.choice(len(embeddings), min(300, len(embeddings)), replace=False)\n",
    "    sample_embs = embeddings[sample_indices]\n",
    "    \n",
    "    hyperplanes = []\n",
    "    for _ in range(num_hyperplanes):\n",
    "        i, j = rng.choice(len(sample_embs), 2, replace=False)\n",
    "        diff = sample_embs[i] - sample_embs[j]\n",
    "        diff = diff / np.linalg.norm(diff)\n",
    "        hyperplanes.append(diff)\n",
    "    \n",
    "    return np.array(hyperplanes, dtype=np.float32)\n",
    "\n",
    "# プレフィックスなし用の超平面\n",
    "hp_no_prefix = generate_hyperplanes(all_embs_no_prefix)\n",
    "\n",
    "# プレフィックスあり用の超平面\n",
    "hp_with_prefix = generate_hyperplanes(all_embs_with_prefix)\n",
    "\n",
    "print(f'超平面形状: {hp_no_prefix.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評価関数\n",
    "def evaluate_pattern(\n",
    "    doc_embeddings: np.ndarray,\n",
    "    query_embeddings: np.ndarray,\n",
    "    hyperplanes: np.ndarray,\n",
    "    search_queries: list,\n",
    "    candidate_limits: list,\n",
    "    top_k: int = 10\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    パターンを評価してDataFrameを返す\n",
    "    \"\"\"\n",
    "    gen = SimHashGenerator(dim=1024, hash_bits=128, seed=0, strategy='random')\n",
    "    gen.hyperplanes = hyperplanes\n",
    "    \n",
    "    doc_hashes = gen.hash_batch(doc_embeddings)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, (query_text, lang, query_type) in enumerate(search_queries):\n",
    "        query_emb = query_embeddings[i]\n",
    "        query_hash = gen.hash_batch(query_emb.reshape(1, -1))[0]\n",
    "        \n",
    "        # Ground Truth\n",
    "        cos_sims = (doc_embeddings @ query_emb) / (norm(doc_embeddings, axis=1) * norm(query_emb))\n",
    "        gt_indices = set(np.argsort(cos_sims)[::-1][:top_k])\n",
    "        \n",
    "        # ハミング距離\n",
    "        distances = [(j, hamming_distance(h, query_hash)) for j, h in enumerate(doc_hashes)]\n",
    "        distances.sort(key=lambda x: x[1])\n",
    "        \n",
    "        for limit in candidate_limits:\n",
    "            candidates = set(idx for idx, _ in distances[:limit])\n",
    "            recall = len(gt_indices & candidates) / top_k\n",
    "            \n",
    "            results.append({\n",
    "                'query': query_text,\n",
    "                'lang': lang,\n",
    "                'query_type': query_type,\n",
    "                'candidate_limit': limit,\n",
    "                'recall': recall\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評価実行\n",
    "print('評価を実行中...')\n",
    "candidate_limits = [1000, 2000, 5000, 10000]\n",
    "\n",
    "# パターン1: 現行（Doc=passage, Query=query）\n",
    "print('  パターン1: Doc=passage, Query=query...')\n",
    "df_current = evaluate_pattern(\n",
    "    all_embs_with_prefix, query_embs_query, hp_with_prefix,\n",
    "    search_queries, candidate_limits\n",
    ")\n",
    "df_current['pattern'] = 'Doc=passage, Query=query'\n",
    "\n",
    "# パターン2: プレフィックスなし（Doc=none, Query=none）\n",
    "print('  パターン2: Doc=none, Query=none...')\n",
    "df_no_prefix = evaluate_pattern(\n",
    "    all_embs_no_prefix, query_embs_none, hp_no_prefix,\n",
    "    search_queries, candidate_limits\n",
    ")\n",
    "df_no_prefix['pattern'] = 'Doc=none, Query=none'\n",
    "\n",
    "# パターン3: Doc=none, Query=query（ハイブリッド）\n",
    "print('  パターン3: Doc=none, Query=query...')\n",
    "df_hybrid = evaluate_pattern(\n",
    "    all_embs_no_prefix, query_embs_query, hp_no_prefix,\n",
    "    search_queries, candidate_limits\n",
    ")\n",
    "df_hybrid['pattern'] = 'Doc=none, Query=query'\n",
    "\n",
    "# 結合\n",
    "df_results = pd.concat([df_current, df_no_prefix, df_hybrid], ignore_index=True)\n",
    "print('完了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果表示\n",
    "print('=' * 90)\n",
    "print('結果: 40万件でのRecall@10比較')\n",
    "print('=' * 90)\n",
    "\n",
    "pivot = df_results.groupby(['pattern', 'candidate_limit'])['recall'].mean().unstack()\n",
    "\n",
    "print(f'\\n{\"パターン\":>30} | {\"1000件\":>10} | {\"2000件\":>10} | {\"5000件\":>10} | {\"10000件\":>10}')\n",
    "print('-' * 80)\n",
    "\n",
    "for pattern in ['Doc=passage, Query=query', 'Doc=none, Query=none', 'Doc=none, Query=query']:\n",
    "    if pattern in pivot.index:\n",
    "        row = pivot.loc[pattern]\n",
    "        print(f'{pattern:>30} | {row[1000]:>10.1%} | {row[2000]:>10.1%} | {row[5000]:>10.1%} | {row[10000]:>10.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# クエリタイプ別（候補2000件）\n",
    "print('\\n' + '=' * 85)\n",
    "print('クエリタイプ別 Recall@10（候補2000件）')\n",
    "print('=' * 85)\n",
    "\n",
    "subset = df_results[df_results['candidate_limit'] == 2000]\n",
    "\n",
    "print(f'\\n{\"パターン\":>30} | {\"JA短文\":>10} | {\"JA曖昧\":>10} | {\"EN短文\":>10} | {\"EN曖昧\":>10}')\n",
    "print('-' * 85)\n",
    "\n",
    "for pattern in ['Doc=passage, Query=query', 'Doc=none, Query=none', 'Doc=none, Query=query']:\n",
    "    pattern_subset = subset[subset['pattern'] == pattern]\n",
    "    \n",
    "    ja_short = pattern_subset[(pattern_subset['lang'] == 'ja') & (pattern_subset['query_type'] == 'short')]['recall'].mean()\n",
    "    ja_amb = pattern_subset[(pattern_subset['lang'] == 'ja') & (pattern_subset['query_type'] == 'ambiguous')]['recall'].mean()\n",
    "    en_short = pattern_subset[(pattern_subset['lang'] == 'en') & (pattern_subset['query_type'] == 'short')]['recall'].mean()\n",
    "    en_amb = pattern_subset[(pattern_subset['lang'] == 'en') & (pattern_subset['query_type'] == 'ambiguous')]['recall'].mean()\n",
    "    \n",
    "    print(f'{pattern:>30} | {ja_short:>10.1%} | {ja_amb:>10.1%} | {en_short:>10.1%} | {en_amb:>10.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 接続を閉じる\n",
    "con.close()\n",
    "print('完了')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# 結論\n\n## Phase 1（小規模実験、1000件）\n\n| パターン | Recall (100件/1000件中) |\n|----------|-------------------------|\n| Doc=passage, Query=query (現行) | 36.8% |\n| Doc=none, Query=none | **68.0%** (+31.2pt) |\n\n## Phase 2（大規模検証、40万件）\n\n`scripts/evaluate_no_prefix_index.py` の結果：\n\n### Recall@10（候補2000件）\n\n| パターン | Recall | 現行比 |\n|----------|--------|--------|\n| Baseline (passage/query) | 32.3% | - |\n| **No Prefix (none/none)** | **44.0%** | **+11.7pt** |\n| Hybrid (none/query) | 40.3% | +8.0pt |\n| Phase1 Best (passage/passage) | 69.3% | +37.0pt |\n\n### 候補数別推移\n\n| パターン | 500 | 1000 | 2000 | 5000 | 10000 | 20000 |\n|----------|-----|------|------|------|-------|-------|\n| Baseline (passage/query) | 17.0% | 24.3% | 32.3% | 45.0% | 56.0% | 66.0% |\n| No Prefix (none/none) | 26.3% | 37.0% | 44.0% | 55.3% | 67.0% | 76.0% |\n| Hybrid (none/query) | 25.0% | 33.0% | 40.3% | 52.7% | 64.7% | 72.0% |\n| Phase1 Best (passage/passage) | 52.0% | 61.3% | 69.3% | 80.7% | 89.0% | 94.3% |\n\n### クエリタイプ別（候補2000件）\n\n| パターン | JA短文 | JA曖昧 | EN短文 | EN曖昧 |\n|----------|--------|--------|--------|--------|\n| Baseline (passage/query) | 34.0% | 23.0% | 58.0% | 22.0% |\n| **No Prefix (none/none)** | **62.0%** | **33.0%** | 48.0% | 26.0% |\n| Phase1 Best (passage/passage) | 81.0% | 63.0% | 74.0% | 54.0% |\n\n### 改善度分析\n\n**改善したクエリ（Top 5）:**\n- 「プログラミング」: 30% → 90% (+60pt)\n- 「経済学」: 20% → 80% (+60pt)\n- 「東京」: 10% → 60% (+50pt)\n\n**悪化したクエリ:**\n- 「World history」: 50% → 0% (-50pt)\n- 「子供の教育において...」: 70% → 10% (-60pt)\n\n## 総合評価\n\n1. **プレフィックスなしで11.7%の改善**（32.3% → 44.0%）\n   - 日本語短文で特に効果的（+28pt）\n   - 一部の英語クエリで悪化あり\n\n2. **最良はpassage/passageパターン（69.3%）**\n   - これはe5の非対称検索を放棄する設定\n   - 実用上の問題がないならこれが最適\n\n3. **プレフィックスなしは現実的な妥協点**\n   - e5設計からは外れるが、LSH相性は改善\n   - 日本語検索主体のシステムで有効\n\n## 推奨事項\n\n**LSH候補選択には以下のいずれかを推奨:**\n\n1. **passage/passage**: Recall@10 = 69.3%（候補2000件）\n   - e5の非対称設計を放棄するが、最高精度\n\n2. **none/none**: Recall@10 = 44.0%（候補2000件）\n   - 中程度の精度だが、日本語に強い\n\n3. **候補数の増加**: 20000件まで増やせばbaselineでも66%達成\n   - 計算コストとのトレードオフ\n\n**結論**: SimHashとe5-largeの組み合わせには本質的な限界があり、\nFAISS/Annoy/ScaNNなどの専用ライブラリへの移行も検討すべき。"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}