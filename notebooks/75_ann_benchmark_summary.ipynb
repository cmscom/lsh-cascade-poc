{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 75. ANN Benchmark Summary\n",
    "\n",
    "## Purpose\n",
    "- Aggregate results from experiments 73-74\n",
    "- Master comparison: Exact vs HNSW vs C-only vs C+Pivot\n",
    "- Analyze ITQ hash quality across different data characteristics\n",
    "- Determine when ITQ-LSH + Pivot is practical vs HNSW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T22:51:22.071551Z",
     "iopub.status.busy": "2026-02-08T22:51:22.071382Z",
     "iopub.status.idle": "2026-02-08T22:51:22.121704Z",
     "shell.execute_reply": "2026-02-08T22:51:22.121118Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path('../data')\n",
    "DATASET_KEYS = ['glove', 'sift', 'fashion', 'nytimes', 'gist']\n",
    "TOP_K_VALUES = [1, 10, 100]\n",
    "CANDIDATE_LIMITS = [100, 500, 1000, 2000, 5000]\n",
    "PIVOT_THRESHOLDS = [15, 20, 25, 30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 1. Load All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T22:51:22.125131Z",
     "iopub.status.busy": "2026-02-08T22:51:22.124920Z",
     "iopub.status.idle": "2026-02-08T22:51:22.146962Z",
     "shell.execute_reply": "2026-02-08T22:51:22.146132Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded results for 5 datasets: ['glove', 'sift', 'fashion', 'nytimes', 'gist']\n"
     ]
    }
   ],
   "source": [
    "# From notebook 73\n",
    "c_only_results = np.load(DATA_DIR / 'ann_c_only_results.npy', allow_pickle=True).item()\n",
    "quality_results = np.load(DATA_DIR / 'ann_quality_results.npy', allow_pickle=True).item()\n",
    "filter_recalls = np.load(DATA_DIR / 'ann_filter_recalls.npy', allow_pickle=True).item()\n",
    "\n",
    "# From notebook 74\n",
    "c_pivot_results = np.load(DATA_DIR / 'ann_c_pivot_results.npy', allow_pickle=True).item()\n",
    "hnsw_results = np.load(DATA_DIR / 'ann_hnsw_results.npy', allow_pickle=True).item()\n",
    "\n",
    "# Dataset metadata\n",
    "dataset_info = {}\n",
    "for key in DATASET_KEYS:\n",
    "    train_path = DATA_DIR / f'ann_{key}_train.npy'\n",
    "    if not train_path.exists():\n",
    "        continue\n",
    "    train = np.load(train_path, mmap_mode='r')\n",
    "    dataset_info[key] = {'n_train': train.shape[0], 'dim': train.shape[1]}\n",
    "\n",
    "available_keys = [k for k in DATASET_KEYS if k in dataset_info]\n",
    "print(f'Loaded results for {len(available_keys)} datasets: {available_keys}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2. Master Comparison Table: Recall@10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T22:51:22.148557Z",
     "iopub.status.busy": "2026-02-08T22:51:22.148414Z",
     "iopub.status.idle": "2026-02-08T22:51:22.154443Z",
     "shell.execute_reply": "2026-02-08T22:51:22.154000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Master Comparison: Recall@10\n",
      "====================================================================================================\n",
      "\n",
      "Dataset       D          N    HNSW   C(L=500)  C(L=1000)  C(L=2000)  P(20,1000)  P(20,2000)  P(25,1000)  P(25,2000)\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "glove       100  1,183,514   79.9%      58.5%      66.9%      74.8%       67.0%       74.6%       66.7%       74.8%\n",
      "sift        128  1,000,000   97.9%      76.2%      85.0%      91.6%       85.3%       91.4%       85.3%       91.5%\n",
      "fashion     784     60,000   99.3%      97.2%      99.1%      99.7%       98.9%       99.4%       99.2%       99.8%\n",
      "nytimes     256    290,000   84.7%      68.9%      74.2%      79.4%       73.0%       78.0%       73.6%       78.6%\n",
      "gist        960  1,000,000   81.6%      48.1%      59.4%      70.2%       59.5%       70.3%       59.4%       70.4%\n"
     ]
    }
   ],
   "source": [
    "print('='*100)\n",
    "print('Master Comparison: Recall@10')\n",
    "print('='*100)\n",
    "\n",
    "L_VALS = [500, 1000, 2000]\n",
    "T_VALS = [20, 25]\n",
    "\n",
    "# Header\n",
    "header = f'{\"Dataset\":<10} {\"D\":>4} {\"N\":>10} {\"HNSW\":>7}'\n",
    "for L in L_VALS:\n",
    "    header += f' {\"C(L=\"+str(L)+\")\":>10}'\n",
    "for t in T_VALS:\n",
    "    for L in [1000, 2000]:\n",
    "        header += f' {\"P(\"+str(t)+\",\"+str(L)+\")\":>11}'\n",
    "print(f'\\n{header}')\n",
    "print('-' * len(header))\n",
    "\n",
    "for key in available_keys:\n",
    "    info = dataset_info[key]\n",
    "    \n",
    "    # HNSW\n",
    "    if key in hnsw_results and hnsw_results[key]['recalls'].get(10) is not None:\n",
    "        hnsw_r10 = f'{hnsw_results[key][\"recalls\"][10]*100:.1f}%'\n",
    "    else:\n",
    "        hnsw_r10 = 'N/A'\n",
    "    \n",
    "    line = f'{key:<10} {info[\"dim\"]:>4} {info[\"n_train\"]:>10,} {hnsw_r10:>7}'\n",
    "    \n",
    "    # C-only\n",
    "    for L in L_VALS:\n",
    "        r10 = c_only_results[key][L][10]\n",
    "        line += f' {r10*100:>9.1f}%'\n",
    "    \n",
    "    # C+Pivot\n",
    "    for t in T_VALS:\n",
    "        for L in [1000, 2000]:\n",
    "            r10 = c_pivot_results[key][t][L]['recalls'][10]\n",
    "            line += f' {r10*100:>10.1f}%'\n",
    "    \n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. Recall@K Detail (K=1, 10, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T22:51:22.155721Z",
     "iopub.status.busy": "2026-02-08T22:51:22.155597Z",
     "iopub.status.idle": "2026-02-08T22:51:22.159741Z",
     "shell.execute_reply": "2026-02-08T22:51:22.159221Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Recall@K Detail (L=1000 for C-only/C+Pivot, t=25 for C+Pivot)\n",
      "====================================================================================================\n",
      "\n",
      "--- Recall@1 ---\n",
      "Dataset        HNSW   C-only  C+Pivot  Gap(HNSW-C)  Gap(C-CP)\n",
      "------------------------------------------------------------\n",
      "glove         84.8%    81.8%    81.1%       +3.0pp     +0.7pp\n",
      "sift          98.1%    91.5%    91.8%       +6.6pp     -0.3pp\n",
      "fashion       99.2%    99.6%    99.8%       -0.4pp     -0.2pp\n",
      "nytimes       82.3%    81.2%    80.8%       +1.1pp     +0.4pp\n",
      "gist          86.8%    72.9%    73.2%      +13.9pp     -0.3pp\n",
      "\n",
      "--- Recall@10 ---\n",
      "Dataset        HNSW   C-only  C+Pivot  Gap(HNSW-C)  Gap(C-CP)\n",
      "------------------------------------------------------------\n",
      "glove         79.9%    66.9%    66.7%      +12.9pp     +0.2pp\n",
      "sift          97.9%    85.0%    85.3%      +12.9pp     -0.4pp\n",
      "fashion       99.3%    99.1%    99.2%       +0.2pp     -0.0pp\n",
      "nytimes       84.7%    74.2%    73.6%      +10.5pp     +0.6pp\n",
      "gist          81.6%    59.4%    59.4%      +22.2pp     +0.0pp\n",
      "\n",
      "--- Recall@100 ---\n",
      "Dataset        HNSW   C-only  C+Pivot  Gap(HNSW-C)  Gap(C-CP)\n",
      "------------------------------------------------------------\n",
      "glove         67.6%    45.0%    44.3%      +22.6pp     +0.7pp\n",
      "sift          91.6%    68.1%    68.2%      +23.6pp     -0.2pp\n",
      "fashion       98.5%    96.4%    96.6%       +2.1pp     -0.2pp\n",
      "nytimes       69.3%    49.5%    48.8%      +19.8pp     +0.7pp\n",
      "gist          69.3%    40.3%    40.3%      +29.0pp     +0.0pp\n"
     ]
    }
   ],
   "source": [
    "print('='*100)\n",
    "print('Recall@K Detail (L=1000 for C-only/C+Pivot, t=25 for C+Pivot)')\n",
    "print('='*100)\n",
    "\n",
    "L_DETAIL = 1000\n",
    "T_DETAIL = 25\n",
    "\n",
    "for k_val in TOP_K_VALUES:\n",
    "    print(f'\\n--- Recall@{k_val} ---')\n",
    "    print(f'{\"Dataset\":<10} {\"HNSW\":>8} {\"C-only\":>8} {\"C+Pivot\":>8} {\"Gap(HNSW-C)\":>12} {\"Gap(C-CP)\":>10}')\n",
    "    print('-'*60)\n",
    "    \n",
    "    for key in available_keys:\n",
    "        # HNSW\n",
    "        if key in hnsw_results and hnsw_results[key]['recalls'].get(k_val) is not None:\n",
    "            hnsw_r = hnsw_results[key]['recalls'][k_val]\n",
    "            hnsw_str = f'{hnsw_r*100:.1f}%'\n",
    "        else:\n",
    "            hnsw_r = None\n",
    "            hnsw_str = 'N/A'\n",
    "        \n",
    "        c_r = c_only_results[key][L_DETAIL][k_val]\n",
    "        cp_r = c_pivot_results[key][T_DETAIL][L_DETAIL]['recalls'][k_val]\n",
    "        \n",
    "        gap_hnsw_c = f'{(hnsw_r - c_r)*100:+.1f}pp' if hnsw_r is not None else 'N/A'\n",
    "        gap_c_cp = f'{(c_r - cp_r)*100:+.1f}pp'\n",
    "        \n",
    "        print(f'{key:<10} {hnsw_str:>8} {c_r*100:>7.1f}% {cp_r*100:>7.1f}% {gap_hnsw_c:>12} {gap_c_cp:>10}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 4. Hash Quality vs Data Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T22:51:22.160962Z",
     "iopub.status.busy": "2026-02-08T22:51:22.160836Z",
     "iopub.status.idle": "2026-02-08T22:51:22.164466Z",
     "shell.execute_reply": "2026-02-08T22:51:22.163927Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Hash Quality: Hamming-Cosine Spearman Correlation\n",
      "================================================================================\n",
      "\n",
      "Dataset      Dim          N   Spearman    Quality\n",
      "--------------------------------------------------\n",
      "glove        100  1,183,514    -0.5083       Fair\n",
      "sift         128  1,000,000    -0.9252       Good\n",
      "fashion      784     60,000    -0.7446       Good\n",
      "nytimes      256    290,000    -0.5084       Fair\n",
      "gist         960  1,000,000    -0.4551       Fair\n",
      "\n",
      "Note: Negative Spearman = larger Hamming ~ lower cosine (expected, good)\n",
      "Stronger negative correlation -> better hash quality\n"
     ]
    }
   ],
   "source": [
    "print('='*80)\n",
    "print('Hash Quality: Hamming-Cosine Spearman Correlation')\n",
    "print('='*80)\n",
    "\n",
    "print(f'\\n{\"Dataset\":<10} {\"Dim\":>5} {\"N\":>10} {\"Spearman\":>10} {\"Quality\":>10}')\n",
    "print('-'*50)\n",
    "\n",
    "for key in available_keys:\n",
    "    info = dataset_info[key]\n",
    "    corr = quality_results[key]\n",
    "    \n",
    "    # Quality rating\n",
    "    if abs(corr) > 0.6:\n",
    "        quality = 'Good'\n",
    "    elif abs(corr) > 0.4:\n",
    "        quality = 'Fair'\n",
    "    elif abs(corr) > 0.2:\n",
    "        quality = 'Weak'\n",
    "    else:\n",
    "        quality = 'Poor'\n",
    "    \n",
    "    print(f'{key:<10} {info[\"dim\"]:>5} {info[\"n_train\"]:>10,} {corr:>10.4f} {quality:>10}')\n",
    "\n",
    "print('\\nNote: Negative Spearman = larger Hamming ~ lower cosine (expected, good)')\n",
    "print('Stronger negative correlation -> better hash quality')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 5. Pivot Pruning Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T22:51:22.165709Z",
     "iopub.status.busy": "2026-02-08T22:51:22.165593Z",
     "iopub.status.idle": "2026-02-08T22:51:22.169546Z",
     "shell.execute_reply": "2026-02-08T22:51:22.169088Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Pivot Pruning: Reduction Rate vs Filter Recall vs Final Recall@10 (L=1000)\n",
      "================================================================================\n",
      "\n",
      "glove (N=1,183,514, D=100):\n",
      "   Threshold  Reduction   FilterR@10  FinalR@10   AvgCands\n",
      "  --------------------------------------------------------\n",
      "  (no pivot)       0.0%       100.0%      66.9%  1,183,514\n",
      "          15      24.3%        95.7%      65.7%    896,148\n",
      "          20       4.7%        99.8%      67.0%  1,128,130\n",
      "          25       0.7%       100.0%      66.7%  1,175,447\n",
      "          30       0.1%       100.0%      67.1%  1,182,525\n",
      "\n",
      "sift (N=1,000,000, D=128):\n",
      "   Threshold  Reduction   FilterR@10  FinalR@10   AvgCands\n",
      "  --------------------------------------------------------\n",
      "  (no pivot)       0.0%       100.0%      85.0%  1,000,000\n",
      "          15      79.3%        98.8%      84.9%    207,124\n",
      "          20      66.8%        99.9%      85.3%    332,284\n",
      "          25      59.3%       100.0%      85.3%    407,290\n",
      "          30      55.1%       100.0%      85.4%    449,048\n",
      "\n",
      "fashion (N=60,000, D=784):\n",
      "   Threshold  Reduction   FilterR@10  FinalR@10   AvgCands\n",
      "  --------------------------------------------------------\n",
      "  (no pivot)       0.0%       100.0%      99.1%     60,000\n",
      "          15      89.3%        97.7%      97.3%      6,409\n",
      "          20      82.1%        99.5%      98.9%     10,755\n",
      "          25      74.3%        99.9%      99.2%     15,416\n",
      "          30      66.0%       100.0%      99.2%     20,381\n",
      "\n",
      "nytimes (N=290,000, D=256):\n",
      "   Threshold  Reduction   FilterR@10  FinalR@10   AvgCands\n",
      "  --------------------------------------------------------\n",
      "  (no pivot)       0.0%       100.0%      74.2%    290,000\n",
      "          15      35.0%        87.6%      68.3%    188,514\n",
      "          20       8.8%        98.4%      73.0%    264,502\n",
      "          25       1.8%        99.8%      73.6%    284,755\n",
      "          30       0.6%        99.9%      73.6%    288,392\n",
      "\n",
      "gist (N=1,000,000, D=960):\n",
      "   Threshold  Reduction   FilterR@10  FinalR@10   AvgCands\n",
      "  --------------------------------------------------------\n",
      "  (no pivot)       0.0%       100.0%      59.4%  1,000,000\n",
      "          15      80.4%        84.1%      56.5%    196,304\n",
      "          20      53.0%        97.9%      59.5%    469,526\n",
      "          25      27.9%        99.8%      59.4%    720,730\n",
      "          30      12.0%       100.0%      59.4%    879,613\n"
     ]
    }
   ],
   "source": [
    "print('='*80)\n",
    "print('Pivot Pruning: Reduction Rate vs Filter Recall vs Final Recall@10 (L=1000)')\n",
    "print('='*80)\n",
    "\n",
    "L_PIVOT = 1000\n",
    "\n",
    "for key in available_keys:\n",
    "    info = dataset_info[key]\n",
    "    print(f'\\n{key} (N={info[\"n_train\"]:,}, D={info[\"dim\"]}):')\n",
    "    print(f'  {\"Threshold\":>10} {\"Reduction\":>10} {\"FilterR@10\":>12} {\"FinalR@10\":>10} {\"AvgCands\":>10}')\n",
    "    print(f'  {\"-\"*56}')\n",
    "    \n",
    "    # No pivot (C-only)\n",
    "    c_r10 = c_only_results[key][L_PIVOT][10]\n",
    "    print(f'  {\"(no pivot)\":>10} {\"0.0%\":>10} {\"100.0%\":>12} {c_r10*100:>9.1f}% {info[\"n_train\"]:>10,}')\n",
    "    \n",
    "    for t in PIVOT_THRESHOLDS:\n",
    "        r = c_pivot_results[key][t][L_PIVOT]\n",
    "        print(f'  {t:>10} {r[\"reduction_rate\"]*100:>9.1f}% {r[\"filter_recall_10\"]*100:>11.1f}% '\n",
    "              f'{r[\"recalls\"][10]*100:>9.1f}% {r[\"mean_candidates\"]:>10,.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 6. Candidate Limit to Match HNSW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T22:51:22.170937Z",
     "iopub.status.busy": "2026-02-08T22:51:22.170822Z",
     "iopub.status.idle": "2026-02-08T22:51:22.174994Z",
     "shell.execute_reply": "2026-02-08T22:51:22.174508Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Candidate Limit Needed to Match HNSW Recall@10\n",
      "================================================================================\n",
      "\n",
      "glove (HNSW R@10=79.9%):\n",
      "  90% of HNSW (71.9%): C-only needs L=2000 (achieves 74.8%)\n",
      "  95% of HNSW (75.9%): C-only needs L=5000 (achieves 83.9%)\n",
      "  99% of HNSW (79.1%): C-only needs L=5000 (achieves 83.9%)\n",
      "\n",
      "sift (HNSW R@10=97.9%):\n",
      "  90% of HNSW (88.1%): C-only needs L=2000 (achieves 91.6%)\n",
      "  95% of HNSW (93.0%): C-only needs L=5000 (achieves 96.9%)\n",
      "  99% of HNSW (96.9%): C-only needs L>5000 (achieves 96.9%)\n",
      "\n",
      "fashion (HNSW R@10=99.3%):\n",
      "  90% of HNSW (89.3%): C-only needs L=500 (achieves 97.2%)\n",
      "  95% of HNSW (94.3%): C-only needs L=500 (achieves 97.2%)\n",
      "  99% of HNSW (98.3%): C-only needs L=1000 (achieves 99.1%)\n",
      "\n",
      "nytimes (HNSW R@10=84.7%):\n",
      "  90% of HNSW (76.2%): C-only needs L=2000 (achieves 79.4%)\n",
      "  95% of HNSW (80.5%): C-only needs L=5000 (achieves 85.8%)\n",
      "  99% of HNSW (83.8%): C-only needs L=5000 (achieves 85.8%)\n",
      "\n",
      "gist (HNSW R@10=81.6%):\n",
      "  90% of HNSW (73.5%): C-only needs L=5000 (achieves 83.0%)\n",
      "  95% of HNSW (77.5%): C-only needs L=5000 (achieves 83.0%)\n",
      "  99% of HNSW (80.8%): C-only needs L=5000 (achieves 83.0%)\n"
     ]
    }
   ],
   "source": [
    "print('='*80)\n",
    "print('Candidate Limit Needed to Match HNSW Recall@10')\n",
    "print('='*80)\n",
    "\n",
    "for key in available_keys:\n",
    "    info = dataset_info[key]\n",
    "    \n",
    "    if key not in hnsw_results or hnsw_results[key]['recalls'].get(10) is None:\n",
    "        print(f'\\n{key}: HNSW not available')\n",
    "        continue\n",
    "    \n",
    "    hnsw_r10 = hnsw_results[key]['recalls'][10]\n",
    "    \n",
    "    print(f'\\n{key} (HNSW R@10={hnsw_r10*100:.1f}%):')\n",
    "    \n",
    "    # C-only: find smallest L that achieves various fractions of HNSW R@10\n",
    "    targets = [0.90, 0.95, 0.99]\n",
    "    for target in targets:\n",
    "        target_r10 = hnsw_r10 * target\n",
    "        found_L = None\n",
    "        for L in CANDIDATE_LIMITS:\n",
    "            if c_only_results[key][L][10] >= target_r10:\n",
    "                found_L = L\n",
    "                break\n",
    "        \n",
    "        status = f'L={found_L}' if found_L else f'L>{CANDIDATE_LIMITS[-1]}'\n",
    "        actual = c_only_results[key][found_L][10] if found_L else c_only_results[key][CANDIDATE_LIMITS[-1]][10]\n",
    "        print(f'  {target*100:.0f}% of HNSW ({target_r10*100:.1f}%): C-only needs {status} '\n",
    "              f'(achieves {actual*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 7. Filter Recall Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T22:51:22.176212Z",
     "iopub.status.busy": "2026-02-08T22:51:22.176088Z",
     "iopub.status.idle": "2026-02-08T22:51:22.179565Z",
     "shell.execute_reply": "2026-02-08T22:51:22.179015Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Filter Recall: True top-10 in Hamming candidates (C-only, no pivot)\n",
      "================================================================================\n",
      "\n",
      "Dataset       L=100    L=500   L=1000   L=2000   L=5000\n",
      "------------------------------------------------------------\n",
      "glove         38.6%    58.5%    66.9%    74.8%    83.9%\n",
      "sift          50.0%    76.2%    85.0%    91.7%    97.0%\n",
      "fashion       79.7%    97.2%    99.1%    99.7%    99.9%\n",
      "nytimes       55.9%    69.8%    75.1%    80.4%    86.8%\n",
      "gist          26.5%    48.1%    59.5%    70.2%    83.0%\n"
     ]
    }
   ],
   "source": [
    "print('='*80)\n",
    "print('Filter Recall: True top-10 in Hamming candidates (C-only, no pivot)')\n",
    "print('='*80)\n",
    "\n",
    "print(f'\\n{\"Dataset\":<10}', end='')\n",
    "for L in CANDIDATE_LIMITS:\n",
    "    print(f' {\"L=\"+str(L):>8}', end='')\n",
    "print()\n",
    "print('-'*60)\n",
    "\n",
    "for key in available_keys:\n",
    "    print(f'{key:<10}', end='')\n",
    "    for L in CANDIDATE_LIMITS:\n",
    "        print(f' {filter_recalls[key][L]*100:>7.1f}%', end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 8. Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T22:51:22.180727Z",
     "iopub.status.busy": "2026-02-08T22:51:22.180610Z",
     "iopub.status.idle": "2026-02-08T22:51:22.186058Z",
     "shell.execute_reply": "2026-02-08T22:51:22.185640Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CONCLUSIONS\n",
      "================================================================================\n",
      "\n",
      "1. ITQ-LSH Hash Quality:\n",
      "   glove: Spearman=-0.5083\n",
      "   sift: Spearman=-0.9252\n",
      "   fashion: Spearman=-0.7446\n",
      "   nytimes: Spearman=-0.5084\n",
      "   gist: Spearman=-0.4551\n",
      "\n",
      "2. C-only Pipeline (ITQ -> Hamming -> Cosine rerank):\n",
      "   glove: R@10=66.9% (L=1000), 83.9% (L=5000)\n",
      "   sift: R@10=85.0% (L=1000), 96.9% (L=5000)\n",
      "   fashion: R@10=99.1% (L=1000), 99.9% (L=5000)\n",
      "   nytimes: R@10=74.2% (L=1000), 85.8% (L=5000)\n",
      "   gist: R@10=59.4% (L=1000), 83.0% (L=5000)\n",
      "\n",
      "3. C+Pivot Pipeline:\n",
      "   glove: Best t=15 -> R@10=65.7%, reduction=24.3%\n",
      "   sift: Best t=15 -> R@10=84.9%, reduction=79.3%\n",
      "   fashion: Best t=15 -> R@10=97.3%, reduction=89.3%\n",
      "   nytimes: Best t=15 -> R@10=68.3%, reduction=35.0%\n",
      "   gist: Best t=15 -> R@10=56.5%, reduction=80.4%\n",
      "\n",
      "4. HNSW Comparison:\n",
      "   glove: HNSW=79.9% vs C-only(L=1000)=66.9% (gap=+12.9pp)\n",
      "   sift: HNSW=97.9% vs C-only(L=1000)=85.0% (gap=+12.9pp)\n",
      "   fashion: HNSW=99.3% vs C-only(L=1000)=99.1% (gap=+0.2pp)\n",
      "   nytimes: HNSW=84.7% vs C-only(L=1000)=74.2% (gap=+10.5pp)\n",
      "   gist: HNSW=81.6% vs C-only(L=1000)=59.4% (gap=+22.2pp)\n",
      "\n",
      "5. Recommendations:\n",
      "   - ITQ-LSH 128-bit provides a reasonable hash quality baseline across diverse datasets\n",
      "   - C-only pipeline at L=1000-2000 balances candidate reduction with Recall\n",
      "   - Pivot pruning (t=20-25) can reduce candidates with controllable Recall loss\n",
      "   - For high-Recall requirements (>95%), larger candidate limits or HNSW preferred\n",
      "   - Dataset characteristics (dim, data distribution) significantly affect hash quality\n"
     ]
    }
   ],
   "source": [
    "print('='*80)\n",
    "print('CONCLUSIONS')\n",
    "print('='*80)\n",
    "\n",
    "print('\\n1. ITQ-LSH Hash Quality:')\n",
    "for key in available_keys:\n",
    "    corr = quality_results[key]\n",
    "    print(f'   {key}: Spearman={corr:.4f}')\n",
    "\n",
    "print('\\n2. C-only Pipeline (ITQ -> Hamming -> Cosine rerank):')\n",
    "for key in available_keys:\n",
    "    r10_1k = c_only_results[key][1000][10]\n",
    "    r10_5k = c_only_results[key][5000][10]\n",
    "    print(f'   {key}: R@10={r10_1k*100:.1f}% (L=1000), {r10_5k*100:.1f}% (L=5000)')\n",
    "\n",
    "print('\\n3. C+Pivot Pipeline:')\n",
    "for key in available_keys:\n",
    "    # Best threshold maintaining >90% of C-only R@10\n",
    "    c_r10 = c_only_results[key][1000][10]\n",
    "    best_t = None\n",
    "    for t in sorted(PIVOT_THRESHOLDS):\n",
    "        cp_r10 = c_pivot_results[key][t][1000]['recalls'][10]\n",
    "        red = c_pivot_results[key][t][1000]['reduction_rate']\n",
    "        if cp_r10 >= c_r10 * 0.90:\n",
    "            best_t = t\n",
    "            break\n",
    "    if best_t:\n",
    "        r = c_pivot_results[key][best_t][1000]\n",
    "        print(f'   {key}: Best t={best_t} -> R@10={r[\"recalls\"][10]*100:.1f}%, '\n",
    "              f'reduction={r[\"reduction_rate\"]*100:.1f}%')\n",
    "    else:\n",
    "        r = c_pivot_results[key][PIVOT_THRESHOLDS[-1]][1000]\n",
    "        print(f'   {key}: Even t={PIVOT_THRESHOLDS[-1]} -> R@10={r[\"recalls\"][10]*100:.1f}%, '\n",
    "              f'reduction={r[\"reduction_rate\"]*100:.1f}%')\n",
    "\n",
    "print('\\n4. HNSW Comparison:')\n",
    "for key in available_keys:\n",
    "    if key in hnsw_results and hnsw_results[key]['recalls'].get(10) is not None:\n",
    "        hnsw_r10 = hnsw_results[key]['recalls'][10]\n",
    "        c_r10 = c_only_results[key][1000][10]\n",
    "        gap = (hnsw_r10 - c_r10) * 100\n",
    "        print(f'   {key}: HNSW={hnsw_r10*100:.1f}% vs C-only(L=1000)={c_r10*100:.1f}% (gap={gap:+.1f}pp)')\n",
    "    else:\n",
    "        print(f'   {key}: HNSW not available')\n",
    "\n",
    "print('\\n5. Recommendations:')\n",
    "print('   - ITQ-LSH 128-bit provides a reasonable hash quality baseline across diverse datasets')\n",
    "print('   - C-only pipeline at L=1000-2000 balances candidate reduction with Recall')\n",
    "print('   - Pivot pruning (t=20-25) can reduce candidates with controllable Recall loss')\n",
    "print('   - For high-Recall requirements (>95%), larger candidate limits or HNSW preferred')\n",
    "print('   - Dataset characteristics (dim, data distribution) significantly affect hash quality')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zkh8h5h3ewl",
   "source": "## 9. 総合評価\n\n### ITQ-LSH + Pivotパイプラインの客観的位置づけ\n\n5つの標準ベンチマークデータセットでの検証により、ITQ-LSH + Pivot パイプラインの**適用条件と限界**が明確になった。\n\n#### データセット特性との対応関係\n\n| データセット | ハッシュ品質 | C-only (L=1000) | HNSW | Gap | Pivot効果 | 総合判定 |\n|---|---|---|---|---|---|---|\n| **Fashion** | Good (-0.74) | 99.1% | 99.3% | 0.2pp | ◎ 82%削減 | **実用的** |\n| **SIFT** | Good (-0.93) | 85.0% | 97.9% | 12.9pp | ◎ 67%削減 | **条件付き実用的** |\n| **NYTimes** | Fair (-0.51) | 74.2% | 84.7% | 10.5pp | × 9%削減 | **限定的** |\n| **GloVe** | Fair (-0.51) | 66.9% | 79.9% | 12.9pp | × 5%削減 | **限定的** |\n| **GIST** | Fair (-0.46) | 59.4% | 81.6% | 22.2pp | ○ 53%削減 | **不適** |\n\n#### ITQ-LSHが有効に機能する条件\n\n1. **ハッシュ品質（Spearman）が-0.7以上**: Fashion、SIFTでは良好な相関を示し、Hamming距離がcosine類似度の良い近似となる\n2. **ビット数が十分確保できる（n_bits ≥ 128）**: GloVeはdim=100のため100bitに制限され、ハッシュ解像度が不足\n3. **データが比較的低次元（< 256D）**: 高次元→128bitの情報圧縮損失が大きいほど精度が劣化（GIST: 960D→128bit）\n\n#### Pivot枝刈りが有効に機能する条件\n\n1. **ハッシュ空間でのピボット距離分散が大きい（std > 10）**: SIFT (16.1), Fashion (15.3)で大きな削減率\n2. **ピボット間距離が十分に大きい**: SIFT (mean=68.5), GIST (mean=70.5)で良好な分離\n3. **低分散データでは無効**: GloVe (std=5.2), NYTimes (std=5.7)ではほぼ枝刈りできない\n\n#### HNSW比較からの学び\n\n- **L=1000固定時**: HNSWに対して0.2〜22.2ppの精度差。Fashionのみ同等\n- **L=5000時**: GloVe (83.9% vs 79.9%) とGIST (83.0% vs 81.6%) ではHNSWを**超過**\n- **SIFTでL=5000**: 96.9%でHNSWの97.9%にほぼ到達\n- → **候補数を十分に取れば、ITQ-LSHはHNSWと同等以上の精度を達成可能**。ただし速度面の考慮が必要\n\n#### 既存実験（Wikipedia 10K/400K）との整合性\n\n- 実験84（Wikipedia英語10K）でのR@10=84.2%（Pivot, t=20）は、SIFTのR@10=85.3%と整合的\n- E5-base-v2（768D→128bit）の精度は、GIST（960D→128bit）と同様の高次元圧縮問題を抱えていると推測される\n- 小規模データ（10K-60K）ではFashionと同様にITQ-LSHが高精度を達成しやすい\n\n### 実用上の推奨\n\n| 利用シナリオ | 推奨設定 | 期待Recall@10 |\n|---|---|---|\n| 小規模 (< 100K)、低-中次元 | C-only, L=1000 | > 95% |\n| 中規模 (100K-1M)、低次元 (< 256D) | C+Pivot(t=20), L=2000 | 85-92% |\n| 中規模、高次元 (> 256D) | C-only, L=5000 or HNSW推奨 | 70-83% |\n| 大規模 (> 1M) | HNSW推奨（ITQ-LSHは候補数増でコスト増大） | - |\n\n### 残課題\n\n1. **ビット数の拡張**: 128bitでは高次元データ（GIST, GloVe）で不十分。256bit/512bitでの評価が必要\n2. **速度ベンチマーク**: 本実験では精度のみ評価。Hamming全探索の速度 vs HNSW速度の比較が未実施\n3. **Multi-probe LSH**: Band filterやConfidence multi-probeの再検討（実験80-84で課題が判明済み）\n4. **Pivot選択の改善**: 低分散データ向けのピボット選択戦略（例：embedding空間での選択→hash空間へのマッピング）",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}