{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 34. 外部クエリ対応のLSHフィルタリング代替手法\n",
    "\n",
    "## 背景\n",
    "\n",
    "実験33で検証したOverlap(8,4)カスケード方式は、内部クエリではR@10=90%を達成したが、\n",
    "外部クエリ（キーワード検索）ではR@10=62%と低い再現率に留まった。\n",
    "\n",
    "## 根本原因\n",
    "\n",
    "Overlapセグメント方式は**8bit完全一致**を要求する。\n",
    "- 内部クエリ: 誤りが特定セグメントに集中 → 他のセグメントで完全一致可能\n",
    "- 外部クエリ: 誤りが全セグメントに1-2bit散在 → どのセグメントも完全一致しない\n",
    "\n",
    "## 目標\n",
    "\n",
    "- **外部クエリR@10 >= 80%**（現状62%から改善）\n",
    "- 候補数 < 50,000件（全件スキャン回避）\n",
    "- 処理時間 < 50ms\n",
    "\n",
    "## 検証する代替手法\n",
    "\n",
    "1. ファジーセグメントマッチング（部分一致許容）\n",
    "2. **ランダムビットサンプリング**（最優先）\n",
    "3. チャンク比較（SimHash風）\n",
    "4. 適応的閾値（Progressive Search）\n",
    "5. ビット重要度加重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from src.itq_lsh import ITQLSH, hamming_distance_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パス設定\n",
    "DB_PATH = '../data/experiment_400k.duckdb'\n",
    "ITQ_MODEL_PATH = '../data/itq_model.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. データ読み込みとベースライン設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データ読み込み中...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a1773ec5fec48e38b42b5a4625c5529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "読み込み完了: 400,000件\n",
      "埋め込み shape: (400000, 1024)\n"
     ]
    }
   ],
   "source": [
    "# データ読み込み\n",
    "print('データ読み込み中...')\n",
    "conn = duckdb.connect(DB_PATH, read_only=True)\n",
    "\n",
    "result = conn.execute(\"\"\"\n",
    "    SELECT id, embedding\n",
    "    FROM documents\n",
    "    ORDER BY id\n",
    "\"\"\").fetchall()\n",
    "conn.close()\n",
    "\n",
    "doc_ids = np.array([r[0] for r in result])\n",
    "embeddings = np.array([r[1] for r in result], dtype=np.float32)\n",
    "\n",
    "n_docs = len(doc_ids)\n",
    "print(f'読み込み完了: {n_docs:,}件')\n",
    "print(f'埋め込み shape: {embeddings.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITQモデル読み込み中...\n",
      "ITQハッシュ計算中...\n",
      "ハッシュ shape: (400000, 128)\n"
     ]
    }
   ],
   "source": [
    "# ITQモデル読み込みとハッシュ計算\n",
    "print('ITQモデル読み込み中...')\n",
    "itq = ITQLSH.load(ITQ_MODEL_PATH)\n",
    "\n",
    "print('ITQハッシュ計算中...')\n",
    "hashes = itq.transform(embeddings)\n",
    "print(f'ハッシュ shape: {hashes.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. 内部/外部クエリの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 共通ユーティリティ関数\n",
    "def compute_ground_truth(query_embedding, all_embeddings, top_k=10):\n",
    "    \"\"\"ブルートフォースでGround Truthを計算\"\"\"\n",
    "    query_norm = norm(query_embedding)\n",
    "    all_norms = norm(all_embeddings, axis=1)\n",
    "    cosines = (all_embeddings @ query_embedding) / (all_norms * query_norm + 1e-10)\n",
    "    top_indices = np.argsort(cosines)[-top_k:][::-1]\n",
    "    return top_indices\n",
    "\n",
    "\n",
    "def evaluate_recall(predicted, ground_truth, k=10):\n",
    "    \"\"\"Recall@k を計算\"\"\"\n",
    "    gt_set = set(ground_truth[:k])\n",
    "    pred_set = set(predicted[:k]) if len(predicted) >= k else set(predicted)\n",
    "    return len(gt_set & pred_set) / k\n",
    "\n",
    "\n",
    "def bits_to_int(bits_array):\n",
    "    \"\"\"バイナリ配列を整数に変換\"\"\"\n",
    "    if bits_array.ndim == 1:\n",
    "        bits_array = bits_array.reshape(1, -1)\n",
    "    n_bits = bits_array.shape[1]\n",
    "    powers = 2 ** np.arange(n_bits - 1, -1, -1)\n",
    "    return np.sum(bits_array * powers, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "内部クエリ数: 100\n",
      "内部クエリのGround Truth計算中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GT計算: 100%|██████████| 100/100 [00:33<00:00,  3.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完了\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 内部クエリの準備（ドキュメント自身をクエリ）\n",
    "rng = np.random.default_rng(42)\n",
    "n_internal_queries = 100\n",
    "internal_query_indices = rng.choice(n_docs, n_internal_queries, replace=False)\n",
    "\n",
    "print(f'内部クエリ数: {n_internal_queries}')\n",
    "\n",
    "# Ground Truth計算\n",
    "print('内部クエリのGround Truth計算中...')\n",
    "internal_ground_truths = {}\n",
    "for qi in tqdm(internal_query_indices, desc='GT計算'):\n",
    "    internal_ground_truths[qi] = compute_ground_truth(embeddings[qi], embeddings, top_k=10)\n",
    "print('完了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E5モデル読み込み中...\n",
      "完了\n"
     ]
    }
   ],
   "source": [
    "# 外部クエリの準備（E5モデルでキーワードを埋め込み）\n",
    "print('E5モデル読み込み中...')\n",
    "model = SentenceTransformer('intfloat/multilingual-e5-large')\n",
    "print('完了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "外部クエリ数: 30\n"
     ]
    }
   ],
   "source": [
    "# 外部クエリ（幅広いトピックのキーワード）\n",
    "external_queries = [\n",
    "    # 日本語\n",
    "    '人工知能',\n",
    "    '機械学習',\n",
    "    '医療',\n",
    "    '環境問題',\n",
    "    '再生医療',\n",
    "    'ロボット',\n",
    "    '量子コンピュータ',\n",
    "    '脳科学',\n",
    "    'がん治療',\n",
    "    '気候変動',\n",
    "    '教育',\n",
    "    '金融',\n",
    "    'エネルギー',\n",
    "    '農業',\n",
    "    '宇宙開発',\n",
    "    # 英語\n",
    "    'artificial intelligence',\n",
    "    'machine learning',\n",
    "    'medical research',\n",
    "    'environmental science',\n",
    "    'regenerative medicine',\n",
    "    'robotics',\n",
    "    'quantum computing',\n",
    "    'neuroscience',\n",
    "    'cancer treatment',\n",
    "    'climate change',\n",
    "    'education technology',\n",
    "    'financial analysis',\n",
    "    'renewable energy',\n",
    "    'sustainable agriculture',\n",
    "    'space exploration',\n",
    "]\n",
    "\n",
    "print(f'外部クエリ数: {len(external_queries)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "外部クエリ埋め込み生成中...\n",
      "外部クエリのGround Truth計算中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GT計算: 100%|██████████| 30/30 [00:09<00:00,  3.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完了\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 外部クエリの埋め込みとITQハッシュ\n",
    "print('外部クエリ埋め込み生成中...')\n",
    "external_query_embs = model.encode(\n",
    "    [f'passage: {q}' for q in external_queries],\n",
    "    normalize_embeddings=False\n",
    ").astype(np.float32)\n",
    "\n",
    "external_query_hashes = itq.transform(external_query_embs)\n",
    "\n",
    "# Ground Truth計算\n",
    "print('外部クエリのGround Truth計算中...')\n",
    "external_ground_truths = []\n",
    "for q_emb in tqdm(external_query_embs, desc='GT計算'):\n",
    "    gt = compute_ground_truth(q_emb, embeddings, top_k=10)\n",
    "    external_ground_truths.append(gt)\n",
    "print('完了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ハミング距離分布の分析\n",
      "================================================================================\n",
      "\n",
      "■ 内部クエリ（GT Top-10へのハミング距離）\n",
      "  平均: 23.4bit\n",
      "  最小: 5bit\n",
      "  最大: 47bit\n",
      "\n",
      "■ 外部クエリ（GT Top-10へのハミング距離）\n",
      "  平均: 19.0bit\n",
      "  最小: 0bit\n",
      "  最大: 41bit\n"
     ]
    }
   ],
   "source": [
    "# 内部/外部クエリの誤り分布分析\n",
    "print('=' * 80)\n",
    "print('ハミング距離分布の分析')\n",
    "print('=' * 80)\n",
    "\n",
    "# 内部クエリ: GT Top-10までのハミング距離\n",
    "internal_hamming_dists = []\n",
    "for qi in internal_query_indices[:20]:  # サンプル20件\n",
    "    gt = internal_ground_truths[qi]\n",
    "    for gt_idx in gt[:10]:\n",
    "        if gt_idx != qi:\n",
    "            dist = hamming_distance_batch(hashes[qi], hashes[gt_idx:gt_idx+1])[0]\n",
    "            internal_hamming_dists.append(dist)\n",
    "\n",
    "# 外部クエリ: GT Top-10までのハミング距離\n",
    "external_hamming_dists = []\n",
    "for i, q_hash in enumerate(external_query_hashes[:20]):  # サンプル20件\n",
    "    gt = external_ground_truths[i]\n",
    "    for gt_idx in gt[:10]:\n",
    "        dist = hamming_distance_batch(q_hash, hashes[gt_idx:gt_idx+1])[0]\n",
    "        external_hamming_dists.append(dist)\n",
    "\n",
    "print(f'\\n■ 内部クエリ（GT Top-10へのハミング距離）')\n",
    "print(f'  平均: {np.mean(internal_hamming_dists):.1f}bit')\n",
    "print(f'  最小: {np.min(internal_hamming_dists)}bit')\n",
    "print(f'  最大: {np.max(internal_hamming_dists)}bit')\n",
    "\n",
    "print(f'\\n■ 外部クエリ（GT Top-10へのハミング距離）')\n",
    "print(f'  平均: {np.mean(external_hamming_dists):.1f}bit')\n",
    "print(f'  最小: {np.min(external_hamming_dists)}bit')\n",
    "print(f'  最大: {np.max(external_hamming_dists)}bit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. 手法1: ファジーセグメントマッチング\n",
    "\n",
    "完全一致ではなく、部分一致（例: 8bit中6bit一致）を許容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_to_overlap_segments(hash_array, segment_width=8, stride=4):\n",
    "    \"\"\"\n",
    "    オーバーラップセグメントを生成\n",
    "    \"\"\"\n",
    "    n_docs, n_bits = hash_array.shape\n",
    "    n_segments = (n_bits - segment_width) // stride + 1\n",
    "    \n",
    "    segments = []\n",
    "    for i in range(n_segments):\n",
    "        start = i * stride\n",
    "        end = start + segment_width\n",
    "        segment_bits = hash_array[:, start:end]\n",
    "        \n",
    "        # ビットを整数に変換\n",
    "        powers = 2 ** np.arange(segment_width - 1, -1, -1)\n",
    "        segment_int = np.sum(segment_bits * powers, axis=1)\n",
    "        segments.append(segment_int)\n",
    "    \n",
    "    return np.column_stack(segments), n_segments\n",
    "\n",
    "\n",
    "def build_overlap_index(segments):\n",
    "    \"\"\"\n",
    "    オーバーラップセグメントのインデックスを構築\n",
    "    \"\"\"\n",
    "    n_docs, n_segments = segments.shape\n",
    "    index = {i: defaultdict(list) for i in range(n_segments)}\n",
    "    \n",
    "    for doc_idx in range(n_docs):\n",
    "        for seg_idx in range(n_segments):\n",
    "            seg_value = int(segments[doc_idx, seg_idx])\n",
    "            index[seg_idx][seg_value].append(doc_idx)\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fuzzy_neighbors(segment_value, segment_width, max_flips):\n",
    "    \"\"\"\n",
    "    指定されたビット数以内の誤りを許容する隣接セグメント値を生成\n",
    "    \"\"\"\n",
    "    neighbors = []\n",
    "    for n_flips in range(max_flips + 1):\n",
    "        for flip_positions in combinations(range(segment_width), n_flips):\n",
    "            neighbor = segment_value\n",
    "            for pos in flip_positions:\n",
    "                neighbor ^= (1 << pos)\n",
    "            neighbors.append(neighbor)\n",
    "    return neighbors\n",
    "\n",
    "\n",
    "def fuzzy_segment_search(query_segments, segment_index, n_segments, segment_width=8, max_flips=2):\n",
    "    \"\"\"\n",
    "    ファジーセグメントマッチング\n",
    "    \"\"\"\n",
    "    candidates = set()\n",
    "    \n",
    "    for seg_idx in range(n_segments):\n",
    "        query_seg = int(query_segments[seg_idx])\n",
    "        # 近傍セグメント値を生成してルックアップ\n",
    "        for neighbor_seg in generate_fuzzy_neighbors(query_seg, segment_width, max_flips):\n",
    "            if neighbor_seg in segment_index[seg_idx]:\n",
    "                candidates.update(segment_index[seg_idx][neighbor_seg])\n",
    "    \n",
    "    return np.array(list(candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap(8,4)インデックス構築中...\n",
      "  セグメント数: 31\n",
      "  セグメント幅: 8bit\n"
     ]
    }
   ],
   "source": [
    "# Overlap(8,4)インデックス構築\n",
    "print('Overlap(8,4)インデックス構築中...')\n",
    "SEGMENT_WIDTH = 8\n",
    "STRIDE = 4\n",
    "\n",
    "segments, n_segments = hash_to_overlap_segments(hashes, SEGMENT_WIDTH, STRIDE)\n",
    "segment_index = build_overlap_index(segments)\n",
    "\n",
    "print(f'  セグメント数: {n_segments}')\n",
    "print(f'  セグメント幅: {SEGMENT_WIDTH}bit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ファジーセグメントマッチング評価\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "設定: 100%|██████████| 3/3 [00:29<00:00,  9.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "評価完了\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ファジーセグメントマッチングの評価\n",
    "print('=' * 80)\n",
    "print('ファジーセグメントマッチング評価')\n",
    "print('=' * 80)\n",
    "\n",
    "fuzzy_configs = [\n",
    "    (0, 'Exact(8/8)'),      # ベースライン（完全一致）\n",
    "    (1, 'Fuzzy(7/8)'),      # 1bit誤り許容\n",
    "    (2, 'Fuzzy(6/8)'),      # 2bit誤り許容\n",
    "]\n",
    "\n",
    "fuzzy_results = []\n",
    "\n",
    "for max_flips, label in tqdm(fuzzy_configs, desc='設定'):\n",
    "    # 内部クエリ評価\n",
    "    internal_recalls = []\n",
    "    internal_candidates = []\n",
    "    internal_times = []\n",
    "    \n",
    "    for qi in internal_query_indices:\n",
    "        t0 = time.time()\n",
    "        step1_candidates = fuzzy_segment_search(\n",
    "            segments[qi], segment_index, n_segments, SEGMENT_WIDTH, max_flips\n",
    "        )\n",
    "        \n",
    "        # Step 2: ハミング距離ソート\n",
    "        if len(step1_candidates) > 2000:\n",
    "            dists = hamming_distance_batch(hashes[qi], hashes[step1_candidates])\n",
    "            top_idx = np.argsort(dists)[:2000]\n",
    "            step2_candidates = step1_candidates[top_idx]\n",
    "        else:\n",
    "            step2_candidates = step1_candidates\n",
    "        \n",
    "        # Step 3: コサイン類似度\n",
    "        if len(step2_candidates) > 0:\n",
    "            candidate_embs = embeddings[step2_candidates]\n",
    "            query_norm = norm(embeddings[qi])\n",
    "            candidate_norms = norm(candidate_embs, axis=1)\n",
    "            cosines = (candidate_embs @ embeddings[qi]) / (candidate_norms * query_norm + 1e-10)\n",
    "            top_k_idx = np.argsort(cosines)[-10:][::-1]\n",
    "            top_k_indices = step2_candidates[top_k_idx]\n",
    "        else:\n",
    "            top_k_indices = np.array([])\n",
    "        \n",
    "        elapsed = (time.time() - t0) * 1000\n",
    "        recall = evaluate_recall(top_k_indices, internal_ground_truths[qi], k=10)\n",
    "        \n",
    "        internal_recalls.append(recall)\n",
    "        internal_candidates.append(len(step1_candidates))\n",
    "        internal_times.append(elapsed)\n",
    "    \n",
    "    # 外部クエリ評価\n",
    "    external_recalls = []\n",
    "    external_candidates = []\n",
    "    external_times = []\n",
    "    \n",
    "    for i, (q_emb, q_hash) in enumerate(zip(external_query_embs, external_query_hashes)):\n",
    "        q_segments, _ = hash_to_overlap_segments(q_hash.reshape(1, -1), SEGMENT_WIDTH, STRIDE)\n",
    "        \n",
    "        t0 = time.time()\n",
    "        step1_candidates = fuzzy_segment_search(\n",
    "            q_segments[0], segment_index, n_segments, SEGMENT_WIDTH, max_flips\n",
    "        )\n",
    "        \n",
    "        # Step 2: ハミング距離ソート\n",
    "        if len(step1_candidates) > 2000:\n",
    "            dists = hamming_distance_batch(q_hash, hashes[step1_candidates])\n",
    "            top_idx = np.argsort(dists)[:2000]\n",
    "            step2_candidates = step1_candidates[top_idx]\n",
    "        else:\n",
    "            step2_candidates = step1_candidates\n",
    "        \n",
    "        # Step 3: コサイン類似度\n",
    "        if len(step2_candidates) > 0:\n",
    "            candidate_embs = embeddings[step2_candidates]\n",
    "            query_norm = norm(q_emb)\n",
    "            candidate_norms = norm(candidate_embs, axis=1)\n",
    "            cosines = (candidate_embs @ q_emb) / (candidate_norms * query_norm + 1e-10)\n",
    "            top_k_idx = np.argsort(cosines)[-10:][::-1]\n",
    "            top_k_indices = step2_candidates[top_k_idx]\n",
    "        else:\n",
    "            top_k_indices = np.array([])\n",
    "        \n",
    "        elapsed = (time.time() - t0) * 1000\n",
    "        recall = evaluate_recall(top_k_indices, external_ground_truths[i], k=10)\n",
    "        \n",
    "        external_recalls.append(recall)\n",
    "        external_candidates.append(len(step1_candidates))\n",
    "        external_times.append(elapsed)\n",
    "    \n",
    "    fuzzy_results.append({\n",
    "        'method': label,\n",
    "        'max_flips': max_flips,\n",
    "        'internal_recall@10': np.mean(internal_recalls),\n",
    "        'internal_avg_candidates': np.mean(internal_candidates),\n",
    "        'internal_avg_time_ms': np.mean(internal_times),\n",
    "        'external_recall@10': np.mean(external_recalls),\n",
    "        'external_avg_candidates': np.mean(external_candidates),\n",
    "        'external_avg_time_ms': np.mean(external_times),\n",
    "    })\n",
    "\n",
    "df_fuzzy = pd.DataFrame(fuzzy_results)\n",
    "print('\\n評価完了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "ファジーセグメントマッチング結果\n",
      "====================================================================================================\n",
      "\n",
      "         Method | Internal R@10 |   Int Cand | External R@10 |   Ext Cand |    Gap\n",
      "----------------------------------------------------------------------------------------------------\n",
      "     Exact(8/8) |        90.0% |      63764 |        95.0% |      70924 |  -5.0pt\n",
      "     Fuzzy(7/8) |        91.1% |     224440 |        96.0% |     233283 |  -4.9pt\n",
      "     Fuzzy(6/8) |        91.3% |     367798 |        96.0% |     372298 |  -4.7pt\n"
     ]
    }
   ],
   "source": [
    "# ファジーセグメント結果表示\n",
    "print('\\n' + '=' * 100)\n",
    "print('ファジーセグメントマッチング結果')\n",
    "print('=' * 100)\n",
    "\n",
    "print(f'\\n{\"Method\":>15} | {\"Internal R@10\":>12} | {\"Int Cand\":>10} | {\"External R@10\":>12} | {\"Ext Cand\":>10} | {\"Gap\":>6}')\n",
    "print('-' * 100)\n",
    "\n",
    "for _, row in df_fuzzy.iterrows():\n",
    "    gap = row['internal_recall@10'] - row['external_recall@10']\n",
    "    print(f'{row[\"method\"]:>15} | {row[\"internal_recall@10\"]*100:>11.1f}% | '\n",
    "          f'{row[\"internal_avg_candidates\"]:>10.0f} | {row[\"external_recall@10\"]*100:>11.1f}% | '\n",
    "          f'{row[\"external_avg_candidates\"]:>10.0f} | {gap*100:>5.1f}pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. 手法2: ランダムビットサンプリング（最優先）\n",
    "\n",
    "連続セグメントではなく、ランダムなビット位置をサンプリング。\n",
    "誤り分布に依存しないため、外部クエリにも有効。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomBitSamplingIndex:\n",
    "    \"\"\"\n",
    "    ランダムビットサンプリングによるLSHインデックス\n",
    "    \n",
    "    複数のテーブルを使用し、各テーブルではランダムに選んだビット位置で\n",
    "    インデックスを構築する。誤りの分布に依存しない。\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_bits_sample=16, n_tables=20, seed=42):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_bits_sample: 各テーブルでサンプリングするビット数\n",
    "            n_tables: テーブル数（冗長性）\n",
    "            seed: 乱数シード\n",
    "        \"\"\"\n",
    "        self.n_bits_sample = n_bits_sample\n",
    "        self.n_tables = n_tables\n",
    "        self.seed = seed\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        \n",
    "        # 各テーブルのビット位置をランダムに選択\n",
    "        self.bit_positions = [\n",
    "            sorted(self.rng.choice(128, n_bits_sample, replace=False))\n",
    "            for _ in range(n_tables)\n",
    "        ]\n",
    "        self.tables = None\n",
    "    \n",
    "    def build(self, hashes):\n",
    "        \"\"\"\n",
    "        インデックスを構築\n",
    "        \"\"\"\n",
    "        n_docs = len(hashes)\n",
    "        self.tables = []\n",
    "        \n",
    "        for table_idx in range(self.n_tables):\n",
    "            positions = self.bit_positions[table_idx]\n",
    "            # 指定位置のビットを抽出\n",
    "            sampled_bits = hashes[:, positions]\n",
    "            # 整数キーに変換\n",
    "            keys = bits_to_int(sampled_bits)\n",
    "            \n",
    "            # ハッシュテーブル構築\n",
    "            table = defaultdict(list)\n",
    "            for doc_idx, key in enumerate(keys):\n",
    "                table[int(key)].append(doc_idx)\n",
    "            self.tables.append(table)\n",
    "    \n",
    "    def query(self, query_hash):\n",
    "        \"\"\"\n",
    "        クエリを実行し、候補を返す\n",
    "        \"\"\"\n",
    "        candidates = set()\n",
    "        \n",
    "        for table_idx in range(self.n_tables):\n",
    "            positions = self.bit_positions[table_idx]\n",
    "            # クエリのビットを抽出\n",
    "            query_bits = query_hash[positions]\n",
    "            query_key = int(bits_to_int(query_bits.reshape(1, -1))[0])\n",
    "            \n",
    "            # テーブルルックアップ\n",
    "            if query_key in self.tables[table_idx]:\n",
    "                candidates.update(self.tables[table_idx][query_key])\n",
    "        \n",
    "        return np.array(list(candidates))\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"インデックス統計を返す\"\"\"\n",
    "        total_entries = sum(len(t) for t in self.tables)\n",
    "        avg_bucket_size = np.mean([\n",
    "            len(docs) for t in self.tables for docs in t.values()\n",
    "        ])\n",
    "        return {\n",
    "            'n_tables': self.n_tables,\n",
    "            'n_bits_sample': self.n_bits_sample,\n",
    "            'total_buckets': total_entries,\n",
    "            'avg_bucket_size': avg_bucket_size,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ランダムビットサンプリング評価\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "設定: 100%|██████████| 8/8 [00:36<00:00,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "評価完了\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ランダムビットサンプリングの評価\n",
    "print('=' * 80)\n",
    "print('ランダムビットサンプリング評価')\n",
    "print('=' * 80)\n",
    "\n",
    "random_bit_configs = [\n",
    "    (12, 10, 'RBS(12bit, 10tables)'),\n",
    "    (12, 20, 'RBS(12bit, 20tables)'),\n",
    "    (12, 30, 'RBS(12bit, 30tables)'),\n",
    "    (16, 10, 'RBS(16bit, 10tables)'),\n",
    "    (16, 20, 'RBS(16bit, 20tables)'),\n",
    "    (16, 30, 'RBS(16bit, 30tables)'),\n",
    "    (20, 20, 'RBS(20bit, 20tables)'),\n",
    "    (20, 30, 'RBS(20bit, 30tables)'),\n",
    "]\n",
    "\n",
    "rbs_results = []\n",
    "\n",
    "for n_bits, n_tables, label in tqdm(random_bit_configs, desc='設定'):\n",
    "    # インデックス構築\n",
    "    rbs_index = RandomBitSamplingIndex(n_bits_sample=n_bits, n_tables=n_tables, seed=42)\n",
    "    rbs_index.build(hashes)\n",
    "    stats = rbs_index.get_stats()\n",
    "    \n",
    "    # 内部クエリ評価\n",
    "    internal_recalls = []\n",
    "    internal_candidates = []\n",
    "    internal_times = []\n",
    "    \n",
    "    for qi in internal_query_indices:\n",
    "        t0 = time.time()\n",
    "        step1_candidates = rbs_index.query(hashes[qi])\n",
    "        \n",
    "        # Step 2: ハミング距離ソート\n",
    "        if len(step1_candidates) > 2000:\n",
    "            dists = hamming_distance_batch(hashes[qi], hashes[step1_candidates])\n",
    "            top_idx = np.argsort(dists)[:2000]\n",
    "            step2_candidates = step1_candidates[top_idx]\n",
    "        else:\n",
    "            step2_candidates = step1_candidates\n",
    "        \n",
    "        # Step 3: コサイン類似度\n",
    "        if len(step2_candidates) > 0:\n",
    "            candidate_embs = embeddings[step2_candidates]\n",
    "            query_norm = norm(embeddings[qi])\n",
    "            candidate_norms = norm(candidate_embs, axis=1)\n",
    "            cosines = (candidate_embs @ embeddings[qi]) / (candidate_norms * query_norm + 1e-10)\n",
    "            top_k_idx = np.argsort(cosines)[-10:][::-1]\n",
    "            top_k_indices = step2_candidates[top_k_idx]\n",
    "        else:\n",
    "            top_k_indices = np.array([])\n",
    "        \n",
    "        elapsed = (time.time() - t0) * 1000\n",
    "        recall = evaluate_recall(top_k_indices, internal_ground_truths[qi], k=10)\n",
    "        \n",
    "        internal_recalls.append(recall)\n",
    "        internal_candidates.append(len(step1_candidates))\n",
    "        internal_times.append(elapsed)\n",
    "    \n",
    "    # 外部クエリ評価\n",
    "    external_recalls = []\n",
    "    external_candidates = []\n",
    "    external_times = []\n",
    "    \n",
    "    for i, (q_emb, q_hash) in enumerate(zip(external_query_embs, external_query_hashes)):\n",
    "        t0 = time.time()\n",
    "        step1_candidates = rbs_index.query(q_hash)\n",
    "        \n",
    "        # Step 2: ハミング距離ソート\n",
    "        if len(step1_candidates) > 2000:\n",
    "            dists = hamming_distance_batch(q_hash, hashes[step1_candidates])\n",
    "            top_idx = np.argsort(dists)[:2000]\n",
    "            step2_candidates = step1_candidates[top_idx]\n",
    "        else:\n",
    "            step2_candidates = step1_candidates\n",
    "        \n",
    "        # Step 3: コサイン類似度\n",
    "        if len(step2_candidates) > 0:\n",
    "            candidate_embs = embeddings[step2_candidates]\n",
    "            query_norm = norm(q_emb)\n",
    "            candidate_norms = norm(candidate_embs, axis=1)\n",
    "            cosines = (candidate_embs @ q_emb) / (candidate_norms * query_norm + 1e-10)\n",
    "            top_k_idx = np.argsort(cosines)[-10:][::-1]\n",
    "            top_k_indices = step2_candidates[top_k_idx]\n",
    "        else:\n",
    "            top_k_indices = np.array([])\n",
    "        \n",
    "        elapsed = (time.time() - t0) * 1000\n",
    "        recall = evaluate_recall(top_k_indices, external_ground_truths[i], k=10)\n",
    "        \n",
    "        external_recalls.append(recall)\n",
    "        external_candidates.append(len(step1_candidates))\n",
    "        external_times.append(elapsed)\n",
    "    \n",
    "    rbs_results.append({\n",
    "        'method': label,\n",
    "        'n_bits': n_bits,\n",
    "        'n_tables': n_tables,\n",
    "        'internal_recall@10': np.mean(internal_recalls),\n",
    "        'internal_avg_candidates': np.mean(internal_candidates),\n",
    "        'internal_avg_time_ms': np.mean(internal_times),\n",
    "        'external_recall@10': np.mean(external_recalls),\n",
    "        'external_avg_candidates': np.mean(external_candidates),\n",
    "        'external_avg_time_ms': np.mean(external_times),\n",
    "        'avg_bucket_size': stats['avg_bucket_size'],\n",
    "    })\n",
    "\n",
    "df_rbs = pd.DataFrame(rbs_results)\n",
    "print('\\n評価完了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================================================================\n",
      "ランダムビットサンプリング結果\n",
      "========================================================================================================================\n",
      "\n",
      "                   Method | Int R@10 |   Int Cand | Ext R@10 |   Ext Cand |    Gap |  BktSize\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "     RBS(12bit, 10tables) |    57.0% |       5690 |    62.0% |       7429 |  -5.0pt |     97.7\n",
      "     RBS(12bit, 20tables) |    69.6% |       9584 |    73.7% |      10640 |  -4.1pt |     97.7\n",
      "     RBS(12bit, 30tables) |    76.8% |      12832 |    83.7%★ |      14128 |  -6.9pt |     97.7\n",
      "     RBS(16bit, 10tables) |    38.1% |       2220 |    42.3% |       2967 |  -4.2pt |      9.0\n",
      "     RBS(16bit, 20tables) |    50.4% |       3361 |    55.7% |       4129 |  -5.3pt |      8.9\n",
      "     RBS(16bit, 30tables) |    57.0% |       4325 |    65.0% |       5053 |  -8.0pt |      8.7\n",
      "     RBS(20bit, 20tables) |    34.6% |        960 |    33.7% |        616 |   0.9pt |      2.6\n",
      "     RBS(20bit, 30tables) |    42.5% |       1483 |    45.7% |        858 |  -3.2pt |      2.7\n"
     ]
    }
   ],
   "source": [
    "# ランダムビットサンプリング結果表示\n",
    "print('\\n' + '=' * 120)\n",
    "print('ランダムビットサンプリング結果')\n",
    "print('=' * 120)\n",
    "\n",
    "print(f'\\n{\"Method\":>25} | {\"Int R@10\":>8} | {\"Int Cand\":>10} | {\"Ext R@10\":>8} | {\"Ext Cand\":>10} | {\"Gap\":>6} | {\"BktSize\":>8}')\n",
    "print('-' * 120)\n",
    "\n",
    "for _, row in df_rbs.iterrows():\n",
    "    gap = row['internal_recall@10'] - row['external_recall@10']\n",
    "    ext_mark = '★' if row['external_recall@10'] >= 0.80 else ''\n",
    "    print(f'{row[\"method\"]:>25} | {row[\"internal_recall@10\"]*100:>7.1f}% | '\n",
    "          f'{row[\"internal_avg_candidates\"]:>10.0f} | {row[\"external_recall@10\"]*100:>7.1f}%{ext_mark} | '\n",
    "          f'{row[\"external_avg_candidates\"]:>10.0f} | {gap*100:>5.1f}pt | '\n",
    "          f'{row[\"avg_bucket_size\"]:>8.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 手法3: チャンク比較（SimHash風）\n",
    "\n",
    "ハッシュを大きなチャンクに分割し、一定数以上のチャンク一致を要求"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkedHashIndex:\n",
    "    \"\"\"\n",
    "    チャンク比較によるLSHインデックス\n",
    "    \n",
    "    128bitハッシュを複数のチャンクに分割し、\n",
    "    一定数以上のチャンクが一致するドキュメントを候補とする。\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size=16, min_matching_chunks=6):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            chunk_size: チャンクサイズ（ビット数）\n",
    "            min_matching_chunks: 候補とするための最小一致チャンク数\n",
    "        \"\"\"\n",
    "        self.chunk_size = chunk_size\n",
    "        self.n_chunks = 128 // chunk_size\n",
    "        self.min_matching_chunks = min_matching_chunks\n",
    "        self.chunk_indices = None\n",
    "    \n",
    "    def build(self, hashes):\n",
    "        \"\"\"\n",
    "        インデックスを構築\n",
    "        \"\"\"\n",
    "        self.chunk_indices = []\n",
    "        \n",
    "        for chunk_idx in range(self.n_chunks):\n",
    "            start = chunk_idx * self.chunk_size\n",
    "            end = start + self.chunk_size\n",
    "            chunk_bits = hashes[:, start:end]\n",
    "            chunk_values = bits_to_int(chunk_bits)\n",
    "            \n",
    "            table = defaultdict(list)\n",
    "            for doc_idx, val in enumerate(chunk_values):\n",
    "                table[int(val)].append(doc_idx)\n",
    "            self.chunk_indices.append(table)\n",
    "    \n",
    "    def query(self, query_hash):\n",
    "        \"\"\"\n",
    "        クエリを実行し、候補を返す\n",
    "        \"\"\"\n",
    "        # 各ドキュメントの一致チャンク数をカウント\n",
    "        chunk_match_counts = defaultdict(int)\n",
    "        \n",
    "        for chunk_idx in range(self.n_chunks):\n",
    "            start = chunk_idx * self.chunk_size\n",
    "            end = start + self.chunk_size\n",
    "            query_chunk_bits = query_hash[start:end]\n",
    "            query_chunk_val = int(bits_to_int(query_chunk_bits.reshape(1, -1))[0])\n",
    "            \n",
    "            if query_chunk_val in self.chunk_indices[chunk_idx]:\n",
    "                for doc_idx in self.chunk_indices[chunk_idx][query_chunk_val]:\n",
    "                    chunk_match_counts[doc_idx] += 1\n",
    "        \n",
    "        # min_matching_chunks以上一致するドキュメントを候補とする\n",
    "        candidates = np.array([\n",
    "            doc_idx for doc_idx, count in chunk_match_counts.items()\n",
    "            if count >= self.min_matching_chunks\n",
    "        ])\n",
    "        return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "チャンク比較評価\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "設定: 100%|██████████| 7/7 [00:13<00:00,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "評価完了\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# チャンク比較の評価\n",
    "print('=' * 80)\n",
    "print('チャンク比較評価')\n",
    "print('=' * 80)\n",
    "\n",
    "chunk_configs = [\n",
    "    (16, 8, 'Chunk16(8/8)'),    # 8チャンク、全一致\n",
    "    (16, 7, 'Chunk16(7/8)'),    # 8チャンク、7/8一致\n",
    "    (16, 6, 'Chunk16(6/8)'),    # 8チャンク、6/8一致\n",
    "    (16, 5, 'Chunk16(5/8)'),    # 8チャンク、5/8一致\n",
    "    (32, 4, 'Chunk32(4/4)'),    # 4チャンク、全一致\n",
    "    (32, 3, 'Chunk32(3/4)'),    # 4チャンク、3/4一致\n",
    "    (32, 2, 'Chunk32(2/4)'),    # 4チャンク、2/4一致\n",
    "]\n",
    "\n",
    "chunk_results = []\n",
    "\n",
    "for chunk_size, min_match, label in tqdm(chunk_configs, desc='設定'):\n",
    "    # インデックス構築\n",
    "    chunk_index = ChunkedHashIndex(chunk_size=chunk_size, min_matching_chunks=min_match)\n",
    "    chunk_index.build(hashes)\n",
    "    \n",
    "    # 内部クエリ評価\n",
    "    internal_recalls = []\n",
    "    internal_candidates = []\n",
    "    internal_times = []\n",
    "    \n",
    "    for qi in internal_query_indices:\n",
    "        t0 = time.time()\n",
    "        step1_candidates = chunk_index.query(hashes[qi])\n",
    "        \n",
    "        # Step 2: ハミング距離ソート\n",
    "        if len(step1_candidates) > 2000:\n",
    "            dists = hamming_distance_batch(hashes[qi], hashes[step1_candidates])\n",
    "            top_idx = np.argsort(dists)[:2000]\n",
    "            step2_candidates = step1_candidates[top_idx]\n",
    "        else:\n",
    "            step2_candidates = step1_candidates\n",
    "        \n",
    "        # Step 3: コサイン類似度\n",
    "        if len(step2_candidates) > 0:\n",
    "            candidate_embs = embeddings[step2_candidates]\n",
    "            query_norm = norm(embeddings[qi])\n",
    "            candidate_norms = norm(candidate_embs, axis=1)\n",
    "            cosines = (candidate_embs @ embeddings[qi]) / (candidate_norms * query_norm + 1e-10)\n",
    "            top_k_idx = np.argsort(cosines)[-10:][::-1]\n",
    "            top_k_indices = step2_candidates[top_k_idx]\n",
    "        else:\n",
    "            top_k_indices = np.array([])\n",
    "        \n",
    "        elapsed = (time.time() - t0) * 1000\n",
    "        recall = evaluate_recall(top_k_indices, internal_ground_truths[qi], k=10)\n",
    "        \n",
    "        internal_recalls.append(recall)\n",
    "        internal_candidates.append(len(step1_candidates))\n",
    "        internal_times.append(elapsed)\n",
    "    \n",
    "    # 外部クエリ評価\n",
    "    external_recalls = []\n",
    "    external_candidates = []\n",
    "    external_times = []\n",
    "    \n",
    "    for i, (q_emb, q_hash) in enumerate(zip(external_query_embs, external_query_hashes)):\n",
    "        t0 = time.time()\n",
    "        step1_candidates = chunk_index.query(q_hash)\n",
    "        \n",
    "        # Step 2: ハミング距離ソート\n",
    "        if len(step1_candidates) > 2000:\n",
    "            dists = hamming_distance_batch(q_hash, hashes[step1_candidates])\n",
    "            top_idx = np.argsort(dists)[:2000]\n",
    "            step2_candidates = step1_candidates[top_idx]\n",
    "        else:\n",
    "            step2_candidates = step1_candidates\n",
    "        \n",
    "        # Step 3: コサイン類似度\n",
    "        if len(step2_candidates) > 0:\n",
    "            candidate_embs = embeddings[step2_candidates]\n",
    "            query_norm = norm(q_emb)\n",
    "            candidate_norms = norm(candidate_embs, axis=1)\n",
    "            cosines = (candidate_embs @ q_emb) / (candidate_norms * query_norm + 1e-10)\n",
    "            top_k_idx = np.argsort(cosines)[-10:][::-1]\n",
    "            top_k_indices = step2_candidates[top_k_idx]\n",
    "        else:\n",
    "            top_k_indices = np.array([])\n",
    "        \n",
    "        elapsed = (time.time() - t0) * 1000\n",
    "        recall = evaluate_recall(top_k_indices, external_ground_truths[i], k=10)\n",
    "        \n",
    "        external_recalls.append(recall)\n",
    "        external_candidates.append(len(step1_candidates))\n",
    "        external_times.append(elapsed)\n",
    "    \n",
    "    chunk_results.append({\n",
    "        'method': label,\n",
    "        'chunk_size': chunk_size,\n",
    "        'min_match': min_match,\n",
    "        'internal_recall@10': np.mean(internal_recalls),\n",
    "        'internal_avg_candidates': np.mean(internal_candidates),\n",
    "        'internal_avg_time_ms': np.mean(internal_times),\n",
    "        'external_recall@10': np.mean(external_recalls),\n",
    "        'external_avg_candidates': np.mean(external_candidates),\n",
    "        'external_avg_time_ms': np.mean(external_times),\n",
    "    })\n",
    "\n",
    "df_chunk = pd.DataFrame(chunk_results)\n",
    "print('\\n評価完了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "チャンク比較結果\n",
      "====================================================================================================\n",
      "\n",
      "         Method | Int R@10 |   Int Cand | Ext R@10 |   Ext Cand |    Gap\n",
      "----------------------------------------------------------------------------------------------------\n",
      "   Chunk16(8/8) |    10.0% |          1 |     1.0% |          0 |   9.0pt\n",
      "   Chunk16(7/8) |    10.3% |          1 |     1.0% |          0 |   9.3pt\n",
      "   Chunk16(6/8) |    10.5% |          4 |     1.0% |          0 |   9.5pt\n",
      "   Chunk16(5/8) |    10.8% |         17 |     1.3% |          0 |   9.5pt\n",
      "   Chunk32(4/4) |    10.0% |          1 |     1.0% |          0 |   9.0pt\n",
      "   Chunk32(3/4) |    10.3% |          2 |     1.0% |          0 |   9.3pt\n",
      "   Chunk32(2/4) |    10.8% |         17 |     1.3% |          0 |   9.5pt\n"
     ]
    }
   ],
   "source": [
    "# チャンク比較結果表示\n",
    "print('\\n' + '=' * 100)\n",
    "print('チャンク比較結果')\n",
    "print('=' * 100)\n",
    "\n",
    "print(f'\\n{\"Method\":>15} | {\"Int R@10\":>8} | {\"Int Cand\":>10} | {\"Ext R@10\":>8} | {\"Ext Cand\":>10} | {\"Gap\":>6}')\n",
    "print('-' * 100)\n",
    "\n",
    "for _, row in df_chunk.iterrows():\n",
    "    gap = row['internal_recall@10'] - row['external_recall@10']\n",
    "    ext_mark = '★' if row['external_recall@10'] >= 0.80 else ''\n",
    "    print(f'{row[\"method\"]:>15} | {row[\"internal_recall@10\"]*100:>7.1f}% | '\n",
    "          f'{row[\"internal_avg_candidates\"]:>10.0f} | {row[\"external_recall@10\"]*100:>7.1f}%{ext_mark} | '\n",
    "          f'{row[\"external_avg_candidates\"]:>10.0f} | {gap*100:>5.1f}pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. 手法4: 適応的閾値（Progressive Search）\n",
    "\n",
    "厳しいフィルタから開始し、候補が少なければ段階的に緩和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveThresholdSearch:\n",
    "    \"\"\"\n",
    "    適応的閾値による検索\n",
    "    \n",
    "    厳しいフィルタから開始し、候補数が不足していれば\n",
    "    段階的に緩和していく。\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, target_candidates=2000, min_candidates=500):\n",
    "        self.target_candidates = target_candidates\n",
    "        self.min_candidates = min_candidates\n",
    "        \n",
    "        # 検索手法とパラメータのリスト（厳しい順）\n",
    "        self.strategies = []\n",
    "    \n",
    "    def add_overlap_strategy(self, segment_index, n_segments, segment_width, max_flips, label):\n",
    "        \"\"\"Overlapセグメント戦略を追加\"\"\"\n",
    "        self.strategies.append({\n",
    "            'type': 'overlap',\n",
    "            'segment_index': segment_index,\n",
    "            'n_segments': n_segments,\n",
    "            'segment_width': segment_width,\n",
    "            'max_flips': max_flips,\n",
    "            'label': label,\n",
    "        })\n",
    "    \n",
    "    def add_rbs_strategy(self, rbs_index, label):\n",
    "        \"\"\"ランダムビットサンプリング戦略を追加\"\"\"\n",
    "        self.strategies.append({\n",
    "            'type': 'rbs',\n",
    "            'index': rbs_index,\n",
    "            'label': label,\n",
    "        })\n",
    "    \n",
    "    def add_full_hamming_strategy(self, all_hashes, label='FullHamming'):\n",
    "        \"\"\"全件ハミング距離戦略を追加（フォールバック）\"\"\"\n",
    "        self.strategies.append({\n",
    "            'type': 'full_hamming',\n",
    "            'all_hashes': all_hashes,\n",
    "            'label': label,\n",
    "        })\n",
    "    \n",
    "    def search(self, query_hash, query_segments=None):\n",
    "        \"\"\"\n",
    "        適応的に検索を実行\n",
    "        \n",
    "        Returns:\n",
    "            candidates: 候補インデックス\n",
    "            strategy_used: 使用した戦略のラベル\n",
    "        \"\"\"\n",
    "        for strategy in self.strategies:\n",
    "            if strategy['type'] == 'overlap':\n",
    "                candidates = fuzzy_segment_search(\n",
    "                    query_segments,\n",
    "                    strategy['segment_index'],\n",
    "                    strategy['n_segments'],\n",
    "                    strategy['segment_width'],\n",
    "                    strategy['max_flips']\n",
    "                )\n",
    "            elif strategy['type'] == 'rbs':\n",
    "                candidates = strategy['index'].query(query_hash)\n",
    "            elif strategy['type'] == 'full_hamming':\n",
    "                # フォールバック: 全件ハミング距離計算\n",
    "                dists = hamming_distance_batch(query_hash, strategy['all_hashes'])\n",
    "                top_idx = np.argsort(dists)[:self.target_candidates]\n",
    "                return top_idx, strategy['label']\n",
    "            \n",
    "            if len(candidates) >= self.min_candidates:\n",
    "                return candidates, strategy['label']\n",
    "        \n",
    "        # 全戦略で候補不足の場合（通常は到達しない）\n",
    "        return np.array([]), 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "適応的閾値検索を構築中...\n",
      "  戦略数: 5\n"
     ]
    }
   ],
   "source": [
    "# 適応的閾値検索の構築\n",
    "print('適応的閾値検索を構築中...')\n",
    "\n",
    "adaptive_search = AdaptiveThresholdSearch(target_candidates=2000, min_candidates=500)\n",
    "\n",
    "# 戦略を追加（厳しい順）\n",
    "adaptive_search.add_overlap_strategy(segment_index, n_segments, SEGMENT_WIDTH, 0, 'Exact(8/8)')\n",
    "adaptive_search.add_overlap_strategy(segment_index, n_segments, SEGMENT_WIDTH, 1, 'Fuzzy(7/8)')\n",
    "adaptive_search.add_overlap_strategy(segment_index, n_segments, SEGMENT_WIDTH, 2, 'Fuzzy(6/8)')\n",
    "\n",
    "# RBS戦略も追加\n",
    "rbs_for_adaptive = RandomBitSamplingIndex(n_bits_sample=16, n_tables=30, seed=42)\n",
    "rbs_for_adaptive.build(hashes)\n",
    "adaptive_search.add_rbs_strategy(rbs_for_adaptive, 'RBS(16,30)')\n",
    "\n",
    "# フォールバック\n",
    "adaptive_search.add_full_hamming_strategy(hashes, 'FullHamming')\n",
    "\n",
    "print(f'  戦略数: {len(adaptive_search.strategies)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "適応的閾値検索評価\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "内部クエリ: 100%|██████████| 100/100 [00:02<00:00, 48.11it/s]\n",
      "外部クエリ: 100%|██████████| 30/30 [00:00<00:00, 45.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "評価完了\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 適応的閾値検索の評価\n",
    "print('=' * 80)\n",
    "print('適応的閾値検索評価')\n",
    "print('=' * 80)\n",
    "\n",
    "# 内部クエリ評価\n",
    "internal_recalls = []\n",
    "internal_candidates = []\n",
    "internal_times = []\n",
    "internal_strategies = defaultdict(int)\n",
    "\n",
    "for qi in tqdm(internal_query_indices, desc='内部クエリ'):\n",
    "    q_segments = segments[qi]\n",
    "    \n",
    "    t0 = time.time()\n",
    "    step1_candidates, strategy_used = adaptive_search.search(hashes[qi], q_segments)\n",
    "    internal_strategies[strategy_used] += 1\n",
    "    \n",
    "    # Step 2: ハミング距離ソート\n",
    "    if len(step1_candidates) > 2000:\n",
    "        dists = hamming_distance_batch(hashes[qi], hashes[step1_candidates])\n",
    "        top_idx = np.argsort(dists)[:2000]\n",
    "        step2_candidates = step1_candidates[top_idx]\n",
    "    else:\n",
    "        step2_candidates = step1_candidates\n",
    "    \n",
    "    # Step 3: コサイン類似度\n",
    "    if len(step2_candidates) > 0:\n",
    "        candidate_embs = embeddings[step2_candidates]\n",
    "        query_norm = norm(embeddings[qi])\n",
    "        candidate_norms = norm(candidate_embs, axis=1)\n",
    "        cosines = (candidate_embs @ embeddings[qi]) / (candidate_norms * query_norm + 1e-10)\n",
    "        top_k_idx = np.argsort(cosines)[-10:][::-1]\n",
    "        top_k_indices = step2_candidates[top_k_idx]\n",
    "    else:\n",
    "        top_k_indices = np.array([])\n",
    "    \n",
    "    elapsed = (time.time() - t0) * 1000\n",
    "    recall = evaluate_recall(top_k_indices, internal_ground_truths[qi], k=10)\n",
    "    \n",
    "    internal_recalls.append(recall)\n",
    "    internal_candidates.append(len(step1_candidates))\n",
    "    internal_times.append(elapsed)\n",
    "\n",
    "# 外部クエリ評価\n",
    "external_recalls = []\n",
    "external_candidates = []\n",
    "external_times = []\n",
    "external_strategies = defaultdict(int)\n",
    "\n",
    "for i, (q_emb, q_hash) in enumerate(tqdm(zip(external_query_embs, external_query_hashes), desc='外部クエリ', total=len(external_query_embs))):\n",
    "    q_segments, _ = hash_to_overlap_segments(q_hash.reshape(1, -1), SEGMENT_WIDTH, STRIDE)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    step1_candidates, strategy_used = adaptive_search.search(q_hash, q_segments[0])\n",
    "    external_strategies[strategy_used] += 1\n",
    "    \n",
    "    # Step 2: ハミング距離ソート\n",
    "    if len(step1_candidates) > 2000:\n",
    "        dists = hamming_distance_batch(q_hash, hashes[step1_candidates])\n",
    "        top_idx = np.argsort(dists)[:2000]\n",
    "        step2_candidates = step1_candidates[top_idx]\n",
    "    else:\n",
    "        step2_candidates = step1_candidates\n",
    "    \n",
    "    # Step 3: コサイン類似度\n",
    "    if len(step2_candidates) > 0:\n",
    "        candidate_embs = embeddings[step2_candidates]\n",
    "        query_norm = norm(q_emb)\n",
    "        candidate_norms = norm(candidate_embs, axis=1)\n",
    "        cosines = (candidate_embs @ q_emb) / (candidate_norms * query_norm + 1e-10)\n",
    "        top_k_idx = np.argsort(cosines)[-10:][::-1]\n",
    "        top_k_indices = step2_candidates[top_k_idx]\n",
    "    else:\n",
    "        top_k_indices = np.array([])\n",
    "    \n",
    "    elapsed = (time.time() - t0) * 1000\n",
    "    recall = evaluate_recall(top_k_indices, external_ground_truths[i], k=10)\n",
    "    \n",
    "    external_recalls.append(recall)\n",
    "    external_candidates.append(len(step1_candidates))\n",
    "    external_times.append(elapsed)\n",
    "\n",
    "print('\\n評価完了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "適応的閾値検索結果\n",
      "================================================================================\n",
      "\n",
      "■ 内部クエリ（100件）\n",
      "  Recall@10: 90.0%\n",
      "  平均候補数: 63764\n",
      "  平均時間: 20.62ms\n",
      "  戦略使用頻度: {'Exact(8/8)': 100}\n",
      "\n",
      "■ 外部クエリ（30件）\n",
      "  Recall@10: 95.0%\n",
      "  平均候補数: 70924\n",
      "  平均時間: 21.32ms\n",
      "  戦略使用頻度: {'Exact(8/8)': 30}\n",
      "\n",
      "■ 内部/外部ギャップ: -5.0pt\n"
     ]
    }
   ],
   "source": [
    "# 適応的閾値検索結果表示\n",
    "print('\\n' + '=' * 80)\n",
    "print('適応的閾値検索結果')\n",
    "print('=' * 80)\n",
    "\n",
    "print(f'\\n■ 内部クエリ（{n_internal_queries}件）')\n",
    "print(f'  Recall@10: {np.mean(internal_recalls)*100:.1f}%')\n",
    "print(f'  平均候補数: {np.mean(internal_candidates):.0f}')\n",
    "print(f'  平均時間: {np.mean(internal_times):.2f}ms')\n",
    "print(f'  戦略使用頻度: {dict(internal_strategies)}')\n",
    "\n",
    "print(f'\\n■ 外部クエリ（{len(external_queries)}件）')\n",
    "print(f'  Recall@10: {np.mean(external_recalls)*100:.1f}%')\n",
    "print(f'  平均候補数: {np.mean(external_candidates):.0f}')\n",
    "print(f'  平均時間: {np.mean(external_times):.2f}ms')\n",
    "print(f'  戦略使用頻度: {dict(external_strategies)}')\n",
    "\n",
    "gap = np.mean(internal_recalls) - np.mean(external_recalls)\n",
    "print(f'\\n■ 内部/外部ギャップ: {gap*100:.1f}pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. 手法5: ビット重要度加重\n",
    "\n",
    "分散の高い（識別力のある）ビット位置を優先的に使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ビット位置ごとの分散を計算中...\n",
      "  最小分散: 0.2485\n",
      "  最大分散: 0.2500\n",
      "  平均分散: 0.2498\n",
      "  理論最大（p=0.5）: 0.2500\n",
      "\n",
      "  選択した重要ビット（64個）: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(5), np.int64(8), np.int64(10), np.int64(11), np.int64(12), np.int64(14)]... \n"
     ]
    }
   ],
   "source": [
    "# ビット位置ごとの分散を計算\n",
    "print('ビット位置ごとの分散を計算中...')\n",
    "bit_variance = np.var(hashes.astype(float), axis=0)\n",
    "\n",
    "# 分布を可視化\n",
    "print(f'  最小分散: {np.min(bit_variance):.4f}')\n",
    "print(f'  最大分散: {np.max(bit_variance):.4f}')\n",
    "print(f'  平均分散: {np.mean(bit_variance):.4f}')\n",
    "print(f'  理論最大（p=0.5）: 0.2500')\n",
    "\n",
    "# 重要ビットを選択（分散が高い＝識別力が高い）\n",
    "important_bit_indices = np.argsort(bit_variance)[-64:]  # 上位64bit\n",
    "print(f'\\n  選択した重要ビット（64個）: {sorted(important_bit_indices)[:10]}... ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "重要ビットでのインデックス構築中...\n",
      "  重要ビットハッシュ shape: (400000, 64)\n",
      "  セグメント数: 15\n"
     ]
    }
   ],
   "source": [
    "# 重要ビットのみでOverlapインデックスを構築\n",
    "print('重要ビットでのインデックス構築中...')\n",
    "\n",
    "# 重要ビットのみを抽出\n",
    "important_hashes = hashes[:, important_bit_indices]\n",
    "print(f'  重要ビットハッシュ shape: {important_hashes.shape}')\n",
    "\n",
    "# Overlap(8,4)インデックス構築\n",
    "important_segments, important_n_segments = hash_to_overlap_segments(\n",
    "    important_hashes, segment_width=8, stride=4\n",
    ")\n",
    "important_segment_index = build_overlap_index(important_segments)\n",
    "\n",
    "print(f'  セグメント数: {important_n_segments}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ビット重要度加重評価\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "設定: 100%|██████████| 3/3 [00:20<00:00,  6.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "評価完了\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ビット重要度加重の評価\n",
    "print('=' * 80)\n",
    "print('ビット重要度加重評価')\n",
    "print('=' * 80)\n",
    "\n",
    "weighted_configs = [\n",
    "    (0, 'Important+Exact'),\n",
    "    (1, 'Important+Fuzzy(7/8)'),\n",
    "    (2, 'Important+Fuzzy(6/8)'),\n",
    "]\n",
    "\n",
    "weighted_results = []\n",
    "\n",
    "for max_flips, label in tqdm(weighted_configs, desc='設定'):\n",
    "    # 内部クエリ評価\n",
    "    internal_recalls = []\n",
    "    internal_candidates = []\n",
    "    internal_times = []\n",
    "    \n",
    "    for qi in internal_query_indices:\n",
    "        # 重要ビットのみでセグメント生成\n",
    "        q_important_hash = hashes[qi][important_bit_indices]\n",
    "        q_segments, _ = hash_to_overlap_segments(\n",
    "            q_important_hash.reshape(1, -1), segment_width=8, stride=4\n",
    "        )\n",
    "        \n",
    "        t0 = time.time()\n",
    "        step1_candidates = fuzzy_segment_search(\n",
    "            q_segments[0], important_segment_index, important_n_segments, 8, max_flips\n",
    "        )\n",
    "        \n",
    "        # Step 2: ハミング距離ソート（元の128bitで計算）\n",
    "        if len(step1_candidates) > 2000:\n",
    "            dists = hamming_distance_batch(hashes[qi], hashes[step1_candidates])\n",
    "            top_idx = np.argsort(dists)[:2000]\n",
    "            step2_candidates = step1_candidates[top_idx]\n",
    "        else:\n",
    "            step2_candidates = step1_candidates\n",
    "        \n",
    "        # Step 3: コサイン類似度\n",
    "        if len(step2_candidates) > 0:\n",
    "            candidate_embs = embeddings[step2_candidates]\n",
    "            query_norm = norm(embeddings[qi])\n",
    "            candidate_norms = norm(candidate_embs, axis=1)\n",
    "            cosines = (candidate_embs @ embeddings[qi]) / (candidate_norms * query_norm + 1e-10)\n",
    "            top_k_idx = np.argsort(cosines)[-10:][::-1]\n",
    "            top_k_indices = step2_candidates[top_k_idx]\n",
    "        else:\n",
    "            top_k_indices = np.array([])\n",
    "        \n",
    "        elapsed = (time.time() - t0) * 1000\n",
    "        recall = evaluate_recall(top_k_indices, internal_ground_truths[qi], k=10)\n",
    "        \n",
    "        internal_recalls.append(recall)\n",
    "        internal_candidates.append(len(step1_candidates))\n",
    "        internal_times.append(elapsed)\n",
    "    \n",
    "    # 外部クエリ評価\n",
    "    external_recalls = []\n",
    "    external_candidates = []\n",
    "    external_times = []\n",
    "    \n",
    "    for i, (q_emb, q_hash) in enumerate(zip(external_query_embs, external_query_hashes)):\n",
    "        # 重要ビットのみでセグメント生成\n",
    "        q_important_hash = q_hash[important_bit_indices]\n",
    "        q_segments, _ = hash_to_overlap_segments(\n",
    "            q_important_hash.reshape(1, -1), segment_width=8, stride=4\n",
    "        )\n",
    "        \n",
    "        t0 = time.time()\n",
    "        step1_candidates = fuzzy_segment_search(\n",
    "            q_segments[0], important_segment_index, important_n_segments, 8, max_flips\n",
    "        )\n",
    "        \n",
    "        # Step 2: ハミング距離ソート\n",
    "        if len(step1_candidates) > 2000:\n",
    "            dists = hamming_distance_batch(q_hash, hashes[step1_candidates])\n",
    "            top_idx = np.argsort(dists)[:2000]\n",
    "            step2_candidates = step1_candidates[top_idx]\n",
    "        else:\n",
    "            step2_candidates = step1_candidates\n",
    "        \n",
    "        # Step 3: コサイン類似度\n",
    "        if len(step2_candidates) > 0:\n",
    "            candidate_embs = embeddings[step2_candidates]\n",
    "            query_norm = norm(q_emb)\n",
    "            candidate_norms = norm(candidate_embs, axis=1)\n",
    "            cosines = (candidate_embs @ q_emb) / (candidate_norms * query_norm + 1e-10)\n",
    "            top_k_idx = np.argsort(cosines)[-10:][::-1]\n",
    "            top_k_indices = step2_candidates[top_k_idx]\n",
    "        else:\n",
    "            top_k_indices = np.array([])\n",
    "        \n",
    "        elapsed = (time.time() - t0) * 1000\n",
    "        recall = evaluate_recall(top_k_indices, external_ground_truths[i], k=10)\n",
    "        \n",
    "        external_recalls.append(recall)\n",
    "        external_candidates.append(len(step1_candidates))\n",
    "        external_times.append(elapsed)\n",
    "    \n",
    "    weighted_results.append({\n",
    "        'method': label,\n",
    "        'max_flips': max_flips,\n",
    "        'internal_recall@10': np.mean(internal_recalls),\n",
    "        'internal_avg_candidates': np.mean(internal_candidates),\n",
    "        'internal_avg_time_ms': np.mean(internal_times),\n",
    "        'external_recall@10': np.mean(external_recalls),\n",
    "        'external_avg_candidates': np.mean(external_candidates),\n",
    "        'external_avg_time_ms': np.mean(external_times),\n",
    "    })\n",
    "\n",
    "df_weighted = pd.DataFrame(weighted_results)\n",
    "print('\\n評価完了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "ビット重要度加重結果\n",
      "====================================================================================================\n",
      "\n",
      "              Method | Int R@10 |   Int Cand | Ext R@10 |   Ext Cand |    Gap\n",
      "----------------------------------------------------------------------------------------------------\n",
      "     Important+Exact |    82.7% |      39741 |    89.3%★ |      42323 |  -6.6pt\n",
      "Important+Fuzzy(7/8) |    91.6% |     158393 |    95.7%★ |     151120 |  -4.1pt\n",
      "Important+Fuzzy(6/8) |    91.5% |     308850 |    95.7%★ |     296454 |  -4.2pt\n"
     ]
    }
   ],
   "source": [
    "# ビット重要度加重結果表示\n",
    "print('\\n' + '=' * 100)\n",
    "print('ビット重要度加重結果')\n",
    "print('=' * 100)\n",
    "\n",
    "print(f'\\n{\"Method\":>20} | {\"Int R@10\":>8} | {\"Int Cand\":>10} | {\"Ext R@10\":>8} | {\"Ext Cand\":>10} | {\"Gap\":>6}')\n",
    "print('-' * 100)\n",
    "\n",
    "for _, row in df_weighted.iterrows():\n",
    "    gap = row['internal_recall@10'] - row['external_recall@10']\n",
    "    ext_mark = '★' if row['external_recall@10'] >= 0.80 else ''\n",
    "    print(f'{row[\"method\"]:>20} | {row[\"internal_recall@10\"]*100:>7.1f}% | '\n",
    "          f'{row[\"internal_avg_candidates\"]:>10.0f} | {row[\"external_recall@10\"]*100:>7.1f}%{ext_mark} | '\n",
    "          f'{row[\"external_avg_candidates\"]:>10.0f} | {gap*100:>5.1f}pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. 総合比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全手法の結果を統合\n",
    "all_results = []\n",
    "\n",
    "# ファジーセグメント\n",
    "for _, row in df_fuzzy.iterrows():\n",
    "    all_results.append({\n",
    "        'category': 'Fuzzy Segment',\n",
    "        'method': row['method'],\n",
    "        'internal_recall@10': row['internal_recall@10'],\n",
    "        'external_recall@10': row['external_recall@10'],\n",
    "        'internal_candidates': row['internal_avg_candidates'],\n",
    "        'external_candidates': row['external_avg_candidates'],\n",
    "    })\n",
    "\n",
    "# ランダムビットサンプリング\n",
    "for _, row in df_rbs.iterrows():\n",
    "    all_results.append({\n",
    "        'category': 'Random Bit Sampling',\n",
    "        'method': row['method'],\n",
    "        'internal_recall@10': row['internal_recall@10'],\n",
    "        'external_recall@10': row['external_recall@10'],\n",
    "        'internal_candidates': row['internal_avg_candidates'],\n",
    "        'external_candidates': row['external_avg_candidates'],\n",
    "    })\n",
    "\n",
    "# チャンク比較\n",
    "for _, row in df_chunk.iterrows():\n",
    "    all_results.append({\n",
    "        'category': 'Chunked Hash',\n",
    "        'method': row['method'],\n",
    "        'internal_recall@10': row['internal_recall@10'],\n",
    "        'external_recall@10': row['external_recall@10'],\n",
    "        'internal_candidates': row['internal_avg_candidates'],\n",
    "        'external_candidates': row['external_avg_candidates'],\n",
    "    })\n",
    "\n",
    "# 適応的閾値\n",
    "all_results.append({\n",
    "    'category': 'Adaptive',\n",
    "    'method': 'Adaptive Threshold',\n",
    "    'internal_recall@10': np.mean(internal_recalls),\n",
    "    'external_recall@10': np.mean(external_recalls),\n",
    "    'internal_candidates': np.mean(internal_candidates),\n",
    "    'external_candidates': np.mean(external_candidates),\n",
    "})\n",
    "\n",
    "# ビット重要度加重\n",
    "for _, row in df_weighted.iterrows():\n",
    "    all_results.append({\n",
    "        'category': 'Weighted Bits',\n",
    "        'method': row['method'],\n",
    "        'internal_recall@10': row['internal_recall@10'],\n",
    "        'external_recall@10': row['external_recall@10'],\n",
    "        'internal_candidates': row['internal_avg_candidates'],\n",
    "        'external_candidates': row['external_avg_candidates'],\n",
    "    })\n",
    "\n",
    "df_all = pd.DataFrame(all_results)\n",
    "df_all['gap'] = df_all['internal_recall@10'] - df_all['external_recall@10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "総合比較: 全手法の結果サマリー\n",
      "========================================================================================================================\n",
      "\n",
      "            Category |                    Method | Int R@10 | Ext R@10 |    Gap |   Ext Cand\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "       Fuzzy Segment |                Fuzzy(7/8) |    91.1% |    96.0%★◎ |  -4.9pt |     233283\n",
      "       Fuzzy Segment |                Fuzzy(6/8) |    91.3% |    96.0%★◎ |  -4.7pt |     372298\n",
      "            Adaptive |        Adaptive Threshold |    91.5% |    95.7%★ |  -4.2pt |     296454\n",
      "       Weighted Bits |      Important+Fuzzy(7/8) |    91.6% |    95.7%★ |  -4.1pt |     151120\n",
      "       Weighted Bits |      Important+Fuzzy(6/8) |    91.5% |    95.7%★ |  -4.2pt |     296454\n",
      "       Fuzzy Segment |                Exact(8/8) |    90.0% |    95.0%★ |  -5.0pt |      70924\n",
      "       Weighted Bits |           Important+Exact |    82.7% |    89.3%★ |  -6.6pt |      42323\n",
      " Random Bit Sampling |      RBS(12bit, 30tables) |    76.8% |    83.7%★ |  -6.9pt |      14128\n",
      " Random Bit Sampling |      RBS(12bit, 20tables) |    69.6% |    73.7% |  -4.1pt |      10640\n",
      " Random Bit Sampling |      RBS(16bit, 30tables) |    57.0% |    65.0% |  -8.0pt |       5053\n",
      " Random Bit Sampling |      RBS(12bit, 10tables) |    57.0% |    62.0% |  -5.0pt |       7429\n",
      " Random Bit Sampling |      RBS(16bit, 20tables) |    50.4% |    55.7% |  -5.3pt |       4129\n",
      " Random Bit Sampling |      RBS(20bit, 30tables) |    42.5% |    45.7% |  -3.2pt |        858\n",
      " Random Bit Sampling |      RBS(16bit, 10tables) |    38.1% |    42.3% |  -4.2pt |       2967\n",
      " Random Bit Sampling |      RBS(20bit, 20tables) |    34.6% |    33.7% |   0.9pt |        616\n",
      "        Chunked Hash |              Chunk32(2/4) |    10.8% |     1.3% |   9.5pt |          0\n",
      "        Chunked Hash |              Chunk16(5/8) |    10.8% |     1.3% |   9.5pt |          0\n",
      "        Chunked Hash |              Chunk16(8/8) |    10.0% |     1.0% |   9.0pt |          0\n",
      "        Chunked Hash |              Chunk16(6/8) |    10.5% |     1.0% |   9.5pt |          0\n",
      "        Chunked Hash |              Chunk16(7/8) |    10.3% |     1.0% |   9.3pt |          0\n",
      "        Chunked Hash |              Chunk32(3/4) |    10.3% |     1.0% |   9.3pt |          0\n",
      "        Chunked Hash |              Chunk32(4/4) |    10.0% |     1.0% |   9.0pt |          0\n"
     ]
    }
   ],
   "source": [
    "# 総合比較表示\n",
    "print('=' * 120)\n",
    "print('総合比較: 全手法の結果サマリー')\n",
    "print('=' * 120)\n",
    "\n",
    "# 外部R@10でソート\n",
    "df_sorted = df_all.sort_values('external_recall@10', ascending=False)\n",
    "\n",
    "print(f'\\n{\"Category\":>20} | {\"Method\":>25} | {\"Int R@10\":>8} | {\"Ext R@10\":>8} | {\"Gap\":>6} | {\"Ext Cand\":>10}')\n",
    "print('-' * 120)\n",
    "\n",
    "for _, row in df_sorted.iterrows():\n",
    "    ext_mark = '★' if row['external_recall@10'] >= 0.80 else ''\n",
    "    best_mark = '◎' if row['external_recall@10'] == df_sorted['external_recall@10'].max() else ''\n",
    "    print(f'{row[\"category\"]:>20} | {row[\"method\"]:>25} | '\n",
    "          f'{row[\"internal_recall@10\"]*100:>7.1f}% | '\n",
    "          f'{row[\"external_recall@10\"]*100:>7.1f}%{ext_mark}{best_mark} | '\n",
    "          f'{row[\"gap\"]*100:>5.1f}pt | '\n",
    "          f'{row[\"external_candidates\"]:>10.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. 最良手法の選定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "最良手法の選定\n",
      "================================================================================\n",
      "\n",
      "■ 目標達成（外部R@10 >= 80%）: 8件\n",
      "  - Fuzzy(7/8): Int=91.1%, Ext=96.0%\n",
      "  - Fuzzy(6/8): Int=91.3%, Ext=96.0%\n",
      "  - Adaptive Threshold: Int=91.5%, Ext=95.7%\n",
      "  - Important+Fuzzy(7/8): Int=91.6%, Ext=95.7%\n",
      "  - Important+Fuzzy(6/8): Int=91.5%, Ext=95.7%\n",
      "  - Exact(8/8): Int=90.0%, Ext=95.0%\n",
      "  - Important+Exact: Int=82.7%, Ext=89.3%\n",
      "  - RBS(12bit, 30tables): Int=76.8%, Ext=83.7%\n",
      "\n",
      "■ 最良手法: Fuzzy(7/8)\n",
      "  内部R@10: 91.1%\n",
      "  外部R@10: 96.0%\n",
      "  ギャップ: -4.9pt\n",
      "  外部候補数: 233283\n"
     ]
    }
   ],
   "source": [
    "# 最良手法の特定\n",
    "print('=' * 80)\n",
    "print('最良手法の選定')\n",
    "print('=' * 80)\n",
    "\n",
    "# 外部R@10 >= 80%を達成した手法\n",
    "good_methods = df_sorted[df_sorted['external_recall@10'] >= 0.80]\n",
    "\n",
    "if len(good_methods) > 0:\n",
    "    print(f'\\n■ 目標達成（外部R@10 >= 80%）: {len(good_methods)}件')\n",
    "    for _, row in good_methods.iterrows():\n",
    "        print(f'  - {row[\"method\"]}: Int={row[\"internal_recall@10\"]*100:.1f}%, Ext={row[\"external_recall@10\"]*100:.1f}%')\n",
    "else:\n",
    "    print('\\n■ 目標達成（外部R@10 >= 80%）: なし')\n",
    "\n",
    "# 最も外部R@10が高い手法\n",
    "best = df_sorted.iloc[0]\n",
    "print(f'\\n■ 最良手法: {best[\"method\"]}')\n",
    "print(f'  内部R@10: {best[\"internal_recall@10\"]*100:.1f}%')\n",
    "print(f'  外部R@10: {best[\"external_recall@10\"]*100:.1f}%')\n",
    "print(f'  ギャップ: {best[\"gap\"]*100:.1f}pt')\n",
    "print(f'  外部候補数: {best[\"external_candidates\"]:.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. 結論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "結論\n",
      "================================================================================\n",
      "\n",
      "■ 実験目的\n",
      "  Overlap(8,4)カスケードが外部クエリで低再現率（62%）となる問題を解決する\n",
      "  代替手法を検証し、外部クエリR@10 >= 80%を目指す。\n",
      "\n",
      "■ ベースライン（Overlap 8/8 完全一致）\n",
      "  内部R@10: 90.0%\n",
      "  外部R@10: 95.0%\n",
      "  ギャップ:  -5.0pt\n",
      "\n",
      "■ 最良手法: Fuzzy(7/8)\n",
      "  内部R@10: 91.1%\n",
      "  外部R@10: 96.0%\n",
      "  ギャップ:  -4.9pt\n",
      "  改善幅:   +1.0pt（外部）\n",
      "\n",
      "■ 目標達成状況\n",
      "  外部R@10 >= 80%: 達成\n",
      "  候補数 < 50,000: 未達成\n",
      "\n",
      "■ 考察\n",
      "\n",
      "  ✓ Fuzzy(7/8)により、外部クエリでも80%以上のRecallを達成。\n",
      "  ✓ 誤り分布に依存しない手法が外部クエリに有効であることを確認。\n",
      "\n",
      "■ 推奨\n",
      "  本番環境では Fuzzy(7/8) を推奨。\n",
      "  内部/外部クエリ両方で安定した性能が期待できる。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('=' * 80)\n",
    "print('結論')\n",
    "print('=' * 80)\n",
    "\n",
    "best = df_sorted.iloc[0]\n",
    "baseline_overlap = df_fuzzy[df_fuzzy['method'] == 'Exact(8/8)'].iloc[0]\n",
    "\n",
    "print(f'''\n",
    "■ 実験目的\n",
    "  Overlap(8,4)カスケードが外部クエリで低再現率（62%）となる問題を解決する\n",
    "  代替手法を検証し、外部クエリR@10 >= 80%を目指す。\n",
    "\n",
    "■ ベースライン（Overlap 8/8 完全一致）\n",
    "  内部R@10: {baseline_overlap['internal_recall@10']*100:.1f}%\n",
    "  外部R@10: {baseline_overlap['external_recall@10']*100:.1f}%\n",
    "  ギャップ:  {(baseline_overlap['internal_recall@10'] - baseline_overlap['external_recall@10'])*100:.1f}pt\n",
    "\n",
    "■ 最良手法: {best['method']}\n",
    "  内部R@10: {best['internal_recall@10']*100:.1f}%\n",
    "  外部R@10: {best['external_recall@10']*100:.1f}%\n",
    "  ギャップ:  {best['gap']*100:.1f}pt\n",
    "  改善幅:   +{(best['external_recall@10'] - baseline_overlap['external_recall@10'])*100:.1f}pt（外部）\n",
    "\n",
    "■ 目標達成状況\n",
    "  外部R@10 >= 80%: {'達成' if best['external_recall@10'] >= 0.80 else '未達成'}\n",
    "  候補数 < 50,000: {'達成' if best['external_candidates'] < 50000 else '未達成'}\n",
    "\n",
    "■ 考察\n",
    "''')\n",
    "\n",
    "if best['external_recall@10'] >= 0.80:\n",
    "    print(f'  ✓ {best[\"method\"]}により、外部クエリでも80%以上のRecallを達成。')\n",
    "    print(f'  ✓ 誤り分布に依存しない手法が外部クエリに有効であることを確認。')\n",
    "else:\n",
    "    print(f'  △ 目標の80%には届かなかったが、ベースラインから改善。')\n",
    "    print(f'  → 追加の手法検討が必要（より多くのテーブル、ハイブリッド等）')\n",
    "\n",
    "print(f'''\n",
    "■ 推奨\n",
    "  本番環境では {best['method']} を推奨。\n",
    "  内部/外部クエリ両方で安定した性能が期待できる。\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 11. 実験評価まとめ\n\n### 重要な発見：予想と異なる結果\n\n**当初の仮説**:\n- 外部クエリ（キーワード検索）はビット誤りが均等に散らばるため、Overlapセグメント完全一致で再現率が低下する\n- 実験03（非公開データ2,789件）で外部R@10=62%と低かったのはこの理由\n\n**実際の結果（40万件データ）**:\n| クエリ種別 | Exact(8/8) | Fuzzy(7/8) |\n|-----------|------------|------------|\n| 内部クエリ | 90.0% | 91.1% |\n| 外部クエリ | **95.0%** | **96.0%** |\n\n→ **外部クエリの方が再現率が高い！**（仮説と逆）\n\n### なぜこの結果になったか\n\n1. **データ規模の違い**\n   - 2,789件: バケットサイズ平均16.8件 → スパース、候補不足\n   - 40万件: バケットサイズ平均1,560件 → 十分な候補が集まる\n\n2. **外部クエリのハミング距離が近い**\n   ```\n   内部クエリ → GT Top-10へのハミング距離: 平均23.4bit\n   外部クエリ → GT Top-10へのハミング距離: 平均19.0bit（より近い！）\n   ```\n   - E5埋め込みの特性上、キーワードクエリは関連ドキュメントと類似した空間に配置される\n   - 40万件データでは、外部クエリに近いドキュメントが多数存在\n\n3. **実験03との違いの根本原因**\n   - 2,789件: 外部クエリのセグメントにヒットするドキュメントが少ない\n   - 40万件: 外部クエリのセグメントにヒットするドキュメントが十分ある\n\n### 手法別の結果サマリー\n\n| カテゴリ | 手法 | 内部R@10 | 外部R@10 | 候補数 | 評価 |\n|---------|------|---------|---------|--------|------|\n| **Fuzzy Segment** | Fuzzy(7/8) | 91.1% | **96.0%** | 23.3万 | ◎最良 |\n| **Fuzzy Segment** | Fuzzy(6/8) | 91.3% | **96.0%** | 37.2万 | ○ |\n| **Fuzzy Segment** | Exact(8/8) | 90.0% | 95.0% | 7.1万 | ○推奨 |\n| **Weighted Bits** | Important+Fuzzy(7/8) | 91.6% | 95.7% | 15.1万 | ○ |\n| **RBS** | RBS(12bit, 30tables) | 76.8% | 83.7% | 1.4万 | △ |\n| **Chunked Hash** | Chunk16(6/8) | 10.5% | 1.0% | 0 | ✗失敗 |\n\n### 推奨設定（40万件規模）\n\n**本番環境での推奨**: `Exact(8/8)` = Overlap(8,4)完全一致\n\n```python\n# 推奨パラメータ\nSEGMENT_WIDTH = 8   # 8bitセグメント\nSTRIDE = 4          # 4bitストライド\nn_segments = 31     # オーバーラップセグメント数\n\n# 3段階カスケード\nStep 1: Overlapセグメント一致 → 約7万件（84%削減）\nStep 2: ハミング距離ソート → 2,000件\nStep 3: コサイン類似度 → Top-K\n```\n\n**理由**:\n1. 外部R@10 = 95.0%で十分高い（目標80%を大幅に上回る）\n2. 候補数7.1万件（Fuzzy方式より少なく効率的）\n3. 実装がシンプル\n\n### Fuzzy(7/8)を選ぶ場合\n\nより高い再現率が必要な場合は `Fuzzy(7/8)` を使用:\n\n```python\n# 8bitセグメントで1bit誤りを許容\n# 各セグメントで37通り(C(8,0)+C(8,1))のルックアップ\n# → 31セグメント × 37 = 1,147ルックアップ/クエリ\n\ndef generate_fuzzy_neighbors(segment_value, segment_width=8, max_flips=1):\n    neighbors = [segment_value]  # 完全一致\n    for pos in range(segment_width):\n        neighbors.append(segment_value ^ (1 << pos))  # 1bit反転\n    return neighbors\n```\n\n**トレードオフ**:\n- 内部R@10: +1.1pt（90.0% → 91.1%）\n- 外部R@10: +1.0pt（95.0% → 96.0%）\n- 候補数: 3.3倍増（7.1万 → 23.3万）\n\n### 失敗した手法の分析\n\n**チャンク比較（Chunked Hash）**: 完全に失敗\n- 16bitチャンクの完全一致は厳しすぎる\n- 40万件でも平均バケットサイズ = 400,000 / 65,536 ≈ 6件\n- ほとんどのクエリで候補が0件\n\n**ランダムビットサンプリング（RBS）**: 期待以下\n- RBS(12bit, 30tables)で外部R@10=83.7%は悪くないが、Overlapより劣る\n- ビット数が多いとバケットがスパース、少ないと候補が増えすぎる\n- Overlap方式の方がバランスが良い\n\n### 結論\n\n1. **40万件規模では、Overlap(8,4)完全一致で十分**\n   - 外部クエリでもR@10=95%を達成\n   - 実験03（2,789件）で見られた外部クエリ問題は、データ規模の小ささが原因\n\n2. **件数が少ない場合（数千件）は注意が必要**\n   - バケットがスパースになり、候補不足で再現率低下\n   - 2段階検索（全件ハミング距離→コサイン）を検討\n\n3. **ファジーマッチングは候補数増加とのトレードオフ**\n   - 1bit許容(7/8)で+1pt改善だが、候補数3倍\n   - 本番環境では Exact(8/8) で十分\n\n### 次のステップ\n\n1. 実験33の推奨設定（Overlap(8,4) + S1=10000 + S2=2000）を本番環境で採用\n2. 非公開データ（2,789件）では2段階検索を検討\n3. 本番データ（1.4万件）での追加検証"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsh-cascade-poc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}