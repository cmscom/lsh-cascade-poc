{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSH Cascade Search - 基本検証\n",
    "\n",
    "このノートブックでは、LSH (Locality Sensitive Hashing) を用いた3段階フィルタリング検索の基本動作を検証します。\n",
    "\n",
    "## 目次\n",
    "1. データの読み込みと確認\n",
    "2. SimHashの動作確認\n",
    "3. HNSW検索 vs LSH Cascade検索\n",
    "4. パラメータ比較（LSH-4, LSH-8, LSH-16）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from src.lsh import SimHashGenerator, chunk_hash, hamming_distance\n",
    "from src.db import VectorDatabase\n",
    "from src.pipeline import LSHCascadeSearcher, HNSWSearcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. データの読み込みと確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 10,000\n",
      "\n",
      "=== テーブルスキーマ ===\n",
      "column_name column_type null key default extra\n",
      "         id     INTEGER   NO PRI    None  None\n",
      "       text     VARCHAR  YES NaN    None  None\n",
      "     vector FLOAT[1024]  YES NaN    None  None\n",
      "    simhash     VARCHAR  YES NaN    None  None\n",
      " lsh_chunks   VARCHAR[]  YES NaN    None  None\n"
     ]
    }
   ],
   "source": [
    "# DuckDBからデータを読み込み\n",
    "db_path = Path('../data/sample_vectors.duckdb')  # 手元のWikipedaデータ（１万件）\n",
    "db = VectorDatabase(db_path=db_path)\n",
    "db.initialize()\n",
    "\n",
    "print(f'Total documents: {db.count():,}')\n",
    "\n",
    "# カラム情報を表示\n",
    "print('\\n=== テーブルスキーマ ===')\n",
    "schema = db.conn.execute('DESCRIBE documents').fetchdf()\n",
    "print(schema.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>vector</th>\n",
       "      <th>simhash</th>\n",
       "      <th>lsh_chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2012 009 conclusion GOD'S WAR</td>\n",
       "      <td>[0.013836248, -0.023572471, -0.043704864, -0.0...</td>\n",
       "      <td>BBABFCA5DB96BAF883038FB9CAD65CDB</td>\n",
       "      <td>[c0_BB, c1_AB, c2_FC, c3_A5, c4_DB, c5_96, c6_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>安部雄一</td>\n",
       "      <td>[0.027291711, 0.001755841, 0.020223374, -0.045...</td>\n",
       "      <td>BA958C25B31EAEF8C903879DC87ECCD6</td>\n",
       "      <td>[c0_BA, c1_95, c2_8C, c3_25, c4_B3, c5_1E, c6_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>アシャンティの伝統的建築物群</td>\n",
       "      <td>[0.041348234, -0.004756351, -0.01896887, -0.02...</td>\n",
       "      <td>AB990C2E9B5EECF8C1028D9DCA6E8C9A</td>\n",
       "      <td>[c0_AB, c1_99, c2_0C, c3_2E, c4_9B, c5_5E, c6_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                           text  \\\n",
       "0   0  2012 009 conclusion GOD'S WAR   \n",
       "1   1                           安部雄一   \n",
       "2   2                 アシャンティの伝統的建築物群   \n",
       "\n",
       "                                              vector  \\\n",
       "0  [0.013836248, -0.023572471, -0.043704864, -0.0...   \n",
       "1  [0.027291711, 0.001755841, 0.020223374, -0.045...   \n",
       "2  [0.041348234, -0.004756351, -0.01896887, -0.02...   \n",
       "\n",
       "                            simhash  \\\n",
       "0  BBABFCA5DB96BAF883038FB9CAD65CDB   \n",
       "1  BA958C25B31EAEF8C903879DC87ECCD6   \n",
       "2  AB990C2E9B5EECF8C1028D9DCA6E8C9A   \n",
       "\n",
       "                                          lsh_chunks  \n",
       "0  [c0_BB, c1_AB, c2_FC, c3_A5, c4_DB, c5_96, c6_...  \n",
       "1  [c0_BA, c1_95, c2_8C, c3_25, c4_B3, c5_1E, c6_...  \n",
       "2  [c0_AB, c1_99, c2_0C, c3_2E, c4_9B, c5_5E, c6_...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# サンプルデータを確認\n",
    "df_sample = db.get_by_ids([0, 1, 2])\n",
    "df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # サンプルデータの確認（全カラム表示）\n",
    "# df_sample = db.get_by_ids([0, 1, 2])\n",
    "# print(df_sample)\n",
    "# print(\"=====\")\n",
    "\n",
    "# print('=== サンプルデータ (3行) ===')\n",
    "# for i, row in df_sample.iterrows():\n",
    "#     print(f'\\n--- Row {row[\"id\"]} ---')\n",
    "#     print(f'id: {row[\"id\"]}')\n",
    "#     print(f'text: {row[\"text\"]}')\n",
    "#     vec = row[\"vector\"]\n",
    "#     print(f'vector: [{vec[0]:.6f}, {vec[1]:.6f}, ...] (len={len(vec)})')\n",
    "#     print(f'simhash: {row[\"simhash\"]}')\n",
    "#     chunks = row[\"lsh_chunks\"]\n",
    "#     print(f'lsh_chunks: {chunks[:4]}... (len={len(chunks)})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SimHashの動作確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector shape: (1024,)\n",
      "SimHash (int): 249458642282415097800268828519741938907\n",
      "SimHash (hex): BBABFCA5DB96BAF883038FB9CAD65CDB\n"
     ]
    }
   ],
   "source": [
    "# SimHashGeneratorの初期化\n",
    "simhash_gen = SimHashGenerator(dim=1024, hash_bits=128, seed=42)\n",
    "\n",
    "# サンプルベクトルでSimHashを生成\n",
    "sample_vec = np.array(df_sample.iloc[0]['vector'], dtype=np.float32)\n",
    "sample_hash = simhash_gen.hash(sample_vec)\n",
    "\n",
    "print(f'Vector shape: {sample_vec.shape}')\n",
    "print(f'SimHash (int): {sample_hash}')\n",
    "print(f'SimHash (hex): {sample_hash:032X}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSH-4: ['c0_BBABFCA5', 'c1_DB96BAF8', 'c2_83038FB9', 'c3_CAD65CDB']... (total 4 chunks)\n",
      "LSH-8: ['c0_BBAB', 'c1_FCA5', 'c2_DB96', 'c3_BAF8']... (total 8 chunks)\n",
      "LSH-16: ['c0_BB', 'c1_AB', 'c2_FC', 'c3_A5']... (total 16 chunks)\n"
     ]
    }
   ],
   "source": [
    "# チャンク分割の確認\n",
    "for num_chunks in [4, 8, 16]:\n",
    "    chunks = chunk_hash(sample_hash, num_chunks)\n",
    "    print(f'LSH-{num_chunks}: {chunks[:4]}... (total {len(chunks)} chunks)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming distance: 32 / 128 bits\n",
      "Cosine similarity: 0.7265\n"
     ]
    }
   ],
   "source": [
    "# ハミング距離の確認\n",
    "vec1 = np.array(df_sample.iloc[0]['vector'], dtype=np.float32)\n",
    "vec2 = np.array(df_sample.iloc[1]['vector'], dtype=np.float32)\n",
    "\n",
    "hash1 = simhash_gen.hash(vec1)\n",
    "hash2 = simhash_gen.hash(vec2)\n",
    "\n",
    "dist = hamming_distance(hash1, hash2)\n",
    "cosine_sim = np.dot(vec1, vec2)\n",
    "\n",
    "print(f'Hamming distance: {dist} / 128 bits')\n",
    "print(f'Cosine similarity: {cosine_sim:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. HNSW検索 vs LSH Cascade検索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 検索器の初期化\n",
    "hnsw_searcher = HNSWSearcher(db)\n",
    "lsh_searcher = LSHCascadeSearcher(\n",
    "    db=db,\n",
    "    simhash_generator=simhash_gen,\n",
    "    num_chunks=16,\n",
    "    # step2_top_n=100,\n",
    "    step2_top_n=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== HNSW Results ===\n",
      "Time: 18.71 ms\n",
      "  id=0, score=1.0000, text=2012 009 conclusion GOD'S WAR...\n",
      "  id=6138, score=0.8416, text=Wikipedia:Files for deletion/2...\n",
      "  id=8425, score=0.8334, text=Category:913 endings...\n",
      "  id=8546, score=0.8263, text=Category:1212 disestablishment...\n",
      "  id=8229, score=0.8261, text=Wikipedia:WikiProject Video ga...\n"
     ]
    }
   ],
   "source": [
    "# クエリベクトル（最初のドキュメントを使用）\n",
    "query_vec = np.array(df_sample.iloc[0]['vector'], dtype=np.float32)\n",
    "\n",
    "# HNSW検索\n",
    "hnsw_results, hnsw_time = hnsw_searcher.search(query_vec, top_k=10)\n",
    "print('=== HNSW Results ===')\n",
    "print(f'Time: {hnsw_time:.2f} ms')\n",
    "for r in hnsw_results[:5]:\n",
    "    print(f'  id={r.id}, score={r.score:.4f}, text={r.text[:30]}...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LSH Cascade Results ===\n",
      "Total time: 252.51 ms\n",
      "Step1 candidates: 8709\n",
      "Step2 candidates: 500\n",
      "\n",
      "  id=0, score=1.0000, text=2012 009 conclusion GOD'S WAR...\n",
      "  id=6138, score=0.8416, text=Wikipedia:Files for deletion/2...\n",
      "  id=8425, score=0.8334, text=Category:913 endings...\n",
      "  id=5551, score=0.8187, text=Coup d'état of December Twelft...\n",
      "  id=7494, score=0.8166, text=Wikipedia:Categories for discu...\n"
     ]
    }
   ],
   "source": [
    "# LSH Cascade検索\n",
    "lsh_results, lsh_metrics = lsh_searcher.search(query_vec, top_k=10)\n",
    "print('=== LSH Cascade Results ===')\n",
    "print(f'Total time: {lsh_metrics.total_time_ms:.2f} ms')\n",
    "print(f'Step1 candidates: {lsh_metrics.step1_candidates}')\n",
    "print(f'Step2 candidates: {lsh_metrics.step2_candidates}')\n",
    "print()\n",
    "for r in lsh_results[:5]:\n",
    "    print(f'  id={r.id}, score={r.score:.4f}, text={r.text[:30]}...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@10: 0.40\n"
     ]
    }
   ],
   "source": [
    "# Recall計算\n",
    "hnsw_ids = set(r.id for r in hnsw_results)\n",
    "lsh_ids = set(r.id for r in lsh_results)\n",
    "recall = len(hnsw_ids & lsh_ids) / len(hnsw_ids)\n",
    "\n",
    "print(f'Recall@10: {recall:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total docs: 10000\n",
      "Step1 candidates: 8709\n",
      "Step1 reduction: 87.1%\n"
     ]
    }
   ],
   "source": [
    "print(f'Total docs: {lsh_metrics.total_docs}')\n",
    "print(f'Step1 candidates: {lsh_metrics.step1_candidates}')\n",
    "print(f'Step1 reduction: {lsh_metrics.step1_candidates / lsh_metrics.total_docs * 100:.1f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ハミング距離とコサイン類似度の相関係数: -0.315\n",
      "\n",
      "=== HNSW正解10件のハミング距離順位 ===\n",
      "id=0: ハミング距離=0.0, 順位=1.0/8709, コサイン類似度=1.0000\n",
      "id=8546: ハミング距離=29.0, 順位=2481.0/8709, コサイン類似度=0.8263\n",
      "id=1858: ハミング距離=30.0, 順位=3030.0/8709, コサイン類似度=0.8217\n",
      "id=8229: ハミング距離=30.0, 順位=2885.0/8709, コサイン類似度=0.8261\n",
      "id=5256: ハミング距離=28.0, 順位=1503.0/8709, コサイン類似度=0.8199\n",
      "id=8425: ハミング距離=20.0, 順位=8.0/8709, コサイン類似度=0.8334\n",
      "id=9195: ハミング距離=30.0, 順位=2774.0/8709, コサイン類似度=0.8244\n",
      "id=5551: ハミング距離=24.0, 順位=206.0/8709, コサイン類似度=0.8187\n",
      "id=1872: ハミング距離=32.0, 順位=4972.0/8709, コサイン類似度=0.8199\n",
      "id=6138: ハミング距離=23.0, 順位=95.0/8709, コサイン類似度=0.8416\n"
     ]
    }
   ],
   "source": [
    "# Step1の候補に対して分析\n",
    "query_hash = simhash_gen.hash(query_vec)\n",
    "query_chunks = chunk_hash(query_hash, 16)\n",
    "candidates = db.search_lsh_chunks(query_chunks)\n",
    "\n",
    "results = []\n",
    "for _, row in candidates.iterrows():\n",
    "    doc_hash = int(row['simhash'], 16)\n",
    "    doc_vec = np.array(row['vector'], dtype=np.float32)\n",
    "    \n",
    "    ham_dist = hamming_distance(query_hash, doc_hash)\n",
    "    cos_sim = np.dot(query_vec, doc_vec)\n",
    "    \n",
    "    results.append({\n",
    "        'id': row['id'],\n",
    "        'hamming_dist': ham_dist,\n",
    "        'cosine_sim': cos_sim,\n",
    "    })\n",
    "\n",
    "df_analysis = pd.DataFrame(results)\n",
    "\n",
    "# 相関係数\n",
    "corr = df_analysis['hamming_dist'].corr(df_analysis['cosine_sim'])\n",
    "print(f'ハミング距離とコサイン類似度の相関係数: {corr:.3f}')\n",
    "\n",
    "# HNSW正解のハミング距離順位を確認\n",
    "df_sorted_ham = df_analysis.sort_values('hamming_dist')\n",
    "df_sorted_ham['ham_rank'] = range(1, len(df_sorted_ham) + 1)\n",
    "\n",
    "print('\\n=== HNSW正解10件のハミング距離順位 ===')\n",
    "hnsw_ids = {0, 8546, 1858, 8229, 5256, 8425, 9195, 5551, 1872, 6138}\n",
    "for doc_id in hnsw_ids:\n",
    "    row = df_sorted_ham[df_sorted_ham['id'] == doc_id].iloc[0]\n",
    "    print(f'id={doc_id}: ハミング距離={row[\"hamming_dist\"]}, 順位={row[\"ham_rank\"]}/{len(df_sorted_ham)}, コサイン類似度={row[\"cosine_sim\"]:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HNSW結果のID: {0, 8546, 1858, 8229, 5256, 8425, 9195, 5551, 1872, 6138}\n",
      "Step1候補に含まれるHNSW正解: 10/10\n",
      "含まれているID: {0, 8546, 1858, 8229, 5256, 8425, 9195, 5551, 1872, 6138}\n",
      "含まれていないID: set()\n"
     ]
    }
   ],
   "source": [
    "# HNSWの正解ID\n",
    "hnsw_ids = set(r.id for r in hnsw_results)\n",
    "print(f'HNSW結果のID: {hnsw_ids}')\n",
    "\n",
    "# Step1の候補ID\n",
    "query_chunks = chunk_hash(simhash_gen.hash(query_vec), 16)\n",
    "candidates = db.search_lsh_chunks(query_chunks)\n",
    "candidate_ids = set(candidates['id'])\n",
    "\n",
    "# 重複確認\n",
    "overlap = hnsw_ids & candidate_ids\n",
    "print(f'Step1候補に含まれるHNSW正解: {len(overlap)}/10')\n",
    "print(f'含まれているID: {overlap}')\n",
    "print(f'含まれていないID: {hnsw_ids - candidate_ids}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Query ID: 857, Text: 安俊洙...\n",
      "==================================================\n",
      "Step1候補数: 9258\n",
      "相関係数: -0.427\n",
      "HNSW正解がStep1に含まれる: 9/10\n",
      "HNSW正解がTop100に含まれる: 3/10\n",
      "HNSW正解がTop500に含まれる: 4/10\n",
      "\n",
      "==================================================\n",
      "Query ID: 4385, Text: ウィタリアヌス (ローマ教皇)...\n",
      "==================================================\n",
      "Step1候補数: 6397\n",
      "相関係数: -0.231\n",
      "HNSW正解がStep1に含まれる: 10/10\n",
      "HNSW正解がTop100に含まれる: 6/10\n",
      "HNSW正解がTop500に含まれる: 6/10\n",
      "\n",
      "==================================================\n",
      "Query ID: 1428, Text: ミシケの戦い...\n",
      "==================================================\n",
      "Step1候補数: 8585\n",
      "相関係数: -0.482\n",
      "HNSW正解がStep1に含まれる: 10/10\n",
      "HNSW正解がTop100に含まれる: 4/10\n",
      "HNSW正解がTop500に含まれる: 8/10\n",
      "\n",
      "==================================================\n",
      "Query ID: 6672, Text: Wikipedia:Articles for deletio...\n",
      "==================================================\n",
      "Step1候補数: 8093\n",
      "相関係数: -0.581\n",
      "HNSW正解がStep1に含まれる: 9/10\n",
      "HNSW正解がTop100に含まれる: 4/10\n",
      "HNSW正解がTop500に含まれる: 4/10\n",
      "\n",
      "==================================================\n",
      "Query ID: 4367, Text: おぼん・こぼん...\n",
      "==================================================\n",
      "Step1候補数: 8178\n",
      "相関係数: -0.489\n",
      "HNSW正解がStep1に含まれる: 10/10\n",
      "HNSW正解がTop100に含まれる: 3/10\n",
      "HNSW正解がTop500に含まれる: 7/10\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# ランダムに5件のクエリを選択\n",
    "random.seed(123)\n",
    "all_docs = db.get_all()\n",
    "query_indices = random.sample(range(len(all_docs)), 5)\n",
    "\n",
    "for idx in query_indices:\n",
    "    query_row = all_docs.iloc[idx]\n",
    "    query_vec = np.array(query_row['vector'], dtype=np.float32)\n",
    "    query_id = query_row['id']\n",
    "    \n",
    "    print(f'\\n{\"=\"*50}')\n",
    "    print(f'Query ID: {query_id}, Text: {query_row[\"text\"][:30]}...')\n",
    "    print(f'{\"=\"*50}')\n",
    "    \n",
    "    # HNSW検索\n",
    "    hnsw_results, _ = hnsw_searcher.search(query_vec, top_k=10)\n",
    "    hnsw_ids = set(r.id for r in hnsw_results)\n",
    "    \n",
    "    # Step1候補取得\n",
    "    query_hash = simhash_gen.hash(query_vec)\n",
    "    query_chunks = chunk_hash(query_hash, 16)\n",
    "    candidates = db.search_lsh_chunks(query_chunks)\n",
    "    \n",
    "    # 相関分析\n",
    "    results = []\n",
    "    for _, row in candidates.iterrows():\n",
    "        doc_hash = int(row['simhash'], 16)\n",
    "        doc_vec = np.array(row['vector'], dtype=np.float32)\n",
    "        ham_dist = hamming_distance(query_hash, doc_hash)\n",
    "        cos_sim = np.dot(query_vec, doc_vec)\n",
    "        results.append({'id': row['id'], 'hamming_dist': ham_dist, 'cosine_sim': cos_sim})\n",
    "    \n",
    "    df_analysis = pd.DataFrame(results)\n",
    "    corr = df_analysis['hamming_dist'].corr(df_analysis['cosine_sim'])\n",
    "    \n",
    "    # HNSW正解の順位確認\n",
    "    df_sorted = df_analysis.sort_values('hamming_dist')\n",
    "    df_sorted['ham_rank'] = range(1, len(df_sorted) + 1)\n",
    "    \n",
    "    ranks_in_100 = 0\n",
    "    ranks_in_500 = 0\n",
    "    for doc_id in hnsw_ids:\n",
    "        if doc_id in df_sorted['id'].values:\n",
    "            rank = df_sorted[df_sorted['id'] == doc_id]['ham_rank'].values[0]\n",
    "            if rank <= 100:\n",
    "                ranks_in_100 += 1\n",
    "            if rank <= 500:\n",
    "                ranks_in_500 += 1\n",
    "    \n",
    "    print(f'Step1候補数: {len(candidates)}')\n",
    "    print(f'相関係数: {corr:.3f}')\n",
    "    print(f'HNSW正解がStep1に含まれる: {len(hnsw_ids & set(candidates[\"id\"]))}/10')\n",
    "    print(f'HNSW正解がTop100に含まれる: {ranks_in_100}/10')\n",
    "    print(f'HNSW正解がTop500に含まれる: {ranks_in_500}/10')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 0, Text: 2012 009 conclusion GOD'S WAR...\n",
      "\n",
      "Seed=42: 相関係数=-0.319, Top100=2/10, Top500=4/10\n",
      "Seed=123: 相関係数=-0.457, Top100=3/10, Top500=4/10\n",
      "Seed=456: 相関係数=-0.440, Top100=2/10, Top500=5/10\n",
      "Seed=789: 相関係数=-0.345, Top100=2/10, Top500=3/10\n",
      "Seed=1000: 相関係数=-0.393, Top100=3/10, Top500=6/10\n"
     ]
    }
   ],
   "source": [
    "# 複数のシード値で比較\n",
    "seeds = [42, 123, 456, 789, 1000]\n",
    "\n",
    "# 固定のクエリを使用（最初のドキュメント）\n",
    "query_row = all_docs.iloc[0]\n",
    "query_vec = np.array(query_row['vector'], dtype=np.float32)\n",
    "\n",
    "print(f'Query ID: {query_row[\"id\"]}, Text: {query_row[\"text\"][:30]}...')\n",
    "print()\n",
    "\n",
    "for seed in seeds:\n",
    "    # 新しいシードでSimHashGeneratorを作成\n",
    "    test_gen = SimHashGenerator(dim=1024, hash_bits=128, seed=seed)\n",
    "    \n",
    "    # クエリのハッシュとチャンク\n",
    "    query_hash = test_gen.hash(query_vec)\n",
    "    \n",
    "    # 全ドキュメントのハッシュを再計算して分析\n",
    "    results = []\n",
    "    for _, row in all_docs.iterrows():\n",
    "        doc_vec = np.array(row['vector'], dtype=np.float32)\n",
    "        doc_hash = test_gen.hash(doc_vec)\n",
    "        \n",
    "        ham_dist = hamming_distance(query_hash, doc_hash)\n",
    "        cos_sim = np.dot(query_vec, doc_vec)\n",
    "        \n",
    "        results.append({\n",
    "            'id': row['id'],\n",
    "            'hamming_dist': ham_dist,\n",
    "            'cosine_sim': cos_sim,\n",
    "        })\n",
    "    \n",
    "    df_analysis = pd.DataFrame(results)\n",
    "    corr = df_analysis['hamming_dist'].corr(df_analysis['cosine_sim'])\n",
    "    \n",
    "    # HNSW正解の順位確認\n",
    "    hnsw_results, _ = hnsw_searcher.search(query_vec, top_k=10)\n",
    "    hnsw_ids = set(r.id for r in hnsw_results)\n",
    "    \n",
    "    df_sorted = df_analysis.sort_values('hamming_dist')\n",
    "    df_sorted['ham_rank'] = range(1, len(df_sorted) + 1)\n",
    "    \n",
    "    ranks_in_100 = sum(1 for doc_id in hnsw_ids \n",
    "                       if df_sorted[df_sorted['id'] == doc_id]['ham_rank'].values[0] <= 100)\n",
    "    ranks_in_500 = sum(1 for doc_id in hnsw_ids \n",
    "                       if df_sorted[df_sorted['id'] == doc_id]['ham_rank'].values[0] <= 500)\n",
    "    \n",
    "    print(f'Seed={seed}: 相関係数={corr:.3f}, Top100={ranks_in_100}/10, Top500={ranks_in_500}/10')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. パラメータ比較（LSH-4, LSH-8, LSH-16）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunks</th>\n",
       "      <th>bits_per_chunk</th>\n",
       "      <th>avg_recall</th>\n",
       "      <th>avg_latency_ms</th>\n",
       "      <th>avg_candidates</th>\n",
       "      <th>reduction_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.341149</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.175816</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>0.45</td>\n",
       "      <td>208.753611</td>\n",
       "      <td>8026.3</td>\n",
       "      <td>0.19737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chunks  bits_per_chunk  avg_recall  avg_latency_ms  avg_candidates  \\\n",
       "0       4              32        0.00        9.341149             0.0   \n",
       "1       8              16        0.00       11.175816             0.0   \n",
       "2      16               8        0.45      208.753611          8026.3   \n",
       "\n",
       "   reduction_rate  \n",
       "0         1.00000  \n",
       "1         1.00000  \n",
       "2         0.19737  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 複数クエリでの比較\n",
    "import random\n",
    "\n",
    "# ランダムに10件のクエリを選択\n",
    "random.seed(42)\n",
    "all_docs = db.get_all()\n",
    "query_indices = random.sample(range(len(all_docs)), 10)\n",
    "\n",
    "results_summary = []\n",
    "\n",
    "for num_chunks in [4, 8, 16]:\n",
    "    searcher = LSHCascadeSearcher(\n",
    "        db=db,\n",
    "        simhash_generator=simhash_gen,\n",
    "        num_chunks=num_chunks,\n",
    "        step2_top_n=100,\n",
    "    )\n",
    "    \n",
    "    recalls = []\n",
    "    latencies = []\n",
    "    candidates = []\n",
    "    \n",
    "    for idx in query_indices:\n",
    "        query_vec = np.array(all_docs.iloc[idx]['vector'], dtype=np.float32)\n",
    "        \n",
    "        # HNSW baseline\n",
    "        hnsw_results, _ = hnsw_searcher.search(query_vec, top_k=10)\n",
    "        hnsw_ids = set(r.id for r in hnsw_results)\n",
    "        \n",
    "        # LSH search\n",
    "        lsh_results, metrics = searcher.search(query_vec, top_k=10)\n",
    "        lsh_ids = set(r.id for r in lsh_results)\n",
    "        \n",
    "        recall = len(hnsw_ids & lsh_ids) / len(hnsw_ids) if hnsw_ids else 0\n",
    "        recalls.append(recall)\n",
    "        latencies.append(metrics.total_time_ms)\n",
    "        candidates.append(metrics.step1_candidates)\n",
    "    \n",
    "    results_summary.append({\n",
    "        'chunks': num_chunks,\n",
    "        'bits_per_chunk': 128 // num_chunks,\n",
    "        'avg_recall': np.mean(recalls),\n",
    "        'avg_latency_ms': np.mean(latencies),\n",
    "        'avg_candidates': np.mean(candidates),\n",
    "        'reduction_rate': 1 - np.mean(candidates) / len(all_docs),\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results_summary)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 0\n",
      "Query SimHash: BBABFCA5DB96BAF883038FB9CAD65CDB\n",
      "\n",
      "=== 4チャンク ===\n",
      "Query chunks: {'c1_DB96BAF8', 'c2_83038FB9', 'c3_CAD65CDB', 'c0_BBABFCA5'}\n",
      "Doc chunks: {'c1_DB96BAF8', 'c2_83038FB9', 'c3_CAD65CDB', 'c0_BBABFCA5'}\n",
      "一致: {'c1_DB96BAF8', 'c2_83038FB9', 'c3_CAD65CDB', 'c0_BBABFCA5'}\n",
      "最初の100件中のマッチ数: 1\n",
      "\n",
      "=== 8チャンク ===\n",
      "Query chunks: {'c4_8303', 'c5_8FB9', 'c6_CAD6', 'c0_BBAB', 'c1_FCA5', 'c2_DB96', 'c3_BAF8', 'c7_5CDB'}\n",
      "Doc chunks: {'c4_8303', 'c5_8FB9', 'c6_CAD6', 'c0_BBAB', 'c1_FCA5', 'c2_DB96', 'c3_BAF8', 'c7_5CDB'}\n",
      "一致: {'c4_8303', 'c5_8FB9', 'c6_CAD6', 'c0_BBAB', 'c1_FCA5', 'c2_DB96', 'c3_BAF8', 'c7_5CDB'}\n",
      "最初の100件中のマッチ数: 6\n",
      "\n",
      "=== 16チャンク ===\n",
      "Query chunks: {'c6_BA', 'c8_83', 'c3_A5', 'c12_CA', 'c2_FC', 'c13_D6', 'c15_DB', 'c7_F8', 'c9_03', 'c0_BB', 'c4_DB', 'c14_5C', 'c10_8F', 'c1_AB', 'c5_96', 'c11_B9'}\n",
      "Doc chunks: {'c6_BA', 'c8_83', 'c3_A5', 'c12_CA', 'c2_FC', 'c13_D6', 'c15_DB', 'c7_F8', 'c9_03', 'c0_BB', 'c4_DB', 'c14_5C', 'c10_8F', 'c1_AB', 'c5_96', 'c11_B9'}\n",
      "一致: {'c12_CA', 'c8_83', 'c3_A5', 'c5_96', 'c2_FC', 'c13_D6', 'c15_DB', 'c1_AB', 'c7_F8', 'c0_BB', 'c4_DB', 'c14_5C', 'c10_8F', 'c6_BA', 'c9_03', 'c11_B9'}\n",
      "最初の100件中のマッチ数: 87\n"
     ]
    }
   ],
   "source": [
    "# デバッグ: 単一クエリで4チャンクのマッチングを確認\n",
    "query_idx = 0\n",
    "query_vec = np.array(all_docs.iloc[query_idx]['vector'], dtype=np.float32)\n",
    "query_hash = simhash_gen.hash(query_vec)\n",
    "\n",
    "print(f'Query ID: {all_docs.iloc[query_idx][\"id\"]}')\n",
    "print(f'Query SimHash: {query_hash:032X}')\n",
    "\n",
    "for num_chunks in [4, 8, 16]:\n",
    "    query_chunks = set(chunk_hash(query_hash, num_chunks))\n",
    "    print(f'\\n=== {num_chunks}チャンク ===')\n",
    "    print(f'Query chunks: {query_chunks}')\n",
    "    \n",
    "    # 自分自身とマッチするか確認\n",
    "    doc_hash = int(all_docs.iloc[query_idx]['simhash'], 16)\n",
    "    doc_chunks = set(chunk_hash(doc_hash, num_chunks))\n",
    "    print(f'Doc chunks: {doc_chunks}')\n",
    "    print(f'一致: {query_chunks & doc_chunks}')\n",
    "    \n",
    "    # 候補数をカウント\n",
    "    match_count = 0\n",
    "    for _, row in all_docs.head(100).iterrows():\n",
    "        doc_hash = int(row['simhash'], 16)\n",
    "        doc_chunks = set(chunk_hash(doc_hash, num_chunks))\n",
    "        if query_chunks & doc_chunks:\n",
    "            match_count += 1\n",
    "    print(f'最初の100件中のマッチ数: {match_count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# データベース接続を閉じる\n",
    "db.close()\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsh-cascade-poc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
